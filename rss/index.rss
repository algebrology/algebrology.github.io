<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>Algebrology</title><description>A gentle introduction to insanity.</description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>Algebrology</title><link>http://localhost:2368/</link></image><generator>Ghost 2.14</generator><lastBuildDate>Sat, 16 Mar 2019 07:33:35 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>Vector Spaces, Subspaces and Sums</title><description>A vector space is a special kind of set containing elements called vectors, which can be added together and scaled in all the ways one would generally expect.</description><link>http://localhost:2368/vector-spaces/</link><guid isPermaLink="false">5c8c6f0bfdce06003e4aeff0</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Sat, 16 Mar 2019 04:04:40 GMT</pubDate><content:encoded>&lt;ol&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#vector-spaces"&gt;Vector Spaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#basic-properties"&gt;Basic Properties&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples"&gt;Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#subspaces-and-sums"&gt;Subspaces and Sums&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="introductionanameintroduction"&gt;Introduction&lt;a name="introduction"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;It's almost ridiculous that I would expose you to free abelian groups before talking about vector spaces and linear algebra. Vector spaces and free abelian groups have a lot in common, but vector spaces are more familiar, more ubiquitous and easier to compute with.&lt;/p&gt;
&lt;p&gt;At their core, vector spaces are very simple and their definition will closely mimic that of groups and topological space. Recall that a group is a set with a binary operation, an identity and inverses for all its elements. A topological space is a set and a collection of &amp;quot;open sets&amp;quot; which include the set itself, the empty set, finite intersections and arbitrary unions of open sets. Vector spaces are defined in a similar manner.&lt;/p&gt;
&lt;p&gt;A vector space is a special kind of set containing elements called vectors, which can be added together and scaled in all the ways one would generally expect.&lt;/p&gt;
&lt;p&gt;You have likely encountered the idea of a vector before as some sort of arrow, anchored to the origin in euclidean space with some well-defined magnitude and direction.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/03/vector-arrow.svg" alt="vector-arrow"&gt;&lt;/p&gt;
&lt;p&gt;This is the sort of vector encountered in introductory physics classes. However, such arrows are not the only mathematical objects that can be added and scaled, so it would be silly to restrict our attention only to them. We will make a more abstract and inclusive definition of vector spaces, which are the main objects of study in linear algebra.&lt;/p&gt;
&lt;p&gt;We would like our definition to include some way to scale vectors so that we can expand of shrink them in magnitude while preserving their direction. Since in general there is no concept of vector multiplication, we will need to bring in additional elements by which we are allowed to multiply our vectors to achieve a scaling effect. These &lt;em&gt;scalars&lt;/em&gt; will be the elements of a field, which we have encountered before in my posts on constructing the rational numbers, but I will give the definition again because it is so important.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;field&lt;/strong&gt; is a set $\F$ of elements called &lt;strong&gt;scalars&lt;/strong&gt;, together with two binary operations:&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Addition&lt;/strong&gt;&lt;br&gt;
        which assigns to any pair of scalars $a,b\in\F$ the scalar $a+b\in\F$,&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Multiplication&lt;/strong&gt;&lt;br&gt;
        which assigns to any pair of scalars $a,b\in\F$ the scalar $ab\in\F$.&lt;/p&gt;
&lt;p&gt;Any field and its operations must satisfy the following properties:&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Additive Identity&lt;/strong&gt;&lt;br&gt;
         There exists $0\in\F$ such that $0+a=a$ for every $a\in\F$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Additive Inverses&lt;/strong&gt;&lt;br&gt;
         For every $a\in\F$, there exists $b\in\F$ for which $a+b=0$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Commutative Property of Addition&lt;/strong&gt;&lt;br&gt;
         For all $a,b\in\F$, we have that $a+b=b+a$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Associative Property of Addition&lt;/strong&gt;&lt;br&gt;
         For all $a,b,c\in\F$, we have that $(a+b)+c=a+(b+c)$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Multiplicative Identity&lt;/strong&gt;&lt;br&gt;
         There exists $1\in\F$, with $1\ne 0$, such that $1a=a$ for every $a\in\F$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Multiplicative Inverses&lt;/strong&gt;&lt;br&gt;
         For every $a\in\F$ with $a\ne 0$, there exists $b\in\F$ for which $ab=1$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Commutative Property of Multiplication&lt;/strong&gt;&lt;br&gt;
         For all $a,b\in\F$, we have that $ab=ba$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Associative Property of Multiplication&lt;/strong&gt;&lt;br&gt;
         For all $a,b,c\in\F$, we have that $(ab)c=a(bc)$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Distributive Property&lt;/strong&gt;&lt;br&gt;
         For all $a,b,c\in\F$, we have that $a(b+c)=ab+ac$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is not too difficult to verify that the set $\R$ of real numbers is a field when considered with the usual addition and multiplication. Similarly, the sets $\C$ of complex numbers and $\Q$ of rational numbers form fields when equipped with their usual arithmetic.&lt;/p&gt;
&lt;p&gt;There are other examples of fields besides these familiar ones. For example, the set $\Z_3$ of integers modulo $3$ is a field when considered with addition and multiplication modulo $3$. The tables below describe addition and multiplication in $\Z_3$, so you can check for yourself that the field axioms hold.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/03/Z_3-tables.svg" alt="Z_3-tables"&gt;&lt;/p&gt;
&lt;p&gt;In fact, there are many more examples of finite fields. For any prime number $p$, the set $\Z_p$ of integers modulo $p$ forms a field. However, for our purposes we will usually only be interested in the fields $\R$ and $\C$.&lt;/p&gt;
&lt;h3 id="vectorspacesanamevectorspaces"&gt;Vector Spaces&lt;a name="vector-spaces"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;With all of this in mind, we can move forward with defining the concept of a vector space over a field. This definition ensures that vectors interact with scalars and other vectors in a reasonable way.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;vector space over a field&lt;/strong&gt; $\F$ is a set $V$ of elements called &lt;strong&gt;vectors&lt;/strong&gt;, together with two operations:&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Vector Addition&lt;/strong&gt;&lt;br&gt;
        which assigns to any pair of vectors $\vec{u},\vec{v}\in V$ the vector $\vec{u}+\vec{v}\in V$,&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Scalar Multiplication&lt;/strong&gt;&lt;br&gt;
        which assigns to any scalar $a\in\F$ and any vector $\vec{v}\in V$ the vector $a\vec{v}\in V$.&lt;/p&gt;
&lt;p&gt;Any vector space and its operations must satisfy the following properties:&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Zero Vector&lt;/strong&gt;&lt;br&gt;
        There exists $\vec{0}\in V$ such that $\vec{0}+\vec{v}=\vec{v}$ for every $\vec{v}\in V$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Additive Inverses&lt;/strong&gt;&lt;br&gt;
        For every $\vec{u}\in V$, there exists $\vec{v}\in V$ for which $\vec{u}+\vec{v}=\vec{0}$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Commutative Property of Addition&lt;/strong&gt;&lt;br&gt;
        For all $\vec{u},\vec{v}\in V$, we have that $\vec{u}+\vec{v}=\vec{v}+\vec{u}$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Associative Property of Addition&lt;/strong&gt;&lt;br&gt;
        For all $\vec{u},\vec{v}, \vec{w}\in V$, we have that $(\vec{u}+\vec{v})+\vec{w}=\vec{u}+(\vec{v}+\vec{w})$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Compatibility with Field Multiplication&lt;/strong&gt;&lt;br&gt;
        For all $a,b\in\F$ and $\vec{v}\in V$, we have that $(ab)\vec{v}=a(b\vec{v})$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Scalar Multiplicative Identity&lt;/strong&gt;&lt;br&gt;
        For every $\vec{v}\in V$, we have that $1\vec{v}=\vec{v}$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;First Distributive Property&lt;/strong&gt;&lt;br&gt;
        For all $a,b\in\F$ and $\vec{v}\in V$, we have that $(a+b)\vec{v}=a\vec{v}+b\vec{v}$.&lt;/p&gt;
&lt;p&gt;    &lt;strong&gt;Second Distributive Property&lt;/strong&gt;&lt;br&gt;
        For all $a\in\F$ and $\vec{u},\vec{v}\in V$, we have that $a(\vec{u}+\vec{v})=a\vec{u}+a\vec{v}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Although the choice of field is important when defining a particular vector space, it is often ignored when talking generally about vector spaces. This is because many results hold true for all vector spaces, regardless of the field over which they are defined. Whenever this information is important, it will be specified. Otherwise, we will often refer simply to a vector space $V$, with the understanding that some field $\F$ is lurking in the background.&lt;/p&gt;
&lt;h3 id="basicpropertiesanamebasicproperties"&gt;Basic Properties&lt;a name="basic-properties"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Now we will verify five basic facts that are true of all vector spaces. They may seem obvious, and many of them closely mimic analogous results we've already seen in group theory. Nonetheless, without proof we could not in good faith use them in our later arguments.&lt;/p&gt;
&lt;p&gt;The first important property is that a vector space only has one zero vector.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; In any vector space $V$, the zero vector is unique.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose there exist two zero vectors, $\vec{0}_1,\vec{0}_2\in V$. Then, using the definition of a zero vector and the commutative property, we have that&lt;/p&gt;
&lt;p&gt;\begin{align}&lt;br&gt;
\vec{0}_1 &amp;amp;= \vec{0}_1+\vec{v}, \\&lt;br&gt;
\vec{0}_2 &amp;amp;= \vec{v}+\vec{0}_2,&lt;br&gt;
\end{align}&lt;/p&gt;
&lt;p&gt;for every $\vec{v}\in V$. In light of these equations,&lt;/p&gt;
&lt;p&gt;\begin{align}&lt;br&gt;
\vec{0}_1 &amp;amp;= \vec{0}_1 + \vec{0}_2 \\&lt;br&gt;
&amp;amp;= \vec{0}_2.&lt;br&gt;
\end{align}&lt;/p&gt;
&lt;p&gt;It follows that the zero vector is unique.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The next fact that we will prove is that each vector in a vector space has only one additive inverse. Its proof is extremely similar to the above.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; In any vector space $V$, every vector $\vec{u}\in V$ has a unique additive inverse.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose $\vec{u}$ has two additive inverses, $\vec{v}_1,\vec{v}_2\in V$. Then, using the definition of an additive inverse and the commutative property, we have that&lt;/p&gt;
&lt;p&gt;\begin{align}&lt;br&gt;
\vec{v}_1+\vec{u} &amp;amp;= \vec{0}, \\&lt;br&gt;
\vec{u}+\vec{v}_2 &amp;amp;= \vec{0}.&lt;br&gt;
\end{align}&lt;/p&gt;
&lt;p&gt;In light of these equations,&lt;/p&gt;
&lt;p&gt;\begin{align}&lt;br&gt;
\vec{v}_1 &amp;amp;= \vec{v}_1 + \vec{0} &amp;amp; \scriptstyle\textit{zero vector}\\&lt;br&gt;
&amp;amp;= \vec{v}_1 + (\vec{u}+\vec{v_2}) &amp;amp; \scriptstyle\textit{additive inverses}\\&lt;br&gt;
&amp;amp;= (\vec{v}_1 + \vec{u}) + \vec{v}_2 &amp;amp; \scriptstyle\textit{associativity}\\&lt;br&gt;
&amp;amp;= \vec{0} + \vec{v}_2 &amp;amp; \scriptstyle\textit{additive inverses}\\&lt;br&gt;
&amp;amp;= \vec{v}_2. &amp;amp; \scriptstyle\textit{zero vector}&lt;br&gt;
\end{align}&lt;/p&gt;
&lt;p&gt;It follows that $u$ has a unique additive inverse.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Because of this theorem, no confusion arises if we write $-\vec{v}$ to denote the additive inverse of $V$. We will adopt this notation for the rest of time.&lt;/p&gt;
&lt;p&gt;We will not take the time to do this, but it should be clear how to modify the above two proofs to show that in any field $\F$, additive and multiplicative identities are unique, as well as additive and multiplicative inverses.&lt;/p&gt;
&lt;p&gt;Next, we show that the scalar product of a field's additive identity $0$ with any vector yields the zero vector.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; In any vector space $V$, we have that $0\vec{v}=\vec{0}$ for every $\vec{v}\in V$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We proceed via the following computation:&lt;/p&gt;
&lt;p&gt;\begin{align}&lt;br&gt;
0\vec{v} + 0\vec{v} &amp;amp;= (0+0)\vec{v} &amp;amp; \scriptstyle\textit{first distributive property}\\&lt;br&gt;
&amp;amp;= 0\vec{v} &amp;amp; \scriptstyle\textit{additive identity}\\&lt;br&gt;
&amp;amp;= 0\vec{v} + \vec{0}. &amp;amp; \scriptstyle\textit{zero vector}&lt;br&gt;
\end{align}&lt;/p&gt;
&lt;p&gt;Adding $-0\vec{v}$ to both sides yields $0\vec{v} = \vec{0}$, the desired equality.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is very important to realize that in the above proof, the scalar $0$ on the left is the additive identity in the underlying field $\F$, while the vector $\vec{0}$ on the right is the zero vector in the vector space $V$. Remember that we do not have any concept of multiplying vectors.&lt;/p&gt;
&lt;p&gt;We will now prove a result of similar obviousness, whose proof is similar to the above.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; In any vector space $V$ over a field $\F$, we have that $a\vec{0}=\vec{0}$ for every $a\in\F$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We proceed via the following computation:&lt;/p&gt;
&lt;p&gt;\begin{align}&lt;br&gt;
a\vec{0} + a\vec{0} &amp;amp;= a(\vec{0}+\vec{0}) &amp;amp; \scriptstyle\textit{second distributive property}\\&lt;br&gt;
&amp;amp;= a\vec{0} &amp;amp; \scriptstyle\textit{zero vector}\\&lt;br&gt;
&amp;amp;= a\vec{0} + \vec{0}. &amp;amp; \scriptstyle\textit{zero vector}&lt;br&gt;
\end{align}&lt;/p&gt;
&lt;p&gt;Adding $-a\vec{0}$ to both sides yields $a\vec{0} = \vec{0}$, the desired equality.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is one obvious fact left to prove, namely that the scalar product of $-1$ with any vector yields the additive inverse of that vector.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; In any vector space $V$, we have that $-1\vec{v}=-\vec{v}$ for every $\vec{v}\in V$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We proceed via the following computation:&lt;/p&gt;
&lt;p&gt;\begin{align}&lt;br&gt;
-1\vec{v}+\vec{v} &amp;amp;= -1\vec{v}+1\vec{v} &amp;amp; \scriptstyle\textit{multiplicative identity}\\&lt;br&gt;
&amp;amp;= (-1+1)\vec{v} &amp;amp; \scriptstyle\textit{first distributive property}\\&lt;br&gt;
&amp;amp;= 0\vec{v} &amp;amp; \scriptstyle\textit{additive inverses}\\&lt;br&gt;
&amp;amp;= \vec{0}. &amp;amp; \scriptstyle\textit{zero vector}&lt;br&gt;
\end{align}&lt;/p&gt;
&lt;p&gt;Adding $-\vec{v}$ to both sides yields $-1\vec{v} = -\vec{v}$, the desired equality.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="examplesanameexamples"&gt;Examples&lt;a name="examples"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We are now armed with a number of facts about abstract vector spaces and their interactions with scalars, but we have yet to exhibit a single actual example of a vector space. We will now examine some vector spaces that are important throughout the entire subject of linear algebra.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; It isn't too hard to see that the set $\{\0\}$, which contains only the zero vector, is a vector space over any field. We are forced to define vector addition and scalar multiplication in the only possible way, and $\0$ must act as both the zero vector and its own additive inverse. This is analogous to the trivial group in group theory, and is not a particularly interesting example.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; For any field $\F$, it happens that $\F$ is a vector space over itself, taking vector addition to be the same as field addition and scalar multiplication to be the same as field multiplication. This, again, is easy to check directly from the definitions. As specific examples, $\R$ and $\C$ are both vector spaces over themselves.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For the next example, recall the definition of cartesian product. In particular, recall that $\F^n$ is the set of all ordered tuples of length $n$ with components in $\F$. For instance, $(i, 3+2i, 4)$ is an element of $\C^3$ and $(0, -\pi, 8, \sqrt{2})$ is an element of $\R^4$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; For any field $\F$, the set $\F^n$ is a vector space over $\F$ if we define vector addition and scalar multiplication in the following (obvious) way.&lt;/p&gt;
&lt;p&gt;Given $\x=(x_1,x_2,\ldots,x_n)$ and $\y=(y_1,y_2,\ldots,y_n)$ in $\F^n$, we define addition component-wise in terms of field addition as follows:&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\x+\y &amp;amp;= (x_1,x_2,\ldots,x_n)+(y_1,y_2,\ldots,y_n) \\&lt;br&gt;
&amp;amp;= (x_1+y_1,x_2+y_2,\ldots,x_n+y_n).&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Similarly, given $a\in\F$ and $\x=(x_1,x_2,\ldots,x_n)\in\F^n$, we define scalar multiplication component-wise in terms of field multiplication as follows:&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
a\x &amp;amp;= a(x_1,x_2,\ldots,x_n) \\&lt;br&gt;
&amp;amp;= (ax_1,ax_2,\ldots,ax_n).&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;It is not terribly difficult to show that $\F^n$ does in fact constitute a vector space over $\F$ with these operations. For instance, clearly $\0=(0,0,\ldots,0)$ acts as the zero vector in $\F^n$, and $-\x=(-x_1,-x_2,\ldots,-x_n)$ acts as the additive inverse of $\x=(x_1,x_2,\ldots,x_n)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Our next example as a vector space may be slightly surprising at first, but it is both highly important and an excellent example of a vector space whose vectors are certainly not arrows of any kind. First, we will need the following definition.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $\F$ denote a field and let $n$ be a natural number. A &lt;strong&gt;polynomial&lt;/strong&gt; with coefficients $a_0,a_1,\ldots,a_n$ in $\F$ is a function $p:\F\to\F$ of the form&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
p(x) &amp;amp;= \sum_{i=0}^n a_ix^i \\&lt;br&gt;
&amp;amp;= a_0 + a_1x+a_2x^2+\cdots+a_nx^n&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;for all $x\in\F$. If $a_n\ne 0$, we say that $n$ is the &lt;strong&gt;degree&lt;/strong&gt; of $p$. We write $\mathscr{P}(\F)$ to denote the set of all polynomials with coefficients in $\F$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a concrete example, the function $p:\R\to\R$ defined by&lt;/p&gt;
&lt;p&gt;$$p(x)=3+2x+8x^2$$&lt;/p&gt;
&lt;p&gt;is a polynomial of degree two with coefficients in $\R$, namely $a_0=3$, $a_1=2$ and $a_2=8$. It is thus an element of $\mathscr{P}(\R)$.&lt;/p&gt;
&lt;p&gt;If $p(x)=0$ for all $x\in\F$, we call $p$ the &lt;strong&gt;zero polynomial&lt;/strong&gt;, whose degree is undefined. However, for the purposes of adding and multiplying polynomials, it is sometimes useful to formally treat the degree of the zero polyomial as $-\infty$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; It is easy to see that for any field $\F$, the set $\mathscr{P}(\F)$ forms a vector space over $\F$ when equipped with the following definitions of vector addition and scalar multiplication.&lt;/p&gt;
&lt;p&gt;For any two vectors $\p,\q\in\mathscr{P}(\F)$, we define their sum $\p+\q\in\mathscr{P}(\F)$ in terms of function addition. That is,&lt;/p&gt;
&lt;p&gt;$$(\p+\q)(x)=\p(x)+\q(x)$$&lt;/p&gt;
&lt;p&gt;for all $x\in\F$. Similarly, for any $a\in\F$ and $\p\in\mathscr{P}(\F)$, we define scalar multiplication as expected:&lt;/p&gt;
&lt;p&gt;$$(a\p)(x)=a\p(x)$$&lt;/p&gt;
&lt;p&gt;for all $x\in\F$. It is clear that the zero polynomial must act as the zero vector. Again, checking that $\mathscr{P}(\F)$ is actually a vector space with these definitions is easy but fairly time consuming, so I won't do it here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="subspacesandsumsanamesubspacesandsums"&gt;Subspaces and Sums&lt;a name="subspaces-and-sums"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;It often happens that a vector space contains a subset which also acts as a vector space under the same operations of addition and scalar multiplication. For instance, the vector space $\{\0\}$ is a (fairly boring) subset of any vector space. This phenomenon is so important that we give it a name.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A subset $U$ of a vector space $V$ is a &lt;strong&gt;subspace&lt;/strong&gt; of $V$ if it is itself a vector space under the same operations of vector addition and scalar multiplication.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It should go without saying that $U$ and $V$ are defined over the same field.&lt;/p&gt;
&lt;p&gt;Note the unfortunate naming conflict with subspaces of topological spaces. Normally they are discussed in different contexts and so this causes no confusion. In cases where confusion may arise, we will sometimes refer to them as &lt;em&gt;topological subspaces&lt;/em&gt; and &lt;em&gt;linear subspaces&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Using this definition, certainly $\{\0\}$ constitutes a subspace of every vector space. At the other extreme, every vector space is a subspace of itself (because every set is a subset of itself). However, the concept of a subspace wouldn't be very interesting if these were the only possibilities. Before demonstrating some nontrivial proper subspaces, we provide a more straightforward method for determining whether a subset of a vector space constitutes a subspace.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; A subset $U$ of a vector space $V$ is a subspace of $V$ if and only if the following three conditions hold:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The zero vector is in $U$.&lt;/li&gt;
&lt;li&gt;The set $U$ is closed under addition of vectors.&lt;/li&gt;
&lt;li&gt;The set $U$ is closed under scalar multiplication.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Proving this would consist largely of statements like &amp;quot;the associative property holds in $U$ because it holds in $V$.&amp;quot; Therefore, I will omit the proof of this proposition.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the set $U=\{(x,0\in\F^2\mid x\in\F\}$, where $\F$ is any field. This is the set of all ordered pairs with entries in $\F$ whose second component is the additive identity.&lt;/p&gt;
&lt;p&gt;The zero vector $(0,0)$ is certainly in $U$. From the definition of addition in $\F^2$, the sum of two vectors in $U$ must be of the form&lt;/p&gt;
&lt;p&gt;$$(x,0) + (y,0) = (x+y,0)$$&lt;/p&gt;
&lt;p&gt;which is certainly in $U$. Similarly, the scalar product of any $a\in\F$ with any vector in $U$ must be of the form&lt;/p&gt;
&lt;p&gt;$$a(x,0)=(ax,0)$$&lt;/p&gt;
&lt;p&gt;which is also in $U$. We have shown that all three conditions are met, so $U$ is a subspace of $\F^2$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; If $\F$ is a field and $n$ is a natural number, then the set of all polynomials whose degree is at most $n$ forms a subspace of $\P(\F)$. We denote this subspace $\P_n(\F)$.&lt;/p&gt;
&lt;p&gt;To see that this is truly a subspace, we will verify the three conditions of the above proposition. We have already stated that the degree of the zero polynomial is $-\infty$, which is certainly less than $N$, so $\0\in\P_n(\F)$.&lt;/p&gt;
&lt;p&gt;Next, suppose that $\p,\q\in\P_n(\F)$. Then there exist degrees $j,k\le n$ and coefficients $a_0,\ldots,a_j,b_0,\ldots,b_k\in\F$ for which&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\p(x) &amp;amp;= \sum_{i=0}^j a_ix^i, \\&lt;br&gt;
\q(x) &amp;amp;= \sum_{i=0}^k b_ix^i&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;for every $x\in\F$. Since $j$ is the degree of $\p$ and $k$ is the degree of $\q$, we know by definition that $a_j,b_k\ne 0$. Suppose without loss of generality that $j\le k$. It follows that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(\p+\q)(x) &amp;amp;= \sum_{i=0}^j a_ix^i + \sum_{i=0}^k b_ix^i \\&lt;br&gt;
&amp;amp;= \sum_{i=0}^j (a_i+b_i)x^i + \sum_{i=j+1}^k b_ix^i&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;for all $x\in\F$. Note that we have combined like terms until the smaller polynomial ran out, then added terms from the higher-degree polynomial until it was also depleted.&lt;/p&gt;
&lt;p&gt;For each $0\le i\le k$, define&lt;/p&gt;
&lt;p&gt;$$c_i =&lt;br&gt;
\begin{cases}&lt;br&gt;
a_i + b_i &amp;amp; \text{if } i\le j, \\&lt;br&gt;
b_i &amp;amp; \text{if } i&amp;gt;j.&lt;br&gt;
\end{cases}$$&lt;/p&gt;
&lt;p&gt;Then $c_0,\ldots,c_k\in\F$ are the coefficients of the polynomial $\p+\q$. That is,&lt;/p&gt;
&lt;p&gt;$$(\p+\q)(x)=\sum_{i=0}^k c_i x^i.$$&lt;/p&gt;
&lt;p&gt;If $c_k\ne 0$ then $\p+\q$ is a polynomial of degree $k$. If $c_k = 0$ then $\p+\q$ is a polynomial of degree less than $k$. Since $k\le n$, it follows either way that $\p+\q\in\P(\F)$.&lt;/p&gt;
&lt;p&gt;Next, suppose that $a\in\F$ and $p\in\P(\F)$. Then there exists a degree $j\le n$ and coefficients $b_0,\ldots,b_j\in\F$ for which&lt;/p&gt;
&lt;p&gt;$$\p(x)=\sum_{i=0}^j b_i x^i$$&lt;/p&gt;
&lt;p&gt;for all $x\in\F$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
a\p(x) &amp;amp;= a\sum_{i=0}^j b_i x^i \\&lt;br&gt;
&amp;amp;= \sum_{i=0}^j ab_i x^i&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;for every $x\in\F$. If $a=0$ then clearly $a\p$ is the zero polynomial and thus $a\p=\0\in\P_n(\F)$. If $a\ne 0$ then for each $0\le i\le j$, define $c_i=ab_i$. Then $c_0,\ldots,c_j$ are the coefficients of $a\p$. That is,&lt;/p&gt;
&lt;p&gt;$$a\p(x)=\sum_{i=0}^j c_i x^i.$$&lt;/p&gt;
&lt;p&gt;Therefore, $a\p$ is a polynomial of degree $j\le n$, so $a\p\in\P(\F)$. We have established (rather painstakingly) that all three conditions hold, and so we conclude that $\P_n(\F)$ is in fact a subspace of $\P(\F)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We will now work toward defining sums of subspaces. We will begin with the following theorem, which establishes that the intersection of subspaces is always a subspace.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $U$ and $W$ are subspaces of a vector space $V$, their intersection  $U\cap V$ is also a subspace of $V$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We will again show that the conditions of the proposition hold. Since $U$ and $W$ are subspaces, we know that they both contain the zero vector and are closed under vector addition and scalar multiplication.&lt;/p&gt;
&lt;p&gt;Since $\0\in U$ and $\0\in W$, certainly $\0\in U\cap W$.&lt;/p&gt;
&lt;p&gt;For any vectors $\u,\v\in U\cap W$, we have that $\u,\v\in U$ and $\u,\v\in W$. Since both subspaces are closed under vector addition, $u+v\in U$ and $u+v\in W$. Thus, $\u+\v\in U\cap W$.&lt;/p&gt;
&lt;p&gt;Lastly, suppose $a\in\F$ and $\v\in U\cap W$. Then $\v\in U$ and $\v\in W$, so $a\v\in U$ and $a\v\in W$ because both subspaces are closed under scalar multiplication. It follows that $a\v\in U\cap W$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Using an inductive argument, it is easy to show that this result can be extended to any finite intersection of subspaces.&lt;/p&gt;
&lt;p&gt;Naturally, we might now consider the question of whether the union of subspaces is a subspaces. A moment's thought will probably be enough to convince you that this is not true in general. The following example shows why.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the subspaces&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U &amp;amp;= \{(x,0)\in\R^2\mid x\in\R\}, \\&lt;br&gt;
W &amp;amp;= \{(0,y)\in\R^2\mid y\in\R\}&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;of $\R^2$. We can picture $U$ as the space of all vectors lying on the horizontal axis of the cartesian plane, and $W$ as the space of all vectors lying on the vertical axis. The union of these subspaces, $U\cup W$, is not closed under vector addition. To illustrate this, notice that $(1,0)\in U$ and $(0,1)\in W$, and that $(1,0)+(0,1)=(1,1)$. However, $(1,1)\notin U$ and $(1,1)\notin W$, so certainly $(1,1)\notin U\cup W$. Therefore, $U\cup W$ is not a subspace of $\R^2$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is somewhat unfortunate behavior, because intersections and unions of sets are very natural and easy to work with. We would therefore like to define a way to combine subspaces in order to obtain a new subspace which is &lt;em&gt;as close as possible&lt;/em&gt; to their union. We mean this in the sense that this new subspace should be the smallest subspace containing their union. To this end, we define the sum of subspaces in the following manner.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $U$ and $W$ be subspaces of a vector space $V$. The &lt;strong&gt;sum&lt;/strong&gt; of $U$ and $W$ is the set&lt;/p&gt;
&lt;p&gt;$$U+W=\{\u+\w\in V\mid\u\in U,\w\in W\}.$$&lt;/p&gt;
&lt;p&gt;Similarly, the sum of subspaces $U_1,\ldots,U_n$ of $V$ is the set&lt;/p&gt;
&lt;p&gt;$$\sum_{i=1}^n U_i = \{\u_1 + \cdots + u_n\in V\mid \u_1\in U_1,\ldots,\u_n\in U_n\}.$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This definition certainly fixes the issue we had with unions of subspaces, because it ensures by its very definition that it contains the sum of any vector in $U$ and any vector in $W$. We should make sure that the sum of subspaces is actually a subspace.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $U$ and $W$ be subspaces of a vector space $V$, their sum $U+W$ is also a subspace of $V$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $U$ and $W$ are subspaces, we know that they both contain the zero vector and are closed under vector addition and scalar multiplication.&lt;/p&gt;
&lt;p&gt;Since $\0\in U$ and $\0\in W$, clearly $\0\in U+W$ because $\0=\0+\0$, i.e., it is the sum of a vector in $U$ and a vector in $W$.&lt;/p&gt;
&lt;p&gt;Next, let $\v_1,\v_2\in U+W$. Then&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\v_1 &amp;amp;= \u_1+\w_1, \\&lt;br&gt;
\v_2 &amp;amp;= \u_2+\w_2&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;for some vectors $\u_1,\u_2\in U$ and $\w_1,\w_2\in W$. Since $U$ and $W$ are closed under vector addition, we know that $\u_1+\u_2\in U$ and $\w_1+\w_2\in W$. Therefore,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\v_1+\v_2 &amp;amp;= (\u_1+\w_1) + (\u_2+\w_2) \\&lt;br&gt;
&amp;amp;= (\u_1+\u_2) + (\w_1+\w_2) \\&lt;br&gt;
&amp;amp;\in U+W&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;since it is the sum of a vector in $U$ and a vector in $W$.&lt;/p&gt;
&lt;p&gt;Finally, let $a\in\F$ and $\v\in U+W$, so that $\v=\u+\w$ for some $\u\in U$ and some $\w\in W$. Since $U$ and $W$ are closed under scalar multiplication, we know that $a\u\in U$ and $a\w\in W$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
a\v &amp;amp;= a(\u+\w) \\&lt;br&gt;
&amp;amp;= a\u + a\w \\&lt;br&gt;
&amp;amp;\in U+W&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;since it is the sum of a vector in $U$ and a vector in $W$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that we have established that the sum of two subspaces is a subspace, we will prove our assertion above — that the sum of two subspaces is the smallest subspace containing their union. We do this by showing that any subspace containing their union must also contain their sum.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $U_1$ and $U_2$ are subspaces of a vector space $V$, then every subspace containing $U_1\cup U_2$ also contains $U_1+U_2$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose $W$ is a subspace of $V$ for which $U_1\cup U_2\subseteq W$, and let $\v\in U_1+U_2$. Clearly $\v=\u_1+\u_2$ for some $\u_1\in U_1$ and some $\u_2\in U_2$, and so $\v\in W$ since $U_1\cup U_2\subseteq W$ and $W$ must be closed under vector addition since it is a subspace of $V$. Thus, $U_1+U_2\subseteq W$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above is equivalent to saying that $U_1+U_2$ is the intersection of all subspaces containing $U_1\cup U_2$. Again, this result extends to any finite number of subspaces via a simple inductive argument.&lt;/p&gt;
&lt;p&gt;This verifies that the sum of subspaces acts as we originally intended, in that it is as close to their union as possible while remaining a subspace. Now that we have demonstrated this nice characterization of sums, I will provide an example before moving on.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the subspaces&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U_1 &amp;amp;= \{(x,0,0)\in\R^3\mid x\in\R\}, \\&lt;br&gt;
U_2 &amp;amp;= \{(0,y,0)\in\R^3\mid y\in\R\}, \\&lt;br&gt;
U_3 &amp;amp;= \{(0,0,z)\in\R^3\mid z\in\R\}&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;of the vector space $\R^3$. These correspond to the subspaces of vectors lying along the $x$, $y$ and $z$ axes, respectively. The sum of these three subspaces is $\R^3$ itself, because for any vector $(x,y,z)\in\R^3$, we may write&lt;/p&gt;
&lt;p&gt;$$(x,y,z)=(x,0,0)+(0,y,0)+(0,0,z),$$&lt;/p&gt;
&lt;p&gt;which is a sum of vectors in $U_1$, $U_2$ and $U_3$, respectively. This sum has a particularly nice property — namely that every vector in $\R^3$ has a &lt;em&gt;unique&lt;/em&gt; representation as a sum of vectors in $U_1$, $U_2$ and $U_3$. Put another way, this sum is special because we are forced to express $(x,y,z)$ as above — there is no other way to break it down as such a sum. This is really why we choose to express points in $\R^3$ using these coordinates. In fact, this property of sums is so important that it has a name. But I will discuss direct sums next time, since it's very late and I'm a sleepyhead.&lt;/p&gt;
&lt;/blockquote&gt;
</content:encoded></item><item><title>Free Abelian Groups</title><description>Essentially, free abelian groups give us a rigorous way of talking about formal linear combinartions of some set of generators.</description><link>http://localhost:2368/free-abelian-groups/</link><guid isPermaLink="false">5c8732fffdce06003e4aef59</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Tue, 12 Mar 2019 04:54:30 GMT</pubDate><content:encoded>&lt;ol&gt;
&lt;li&gt;&lt;a href="#cyclic-groups"&gt;Cyclic Groups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#direct-sums"&gt;Direct Sums&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#free-abelian-groups"&gt;Free Abelian Groups&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;This is gonna be a long one. And before we can even talk about free abelian groups we'll need a few definitions. Cyclic groups and direct sums are interesting in their own right. However, I will spend as little time on them as possible right now, using them only as a means to an end. So consider this a speed-run through the machinery needed to construct and effectively use free abelian groups.&lt;/p&gt;
&lt;h3 id="cyclicgroupsanamecyclicgroups"&gt;Cyclic Groups&lt;a name="cyclic-groups"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; If $G$ is a group and $x\in G$, then the &lt;strong&gt;cyclic subgroup&lt;/strong&gt; of $G$ &lt;strong&gt;generated by $x$&lt;/strong&gt; is the set&lt;/p&gt;
&lt;p&gt;$$\inner{x}=\{x^n\mid n\in\Z\}.$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So basically a cyclic subgroup consists of everything you can get by multiplying a single element repeatedly. Let's look at a few examples:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Let's look at some cyclic subgroups of the infinite group $\Z$ of integers whose operation is the usual addition. Since this is an abelian group, we will be using additive notation in this example, so $x^n$ becomes $nx$ and $ab$ becomes $a+b$.&lt;/p&gt;
&lt;p&gt;The cyclic subgroup of $\Z$ generated by $0$ is the set&lt;/p&gt;
&lt;p&gt;$$\inner{0}=\{0n\mid n\in\Z\},$$&lt;/p&gt;
&lt;p&gt;which is really just $\{0\}$, the trivial group.&lt;/p&gt;
&lt;p&gt;The cyclic subgroup of $\Z$ generated by $1$ is the set&lt;/p&gt;
&lt;p&gt;$$\inner{1}=\{1n\mid n\in\Z\},$$&lt;/p&gt;
&lt;p&gt;which is quite clearly just $\Z$ itself.&lt;/p&gt;
&lt;p&gt;The cyclic subgroup of $\Z$ generated by $2$ is the set&lt;/p&gt;
&lt;p&gt;$$\inner{2}=\{2n\mid n\in\Z\},$$&lt;/p&gt;
&lt;p&gt;which is the set $2\Z$ of multiples of two.&lt;/p&gt;
&lt;p&gt;It shouldn't be too hard to see that in general,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\inner{k} &amp;amp;= \{kn\mid n\in\Z\} \\&lt;br&gt;
&amp;amp;= \abs{k}\Z,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;which is the set of integer multiples of $k$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Now let's look at a finite group. We'll use $D_3$, the dihedral group of the triangle, which I introduced in my &lt;a href="http://localhost:2368/groups-and-their-basic-properties/#examples"&gt;first post about groups&lt;/a&gt;. Recall that&lt;/p&gt;
&lt;p&gt;$$D_3 = \{1,r_1,r_2,f_1,f_2,f_3\},$$&lt;/p&gt;
&lt;p&gt;where $1$ is the identity transformation, $r_1$ and $r_2$ are rotations and $f_1,f_2$ and $f_3$ are reflections. Recall also here that the group operation is composition, so that $f_1\circ r_2$ is the result of first rotating via $r_2$ and then reflecting via $f_1$.&lt;/p&gt;
&lt;p&gt;To make the following discussion simpler, I have computed the entire group table below:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/03/D_3-multiplication-table-2.svg" alt="D_3-multiplication-table-2"&gt;&lt;/p&gt;
&lt;p&gt;The section I've highlighted in red is the subgroup of rotations, $\{1,r_1,r_2\}$. Recall that the reflections do not form a subgroup, since the composition of two reflections is usually a rotation.&lt;/p&gt;
&lt;p&gt;The cyclic subgroup of $D_3$ generated by $1$ is the set&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\inner{1} &amp;amp;= \{1^n\mid n\in\Z\} \\&lt;br&gt;
&amp;amp;= \{1\},&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;since composing the identity with itself always results in the identity no matter how many times we do it.&lt;/p&gt;
&lt;p&gt;The cyclic subgroup of $D_3$ generated by $r_1$ is the set&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\inner{r_1} &amp;amp;= \{r_1^n\mid n\in\Z\} \\&lt;br&gt;
&amp;amp;= \{\ldots, r_1^0, r_1^1, r_1^2, \ldots\} \\&lt;br&gt;
&amp;amp;= \{1,r_1,r_2\},&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;the subgroup of rotations. This cyclic subgroup has order three, because once we reach $r_2$, composing with $r_1$ again gets us back to the identity.&lt;/p&gt;
&lt;p&gt;The cyclic subgroup of $D_3$ generated by $r_2$ is the same as the above, since $r_2^0=1$, $r_2^1=r_2$, $r_2^2=r_1$ and $r_2^3=1$, bringing us right back to where we started.&lt;/p&gt;
&lt;p&gt;The cyclic subgroup of $D_3$ generated by $f_1$ is the set&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\inner{f_1} &amp;amp;= \{f_1^n\mid n\in\Z\} \\&lt;br&gt;
&amp;amp;= \{\ldots, f_1^0, f_1^1, f_1^2, \ldots\} \\&lt;br&gt;
&amp;amp;= \{1, f_1\}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;That's because $f_1$ is its own inverse! The other reflections exhibit the same sort of behavior since they are also each their own inverse.&lt;/p&gt;
&lt;p&gt;So to wrap things up, $D_3$ has five cyclic subgroups, one of order $1$, one of order $3$, and three of order $2$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I will not bother proving that cyclic subgroups are actually subgroups, or that they are always abelian, since the proofs of these facts are basically encompassed in the definition of cyclic subgroups.&lt;/p&gt;
&lt;p&gt;Another important thing to notice is the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note.&lt;/strong&gt; All cyclic groups of the same order are isomorphic.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In addition to finite groups, the above additionally implies that all infinite cyclic groups are isomorphic! This means that any infinite cyclic group is basically $\Z$.&lt;/p&gt;
&lt;h3 id="directsumsanamedirectsums"&gt;Direct Sums&lt;a name="direct-sums"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Given a collection of abelian groups, it is often convenient to form a new group which combines them in the following manner:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given a collection of abelian groups $\{G_i\}_{i\in I}$ their &lt;strong&gt;direct sum&lt;/strong&gt; is defined as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The underlying set is the cartesian product $\displaystyle\prod_{i\in I}G_i$.&lt;/li&gt;
&lt;li&gt;The group operation $+$ is given componentwise:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$(g_1,g_2,g_3,\ldots) + (h_1,h_2,h_3,\ldots) = (g_1 + h_1, g_2 + h_2, g_3 +h_3,\ldots),$$&lt;/p&gt;
&lt;p&gt;where addition in the $i$th slot represents the group operation in $G_i$.&lt;/p&gt;
&lt;p&gt;The resulting group is denoted $\displaystyle\bigoplus_{i\in I}G_i$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Since I am speed-running this, I will not bother proving that the direct sum of abelian groups is an abelian group. The proof is very straightforward — it essentially inherits its group properties from the groups it is made from.&lt;/p&gt;
&lt;p&gt;I will, however, give one example.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Let's look at the direct sum of $\Z$ and $2\Z$. This group consists of all ordered pairs of the form $(n, 2m)$, where $n$ and $m$ are integers. That is, all ordered pairs where the first component is any integer and the second is any even integer.&lt;/p&gt;
&lt;p&gt;Given two elements $(n_1, 2m_1)$ and $(n_2, 2m_2)$ of the direct sum $\Z\oplus 2\Z$, their sum is $$(n_1+n_2, 2(m_1+m_2)).$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="freeabeliangroupsanamefreeabeliangroups"&gt;Free Abelian Groups&lt;a name="free-abelian-groups"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Essentially, free abelian groups give us a rigorous way of talking about formal linear combinartions of some set of generators. I will explain what I mean by this in a bit more detail laters on.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $B$ denote a subset of an abelian group $F$. Then $F$ is a &lt;strong&gt;free abelian group&lt;/strong&gt; with &lt;strong&gt;basis&lt;/strong&gt; $B$ if $F=\displaystyle\bigoplus_{b\in B}\inner{b}$ and the cyclic subgroup $\inner{b}$ is infinite for every $b\in B$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is easy to see that free abelian groups are always isomorphic to direct sums of copies of $\Z$, since all infinite cyclic groups are isomorphic to the integers. Furthermore, if $F$ is free abelian then any element $x\in F$ can be written uniquely as&lt;/p&gt;
&lt;p&gt;$$x=\sum_{b\in B}m_b b,$$&lt;/p&gt;
&lt;p&gt;where $m_b\in\Z$ and all but a finite number of the $m_b$ are zero.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Let $B=\{\text{cat},\text{mouse},\text{sheep}\}$. Then any element in the free abelian group with basis $B$ can be written&lt;/p&gt;
&lt;p&gt;$$a\cdot\text{cat} + b\cdot\text{mouse} + c\cdot\text{sheep},$$&lt;/p&gt;
&lt;p&gt;where $a,b$ and $c$ are integers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The next theorem is incredibly important. If you are familiar with linear algebra, it shows us that homomorphisms between free abelian groups behave very similarly to linear maps between vector spaces, in that they are determined entirely by how they act on their bases.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $F$ be a free abelian group with basis $B$. For any abelian group $G$ and any function $f':B\to G$, there exists a unique homomorphism $f:F\to G$ for which $f\restriction{B}=f'$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose $x\in F$, so that $x=\displaystyle\sum_{b\in B}m_b b$ for some integers $\{m_b\}_{b\in B}$. Define&lt;/p&gt;
&lt;p&gt;$$f(x)=\sum_{b\in B}m_b f'(b).$$&lt;/p&gt;
&lt;p&gt;This is clearly a well-defined homomorphism because the expression for $x$ is unique (by definition). In addition, $f$ is unique because two homomorphisms that agree on generators must be equal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The above construction of $f$ from $f'$ is called &lt;strong&gt;extending by linearity&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is not difficult to show that any two bases for the same free abelian group must have the same cardinality. This cardinality is akin to the dimension of a vector space, but we use a different name for it.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; If $F$ is a free abelian group with basis $B$, then the &lt;strong&gt;rank&lt;/strong&gt; of $F$ is an invariant equal to the cardinality of $B$. That is, $\text{rank }F=\abs{B}$, and this does not depend on our choice of basis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I'm going to end the post here and save the good stuff for next time. In particular, I still need to talk about matrices for homomorphisms between free abelian groups, Smith Normal Form, and the Fundamental Theorem of Finitely Generated Abelian Groups.&lt;/p&gt;
&lt;p&gt;Until next time :D&lt;/p&gt;
</content:encoded></item><item><title>The First Isomorphism Theorem</title><description>It would be nice if there was some sort of relationship between cosets of the kernel and the image of a homomorphism. Oh wait... there is!</description><link>http://localhost:2368/the-first-isomorphism-theorem/</link><guid isPermaLink="false">5c84a734fdce06003e4aeefc</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Sun, 10 Mar 2019 06:34:06 GMT</pubDate><content:encoded>&lt;p&gt;In the study of group theory, there are a few important theorems called the First, Second and Third Isomorphism Theorems. The second and third are really just special cases of the first, and they will not be immediately useful to us so I will put off their discussion until some other time.&lt;/p&gt;
&lt;p&gt;Before proceeding any further, I would like to make a notational change that will make my life a little easier when writing these posts about algebra. Thus far, I have been using $e$ to denote the identity element of a group. From here on out, I will use the following convention:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Notation.&lt;/strong&gt; Given an arbitrary group $G$, its identity element will be written $1_G$ (or simply $1$ when no confusion can arise).&lt;/p&gt;
&lt;p&gt;If we know we are working with an additive abelian group, the identity will instead be written $0_G$ (or simply $0$).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This notational convention may seem odd at first, but it will make things easier to understand in general.&lt;/p&gt;
&lt;p&gt;We will need the result I proved in my &lt;a href="http://localhost:2368/normal-subgroups-and-quotient-groups/"&gt;previous post&lt;/a&gt; that the kernel of a group homomorphism is a normal subgroup of the domain. We will also need the following theorem, which basically states that for any homomorphism, elements in the same coset of the kernel all get mapped to the same element.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $f:G\to H$ be a group homomorphism with $x,y\in G$ and let $K=\ker f$ to keep things concise. Then $Kx=Ky$ if and only if $f(x)=f(y)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose first that $Kx=Ky$. Then $xy^{-1}\in K=\ker f$, so&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f(xy^{-1}) &amp;amp;= f(x)f(y^{-1}) \\&lt;br&gt;
&amp;amp;= f(x)f(y)^{-1} \\&lt;br&gt;
&amp;amp;= 1_H.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Multiplication on the right by $f(y)$ yields $f(x)=f(y)$.&lt;/p&gt;
&lt;p&gt;Suppose conversely that $f(x)=f(y)$. Then we can essentially just do the above in reverse. Multiplication on the right by $f(y)^{-1}$ yields&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f(x)f(y)^{-1} &amp;amp;= f(x)f(y^{-1})\\&lt;br&gt;
&amp;amp;= f(xy^{-1}) \\&lt;br&gt;
&amp;amp;= 1_H,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;and thus $xy^{-1}\in\ker f=K$ by definition, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This theorem, while deceptively simple, tells us something extremely important. Namely, that each coset of the kernel corresponds to precisely one element of the codomain. It would be nice if there was some sort of relationship between cosets of the kernel and the image of a homomorphism. Oh wait... there is!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;First Isomorphism Theorem.&lt;/strong&gt; If $f:G\to H$ is a group homomorphism, then $G/\ker f$ is isomorphic to $\im f$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Our claim is that the function $\varphi:G/\ker f\to\im f$ defined by $$\varphi(Kx)=f(x)$$ for every $Kx\in G/\ker f$ is an isomorphism. Again for convenience, we let $K=\ker f$.&lt;/p&gt;
&lt;p&gt;First, since $\varphi$ acts on cosets we must show that it is well defined, i.e., if $Kx=Ky$ then $\varphi(Kx)=\varphi(Ky)$. But from the above theorem, if $Kx=Ky$ then $f(x)=f(y)$, so it follows immediately that this is true.&lt;/p&gt;
&lt;p&gt;Next we argue that $\varphi$ is a homomorphism. Again, choose cosets $Kx$ and $Ky$ in $G/K$. Then&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\varphi(Kx)\varphi(Ky) &amp;amp;= f(x)f(y) \\&lt;br&gt;
&amp;amp;= f(xy) \\&lt;br&gt;
&amp;amp;= \varphi(K(xy))&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;because $f$ is a homomorphism, so $\varphi$ essentially inherits this property from $f$.&lt;/p&gt;
&lt;p&gt;Finally, we need to show that $\varphi$ is a bijection. To see that it is injective, suppose that $\varphi(Kx)=\varphi(Ky)$ for some cosets $Kx$ and $Ky$ in $G/K$. Then $f(x)=f(y)$, and so $Kx=Ky$ by the above theorem. To see that $\varphi$ is surjective, choose any $b\in\im f$. By definition, there exists $a\in G$ for which $f(a)=b$. Thus $b=\varphi(Ka)$, and so it follows that $\varphi$ is bijective, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I'm trying to keep this post short, but I should provide at least one example of how this theorem can be applied.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the abelian group $\R^*$ of nonzero real numbers under multiplication, the group $\R^+$ of positive real numbers under multiplication, and the subgroup $\{-1,1\}$ of $\R^*$. This subgroup is certainly normal since it comes from an abelian group.&lt;/p&gt;
&lt;p&gt;We will show that the quotient group $\frac{\R^*}{\{-1,1\}}$ is isomorphic to $\R^+$. And we'll do it using the first isomorphism theorem!&lt;/p&gt;
&lt;p&gt;We need to define a surjective homomorphism $f:\R^*\to \R^+$ for which $\ker f=\{-1,1\}$. This is actually really easy. We'll just let $f(x)=\abs{x}$. Clearly $f(x)=1$ precisely when either $x=1$ or $x=-1$, and so $\ker f=\{-1,1\}$ as desired. That $f$ is a homomorphism follows from the fact that $\abs{xy}=\abs{x}\abs{y}$ for all real numbers. It's just as easy to see that $f$ is surjective, because for any $x\in\R^+$ we know that at least $\abs{x}=x$.&lt;/p&gt;
&lt;p&gt;We've demonstrated that there exists a surjective homomorphism from $\R^*$ to $\R^+$ whose kernel is $\{-1,1\}$. The first isomorphism theorem immediately yields the desired result — that $\frac{\R^*}{\{-1,1\}}$ is isomorphic to $\R^+$.&lt;/p&gt;
&lt;p&gt;We don't even need to construct the isomorphism, although it's fairly straightforward to do so (the proof of the first isomorphism theorem tells us how).&lt;/p&gt;
&lt;p&gt;And intuitively, these groups &lt;em&gt;should&lt;/em&gt; be isomorphic, if we think of the quotient group $\frac{\R^*}{\{-1,1\}}$ in the correct way. Basically, $\R^*$ is a gluing together of two copies of $\R^+$ — one copy containing all the positive numbers, and the other a mirror image containing all the negative numbers. The quotient group essentially &amp;quot;factors out&amp;quot; the sign of each number, leaving us with only one copy. The result is something that looks, acts and feels exactly like $\R^+$.&lt;/p&gt;
&lt;/blockquote&gt;
</content:encoded></item><item><title>Normal Subgroups and Quotient Groups</title><description>Let's now revisit the quotient set $G/H$, where $H$ is a subgroup of $G$. What we'd really like to do is turn $G/H$ into a group in a meaningful way. What should the group operation be, though?</description><link>http://localhost:2368/normal-subgroups-and-quotient-groups/</link><guid isPermaLink="false">5c7f40876fe30a003ed53402</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Wed, 06 Mar 2019 04:52:39 GMT</pubDate><content:encoded>&lt;ol&gt;
&lt;li&gt;&lt;a href="#normal-subgroups"&gt;Normal Subgroups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#quotient-groups"&gt;Quotient Groups&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="normalsubgroupsanamenormalsubgroups"&gt;Normal Subgroups&lt;a name="normal-subgroups"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Let's now revisit the quotient set $G/H$, where $H$ is a subgroup of $G$. What we'd really like to do is turn $G/H$ into a group in a meaningful way. What should the group operation be, though? Recall that the elements of $G/H$ are the cosets of $H$, so what we really need to define is a meaningful way to multiply cosets.&lt;/p&gt;
&lt;p&gt;It would be great if we could define coset multiplication as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Desired Definition.&lt;/strong&gt; If $H$ is a subgroup of $G$ and $x,y\in G$, then&lt;br&gt;
$$(Hx)(Hy)=H(xy).$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Unfortunately, this is not always uniquely defined. That is, if $Hx=Ha$ and $Hy=Hb$ for some $a,b\in G$, it is possible that $H(xy)\ne H(ab)$, which means this isn't an acceptable way to multiply cosets. This is the same behavior we saw when I proposed an incorrect definition of addition while &lt;a href="http://localhost:2368/constructing-the-rational-numbers-1/#construction"&gt;constructing the rational numbers&lt;/a&gt;. The result cannot depend on our choice of representative for the equivalence class!&lt;/p&gt;
&lt;p&gt;Interestingly, in this case the remedy is not to define coset multiplication in a different manner. Rather, we choose to restrict our attention only to cosets of a certain, particularly nice sort of subgroup.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A subgroup $N$ of a group $G$ is a &lt;strong&gt;normal subgroup&lt;/strong&gt; if $xnx^{-1}\in N$ whenever $n\in N$ and $x\in G$. We refer to this defining property of normal subgroups by saying they are &lt;strong&gt;closed under conjugation&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It goes without saying that every subgroup of an abelian group is normal, since in that case&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
xnx^{-1} &amp;amp;= xx^{-1}n \\&lt;br&gt;
&amp;amp;= n,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;which is in $N$ by definition. However, there are certainly non-abelian groups with normal subgroups. And normal subgroups are particularly special because their left and right cosets are the same, even if they are not subgroups of an abelian group!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $N$ is a normal subgroup of a group $G$, then $xN=Nx$ for every $x\in G$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; For any $g\in xN$, we have that $g=xn$ for some $n\in N$. Since $N$ is closed under conjugation, $xnx^{-1}\in N$ and so $g=(xnx^{-1}x)\in Nx$. Thus, $xN\subseteq Nx$.&lt;/p&gt;
&lt;p&gt;The proof that $Nx\subseteq xN$ is nearly identical, so I will not include it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In a recent post I showed that the kernel of a homomorphism is a always a subgroup of its domain. It turns out that they are actually &lt;em&gt;normal&lt;/em&gt; subgroups, and this will be very important to us later on.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; The kernel of any group homomorphism is a normal subgroup of the domain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Let $G$ and $H$ be groups and let $f:G\to H$ denote a homomorphism between them. Suppose $x\in G$ and $k\in\ker f$. We need to show that $xkx^{-1}\in\ker f$.&lt;/p&gt;
&lt;p&gt;Since $k\in\ker f$, we have by definition that $f(k)=e$. Furthermore, since $f$ is a homomorphism,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f(xkx^{-1}) &amp;amp;= f(x)f(k)f(x^{-1}) \\&lt;br&gt;
&amp;amp;= f(x)ef(x)^{-1} \\&lt;br&gt;
&amp;amp;= f(x)f(x)^{-1} \\&lt;br&gt;
&amp;amp;= e.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Thus, $xkx^{-1}$ is in $\ker f$ by definition, so $\ker f$ is a normal subgroup of $G$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="quotientgroupsanamequotientgroups"&gt;Quotient Groups&lt;a name="quotient-groups"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We are now ready to define quotient &lt;em&gt;groups&lt;/em&gt;!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $N$ denote a normal subgroup of a group $G$. The &lt;strong&gt;quotient group of $G$ modulo $N$&lt;/strong&gt; is the set $G/N$ together with coset multiplication defined by $(Nx)(Ny)=N(xy)$ for all $x,y\in G$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's all well and good, but we're getting ahead of ourselves again. We still haven't actually shown that this multiplication is well defined when restricted to cosets of normal subgroups. Let's do that right now. After that, we'll still need to prove that this thing we'ved defined is really a group.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $N$ denote a normal subgroup of a group $G$, with $Nx=Na$ and $Ny=Nb$ for some $a,b,x,y\in G$. Then $N(xy)=N(ab)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $Nx=Na$, we know that $x\in Na$ and thus $x=n_1a$ for some $n_1\in N$. Similarly, since $Ny=Nb$, we know that $y\in Nb$ and thus $y=n_2b$ for some $n_2\in N$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
xy &amp;amp;= n_1an_2b \\&lt;br&gt;
&amp;amp;\in NaNb \\&lt;br&gt;
&amp;amp;= N(ab),&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;so $N(xy)=N(ab)$, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now we need to show that quotient groups are actually groups. The proof of this is fairly straightforward.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; The quotient group as defined above is in fact a group.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We have already shown that coset multiplication is well defined. We will show first that it is associative. Consider $Nx, Ny, Nz\in G/N$. By definition,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
Nx(NyNz) &amp;amp;= NxN(yz) \\&lt;br&gt;
&amp;amp;= N(xyz) \\&lt;br&gt;
&amp;amp;= N(xy)Nz \\&lt;br&gt;
&amp;amp;= (NxNy)Nz.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Next, we will show that $N$ is the identity element of $G/N$. Because $N$ is a subgroup of $G$, the identity element $e\in G$ is in $N$. Thus, for any $Nx\in G/N$, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
NNx &amp;amp;= NeNx \\&lt;br&gt;
&amp;amp;= N(ex) \\&lt;br&gt;
&amp;amp;= Nx \\&lt;br&gt;
&amp;amp;= N(xe) \\&lt;br&gt;
&amp;amp;= NxNe \\&lt;br&gt;
&amp;amp;= NxN.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Finally, we will show that for any $Nx\in G/N$, the coset $Nx^{-1}$ is its inverse. This is clear because&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
NxNx^{-1} &amp;amp;= N(xx^{-1}) \\&lt;br&gt;
&amp;amp;= Ne \\&lt;br&gt;
&amp;amp;= N(x^{-1}x) \\&lt;br&gt;
&amp;amp;= Nx^{-1}Nx.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;We have shown that $G/N$ has all the properties of a group, so the proof is complete.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Before moving on, let's look at a concrete example of a quotient group which is hopefully already familiar to you.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider again the group $\Z$ of integers under addition and its subgroup $2\Z$ of even integers. Certainly $2\Z$ is a normal subgroup because $\Z$ is abelian, and we may thus form the quotient group $\Z/2\Z$. Recall that this quotient group contains only two cosets, namely $2\Z$ and $2\Z+1$.&lt;/p&gt;
&lt;p&gt;Coset &amp;quot;multiplication&amp;quot; here is really coset addition because we are working in an additive group. Since $2\Z$ is the identity, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
2\Z+2\Z &amp;amp;= 2\Z, \\&lt;br&gt;
2\Z + (2\Z+1) &amp;amp;= 2\Z+1,\\&lt;br&gt;
(2\Z+1) + 2\Z &amp;amp;= 2\Z+1, \\&lt;br&gt;
(2\Z+1) + (2\Z+1) &amp;amp;= 2\Z.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Let's rename these cosets, just for kicks. We'll refer to $2\Z$ as $0$ and $2\Z+1$ as $1$. Then we get&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
0 + 0 &amp;amp;= 0, \\&lt;br&gt;
0 + 1 &amp;amp;= 1,\\&lt;br&gt;
1 + 0 &amp;amp;= 1, \\&lt;br&gt;
1 + 1 &amp;amp;= 0.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;This is precisely the group $\Z_2$ of integers modulo $2$, with the operation of addition modulo $2$. In fact, the formal way to define $\Z_n$, the group of integers modulo $n$, is as the quotient group $\Z/n\Z$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now I'd like to give some motivation for why quotient groups are so important and useful. This is best done by example, because otherwise my explanation would likely turn into another incomprehensible rant.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; To start, we define a &lt;strong&gt;commutator&lt;/strong&gt; of a group $G$ to be any element of the form $aba^{-1}b^{-1}$, where $a,b\in G$. Notice that $aba^{-1}b^{-1}$ reduces to the identity $e$ if and only if $ab=ba$. That is, the commutator $aba^{-1}b^{-1}$ collapses to the identity precisely when $a$ and $b$ commute. Clearly in any abelian group, every commutator is equal to the identity element.&lt;/p&gt;
&lt;p&gt;Suppose a quotient group $G/N$ is abelian. That is, for any two cosets $Nx, Ny\in G/N$, their product commutes, i.e.,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
NxNy &amp;amp;= N(xy) \\&lt;br&gt;
&amp;amp;= N(yx) \\&lt;br&gt;
&amp;amp;= NyNx.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;But this is true if and only if&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
xy(yx)^{-1} &amp;amp;= xyx^{-1}y^{-1} \\&lt;br&gt;
&amp;amp;\in N.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Thus, a quotient group is abelian precisely when all commutators are contained within the identity coset!&lt;/p&gt;
&lt;p&gt;A slightly more liberal, though meaningful, way of phrasing this is by saying that if we &lt;em&gt;factor out&lt;/em&gt; all the commutators of $G$, we are always left with an abelian group. This because all commutators are collapsed to the identity element in the quotient group when we quotient out the commutator subgroup. This is just one example of a more general phenomenon.&lt;/p&gt;
&lt;/blockquote&gt;
</content:encoded></item><item><title>Cosets and Lagrange's Theorem</title><description>It's a bit difficult to explain exactly why cosets are so important without working with them for a while first. But as you'll hopefully start to understand within my next few posts, cosets pop up everywhere and are a necessary tool to get anything done in the world of algebra.</description><link>http://localhost:2368/cosets-and-lagranges-theorem/</link><guid isPermaLink="false">5c7df11d6fe30a003ed533c6</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Tue, 05 Mar 2019 03:53:03 GMT</pubDate><content:encoded>&lt;ol&gt;
&lt;li&gt;&lt;a href="#cosets"&gt;Cosets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lagrange's-theorem"&gt;Lagrange's Theorem&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="cosetsanamecosets"&gt;Cosets&lt;a name="cosets"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;It's a bit difficult to explain exactly why cosets are so important without working with them for a while first. But as you'll hopefully start to understand within my next few posts, cosets pop up everywhere and are a necessary tool to get anything done in the world of algebra. Let's dig in, shall we?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $H$ denote a subgoup of a group $G$. For any fixed element $x\in G$, the &lt;strong&gt;right coset&lt;/strong&gt; of $H$ with respect to $x$ is the set $Hx=\{hx\in G\mid h\in H\}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $H$ denote a subgoup of a group $G$. For any fixed element $x\in G$, the &lt;strong&gt;left coset&lt;/strong&gt; of $H$ with respect to $x$ is the set $xH=\{xh\in G\mid h\in H\}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note.&lt;/strong&gt; For abelian groups with additive notation, right and left cosets are instead denoted by $H+x$ and $x+H$, respectively.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So basically a coset is a set obtained by taking the elements of a subgroup and adding a particular element to all of them. Note that a coset does need not be a subgroup! To get a feeling for what cosets really are and how they behave, let's look at an example.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Take $G=\Z$, the additive group of integers, and $H=2\Z$, the set of even integers. Certainly $2\Z$ is a subgroup of $Z$ because it contains the identity $0$, the sum of two even integers is even, and every even integer has an even inverse — its negative.&lt;/p&gt;
&lt;p&gt;Let's look at a few cosets of $2\Z$. How about the right cosets with respect to $0,1,2$ and $3$? Notice that because $\Z$ is an additive group, these cosets will be written $2\Z+0$, $2\Z+1$, $2\Z+2$ and $2\Z+3$.&lt;/p&gt;
&lt;p&gt;Directly from the definition of a right coset, we see that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
2\Z+0 &amp;amp;= \{x+0\in\Z\mid x\in 2\Z\} \\&lt;br&gt;
&amp;amp;= \{x\in\Z\mid x\in 2\Z\} \\&lt;br&gt;
&amp;amp;= 2\Z.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;So in this case, the coset with respect to $0$ is just the subgroup $2\Z$. This actually always happens, as you can easily see. Adding the identity to all elements of a subgroup will of course just yield that subgroup again!&lt;/p&gt;
&lt;p&gt;Next, let's look at $2\Z+1$. This is the set $\{x+1\in\Z\mid x\in 2\Z\}$, which consists of things like $\ldots,-3,-1,1,3,5,\ldots$. This is really just the set of odd integers! It's definitely not a subgroup of $\Z$ though, since it doesn't contain $0$.&lt;/p&gt;
&lt;p&gt;The coset $2\Z+2$ is the set $\{x+2\in\Z\mid z\in 2\Z\}$, which contains elements like $\ldots,-4,-2,0,2,4,\ldots$. But this is the set of even integers again! That means $2\Z+2=2\Z$.&lt;/p&gt;
&lt;p&gt;Lastly, let's look at $2\Z+2$. This is the set $\{x+3\in\Z\mid z\in 2\Z\}$, which contains things like $\ldots,-3,-1,1,3,5,\ldots$. We've already seen that somewhere before. It's just the set of odd integers again. That is, $2\Z+3=2\Z+1$.&lt;/p&gt;
&lt;p&gt;It looks like there might actually only be two distinct cosets of $2\Z$. They are $2\Z$ itself and $2\Z+1$. Furthermore, every element of $\Z$ is in either one coset or the other, but never both (because an integer is either even or odd). So these cosets actually &lt;em&gt;partition&lt;/em&gt; $\Z$, which is a very important point. But let's not get too far ahead of ourselves.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The first thing I'd like to prove about cosets is fairly simple — if we are working in an abelian group, left and right cosets are the same!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $H$ is a subgroup of an abelian group $G$, then $H+x=x+H$ for every $x\in G$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We will proceed by demonstrating that each side is a subset of the other.&lt;/p&gt;
&lt;p&gt;We show first that $H+x\subseteq x+H$. Choose $g\in H+x$, so that $g=h+x$ for some $h\in H$. Since $G$ is abelian, $h+x=x+h$ and thus $g=x+h\in x+H$. It follows that $H+x\subseteq x+H$.&lt;/p&gt;
&lt;p&gt;The proof that $x+H\subseteq H+x$ is completely analogous to the above, so we won't bother with it. We can thus conclude that $H+x=x+H$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This implies, for instance, that we could instead write $1+2\Z$ to denote the odd integers, but to me this doesn't look right for some reason and so I usually don't. In fact, for most of our purposes we really only need to consider one variety of coset. I tend to favor right cosets.&lt;/p&gt;
&lt;p&gt;Now it's time to prove my suspicion from the above example. This is important, so pay close attention. If you don't remember what a partition is, I advise you to read &lt;a href="http://localhost:2368/equivalence-relations-and-quotient-sets/"&gt;my earlier post about them&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $H$ is a subgroup of a group $G$, then the (left/right) cosets of $H$ partition $G$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We will prove the result for right cosets, since the proof for left cosets is practically identical.&lt;/p&gt;
&lt;p&gt;It is clear that every coset $Hx$ is nonempty because the identity element $e$ is in $H$ by virtue of it being a subgroup, and thus $x=ex\in Hx$.&lt;/p&gt;
&lt;p&gt;The next thing we need to show is that cosets cover all of $G$. But is clear that $$\bigcup_{x\in G}x=G\subseteq\bigcup_{x\in G}Hx,$$&lt;/p&gt;
&lt;p&gt;and similarly that $$\bigcup_{x\in G}Hx\subseteq G=\bigcup_{x\in G}x,$$&lt;/p&gt;
&lt;p&gt;because $Hx\subseteq G$ for every $x\in G$. Thus, $G=\bigcup_{x\in G}Hx$.&lt;/p&gt;
&lt;p&gt;The last thing we need to show is that for any $x,y\in G$, if $Hx\ne Hy$ then $Hx\cap Hy=\varnothing$. That is, either cosets are the same or they are disjoint. We will argue the contrapositve, supposing that $Hx\cap Hy$ is nonempty. Then there exists some element $a\in Hx\cap Hy$. That is, $a\in Hx$ and $a\in Hy$, so $a=h_1x$ and $a=h_2y$ for some elements $h_1,h_2\in H$. It follows that $h_1x=h_2y$, and multiplication on the left by $h_1^{-1}$ show that $x=h_1^{-1}h_2y\in Hy$ because $h_1^{-1}h_2\in H$. Thus $Hx=Hy$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is pretty exciting! It means that we can form an equivalence relation $\sim$ on any group $G$, where elements are equivalent if they are in the same coset of $H$. Moreover, we can form the quotient set $G\quotient{\sim}$, whose elements are precisely the cosets of $H$. For simplicity, we generally write $G/H$ instead of to denote this quotient set, since it is so very important that we are talking about the set of cosets of $H$. We'll revisit this special type of quotient set in my next post.&lt;/p&gt;
&lt;h3 id="lagrangestheoremanamelagrangestheorem"&gt;Lagrange's Theorem&lt;a name="lagrange's-theorem"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We're actually very close to proving this famous theorem already. We just need one lemma first:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma.&lt;/strong&gt; If $H$ is a subgroup of a group $G$ and $x\in G$, there exists a bijection $f:G\to Hx$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We define a function $f:H\to Hx$ by $f(h)=hx$ for every $h\in H$. That is, the function that maps every element of $H$ to its product with $x$. Clearly $f$ is injective because if $f(h_1)=f(h_2)$ then $h_1x=h2_x$ and the right cancellation law yields $h_1=h_2$. Furthermore, $f$ is surjective because for any $y\in Hx$ we have by definition that $y=hx$ for some $h\in H$, and thus $y=f(h)$. It follows that $f$ is bijective, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Recall that a bijection exists between finite sets only if those sets contain the same number of elements. This fact, along with the lemma above, tells us that all cosets of a subgroup are the same size! We are now ready to prove Lagrange's Theorem, which is actually very easy at this point.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lagrange's Theorem.&lt;/strong&gt; If $H$ is a subgroup of a finite group $G$, then $\abs{G}$ is a multiple of $\abs{H}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We have already established that every coset of $H$ contains the same number of elements, $\abs{H}$. Since $G$ is finite, there are a finite number of distinct cosets of $H$, say $n$ of them. Because these cosets partition $G$, it follows that $\abs{G}=n\abs{H}$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is probably somewhat surprising unless you're already familiar with groups. But it's just part of the astonishing usefulness of cosets. Lagrange's Theorem is actually incredibly useful because it tells us instantly that certain things &lt;em&gt;cannot&lt;/em&gt; be subgroups of other things. For instance, a group of order $12$ cannot ever have subgroups of order $5,7,8,9,10$ or $11$. Without this theorem, you might have guessed that this was the case, but it would have been pretty tricky to prove it conclusively.&lt;/p&gt;
</content:encoded></item><item><title>Group Homomorphisms</title><description>A recurring theme in mathematics is that examining the maps between objects is indispensable to understanding those objects themselves. Of course, that depends on choosing the "correct" type of maps.</description><link>http://localhost:2368/group-homomorphisms/</link><guid isPermaLink="false">5c7b67366fe30a003ed53390</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Sun, 03 Mar 2019 05:37:37 GMT</pubDate><content:encoded>&lt;ol&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#homomorphisms"&gt;Homomorphisms&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="introductionanameintroduction"&gt;Introduction&lt;a name="introduction"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A long time ago I wrote &lt;a href="http://localhost:2368/groups-and-their-basic-properties/"&gt;my first post about group theory&lt;/a&gt;, in which I defined groups and subgroups, gave several examples of groups and proved a few of their basic but important properties. Hopefully at the very least you remember that a group is a set with an associative binary operation, an identity element and inverses for all its elements. The identity, as well as each element's invers, is unique and we can cancel like terms from both sides of equations like we generally do without thinking anyway.&lt;/p&gt;
&lt;p&gt;Before I dive in, there are just a few quick things we will need before we can talk about homomorphisms. The first is an easy way to identify whether a subset of a group is actually a subgroup.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma.&lt;/strong&gt; A subset $H$ of a group $G$ is a subgroup of $G$ if it is nonempty and if $ab^{-1}\in H$ whenever $a,b\in H$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since the operation on $H$ is inherited from $G$, it is clearly associative. Next, since $H$ is nonempty there exists $x\in H$, and thus by hypothesis $xx^{-1}=e\in H$, where $e$ is the identity element. Furthermore, since $e,x\in H$, we have that $ex^{-1}=x^{-1}\in H$, so $H$ contains inverses for all its elements. Finally, for any $a,b\in H$ we have established that $b^{-1}\in H$. So again by hypothesis, $a(b^{-1})^{-1}=ab\in H$, and so $H$ is closed under products. We have shown that $H$ is a group, and thus it is a subgroup of $G$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Notice that when I say something is &amp;quot;closed under products,&amp;quot; what this really means is that the group operation is a well defined binary operation. For multiplicative groups, the word &amp;quot;product&amp;quot; fits nicely. For abelian groups with additive notation, it doesn't make quite as much sense but I will probably slip up and use it anyway.&lt;/p&gt;
&lt;p&gt;For the most part, this lemma will simply make it a little bit faster to verify that certain things are subgroups of other groups. This will help keep future proofs nice and short.&lt;/p&gt;
&lt;p&gt;The next thing I should mention is the concept of a group's order. Because I have not talked about cardinality anywhere on my blog yet, I will only define order for finite groups. Luckily, this is all we'll need for a while.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;order&lt;/strong&gt; of a finite group is the number of elements that group contains. We denote the order of a group $G$ by $\abs{G}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's a pretty simple definition. The trivial group contains only the identity, so it is a group of order one. The dihedral group of the regular $n$-gon, written $D_n$ and which I talked about in my first post on groups, has $2n$ elements and thus $\abs{D_n}=2n$. There are an abundance of examples of interesting finite groups, and the order of a finite group is really just its size.&lt;/p&gt;
&lt;h3 id="homomorphismsanamehomomorphisms"&gt;Homomorphisms&lt;a name="homomorphisms"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A recurring theme in mathematics is that examining the maps between objects is indispensable to understanding those objects themselves. Of course, that depends on choosing the &amp;quot;correct&amp;quot; type of maps. For topological spaces, it is the continuous maps that help us to understand their structure. For groups, we have the following analogous idea.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;group homomorphism&lt;/strong&gt; between groups $G$ and $H$ is a function $f:G\to H$ such that $f(x)f(y)=f(xy)$ for all $x,y\in G$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When it is understood that I am talking about groups, I will often refer to these things simply as homomorphisms, rather than group homomorphisms.&lt;/p&gt;
&lt;p&gt;The first thing I want to stress is that &lt;em&gt;homomorphism&lt;/em&gt; and &lt;em&gt;homeomorphism&lt;/em&gt; are different words and different concepts. Homomorphisms are functions that preserve group multiplication, whereas homeomorphisms preserve open sets in topological spaces.&lt;/p&gt;
&lt;p&gt;The seconds thing I need to mention is that in the above definition, the multiplication of $f(x)$ and $f(y)$ on the left side of the equation takes place in the group $H$, whereas the multiplication of $x$ and $y$ on the right takes place in $G$. That is to say, homomorphisms are precisely the maps for which is does not matter whether we multiply elements before or after applying the map.&lt;/p&gt;
&lt;p&gt;To put it another way, we can multiply elements in $G$ and take the image of their product, or we can take their images first and then multiply them in $H$. Either way, we always get the same answer. This means that the group operation is preserved by homomorphisms. And that means that much of the group's structure is also preserved. There is, in fact, a special type of homomorphism which perfectly preserves a group's structure.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;group isomorphism&lt;/strong&gt; between groups $G$ and $H$ is a bijective homomorphism $f:G\to H$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Two groups are &lt;strong&gt;isomorphic&lt;/strong&gt; if there exists a group isomorphism between them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Isomorphisms are the nicest sort of map between groups. Since they are bijective, they map each group element to precisely one element in the other group. Since they are homomorphisms, they preserve the group operation. This means that an isomorphism is essentially just a way of renaming the elements of a group. They do not alter the way that elements interaction with each other. Perhaps an example will help get my point across.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The group $(\R, +)$ of real numbers under addition and the group $(\R^+, \cdot)$ of positive real numbers under multiplication are isomorphic. We can establish this by exhibiting an isomorphism between them.&lt;/p&gt;
&lt;p&gt;Let's define the function $f:\R^+\to\R$ by $f(x)=\log x$. We will argue that $f$ is an isomorphism. First, it is clearly a homomorphism because from the basic properties of the logarithm,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f(x) + f(y) &amp;amp;= \log x +\log y \\&lt;br&gt;
&amp;amp;= \log(x \cdot y) \\&lt;br&gt;
&amp;amp;= f(x \cdot y).&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;In addition, we know that $f$ is bijective because it has an inverse function, the exponential map $f^{-1}:\R\to\R^+$ given explicitly by $f^{-1}(x)=e^x$.&lt;/p&gt;
&lt;p&gt;What does it really mean when we say that these groups are isomorphic? It isn't just some meaningless abstract statement that these groups are &amp;quot;essentially&amp;quot; the same. It actually means that we can do algebra in one and then easily transfer it over to the other group! That is, we can do multiplication in $\R^+$ simply by doing the corresponding addition in $\R$ and then applying the logarithm. This is the principle that slide rules are based on. Addition is easier than multiplication, so we add our numbers by hand and then use the magic slide rule isomorphism machine to take us between groups and into the world of multiplication.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Isomorphisms may be useful in certain cases, but they are actually a bit boring precisely &lt;em&gt;because&lt;/em&gt; they perfectly preserve structure. After all, how interesting is a flat renaming of group elements? We might as well just call two isomorphic groups the same. General homomorphisms are actually much more interesting.&lt;/p&gt;
&lt;p&gt;It may be enlightening to see an example of a homomorphism that is not an isomorphism, but is still surprisingly structure-preserving.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the group $\Z$ of integers under addition and the group $\{-1,1\}$ under multiplication. In $\{-1,1\}$ we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
1\cdot 1 &amp;amp;= 1, \\&lt;br&gt;
-1\cdot 1 &amp;amp;= -1. \\&lt;br&gt;
1\cdot -1 &amp;amp;= -1, \\&lt;br&gt;
-1\cdot -1 &amp;amp;= 1,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;So clearly the identity element in $\{-1,1\}$ is $1$ and every element is its own inverse. Let's construct a function $f:\Z\to\{-1,1\}$ defined by&lt;/p&gt;
&lt;p&gt;$$f(x) =&lt;br&gt;
\begin{cases}&lt;br&gt;
1 &amp;amp; \text{if } x\in 2\Z, \\&lt;br&gt;
-1 &amp;amp; \text{if } x\in 2\Z+1.&lt;br&gt;
\end{cases}$$&lt;/p&gt;
&lt;p&gt;(Here $2\Z$ is the set of even integers and $2\Z+1$ is the set of odd integers. My reason for denoting them this way will become apparent once we talk about cosets in a later post.)&lt;/p&gt;
&lt;p&gt;Let's show that this function is a homomorphism. Choose $m,n\in\Z$. There are three possible cases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Both $m$ and $n$ are even. That is, $m,n\in 2\Z$.&lt;/li&gt;
&lt;li&gt;Both $m$ and $n$ are odd. That is, $m,n\in 2\Z+1$.&lt;/li&gt;
&lt;li&gt;Either $m$ is even and $n$ is odd or vice versa. Because both groups we're considering are abelian, it suffices to consider only one of these cases, $m\in 2\Z$ and $n\in 2\Z+1$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We examine case 1 first. Suppose $m,n\in 2\Z$, so that $f(m)=f(n)=1$. Then $m=2k_1$ and $n=2k_2$ for some $k_1,k_2\in\Z$. This means that $$m+n=2k_1+2k_2=2(k_1+k_2)\in 2\Z.$$ Thus, $$f(m)f(n)=1\cdot 1=1=f(m+n).$$&lt;/p&gt;
&lt;p&gt;Next, we look at case 2. Suppose that $m,n\in 2\Z+1$, so that $f(m)=f(n)=-1$. Then $m=2k_1+1$ and $n=2k_2+1$ for some $k_1,k_2\in\Z$. This means that $$m+n=(2k_1+1)+(2k_2+1)=2k_1+2k_2+2=2(k_1+k_2+1)\in 2\Z.$$ Thus, $$f(m)f(n)=-1\cdot -1=1=f(m+n).$$&lt;/p&gt;
&lt;p&gt;Lastly, we examine case 3. Suppose that $m\in 2\Z$ but $n\in 2\Z+1$, so that $f(m)=1$ and $f(n)=-1$. Then $m=2k_1$ and $n=2k_2+1$ for some $k_1,k_2\in\Z$. This means that $$m+n=2k_1+2k_2+1=2(k_1+k_2)+1\in 2\Z+1.$$ Thus, $$f(m)f(n)=1\cdot -1=-1=f(m+n).$$&lt;/p&gt;
&lt;p&gt;Since we have now checked that $f(m)f(n)=f(m+n)$ for all $m,n\in\Z$, it follows that $f$ is a homomorphism.&lt;/p&gt;
&lt;p&gt;That was really us showing what we already know — that the sum of two even numbers is even, the sum of two odd numbers is also even, and the sum of an even and an odd number is odd. The property of being even or odd is sometimes called &lt;strong&gt;parity&lt;/strong&gt;, and this homomorphism preseves the parity of integers, as well as what happens to the parity of integers after you add them together. Actually, parity is basically all that is preserved by this homomorphism, but it's still a nice example of how homomorphisms can preserve important aspects of a group's structure.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now for some basic properties of homomorphisms. I promise these next two proofs are easy and short. The first property is that homomorphisms always map the identity of one group to the identity of the other. If we were being very careful, we would represent the identity of $G$ by $e_G$ and the identity in $H$ by $e_H$. However, it is common to be a bit sloppy and use $e$ to represent the identity element in both groups. If proper care is taken, this should never result in any confusion.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $f:G\to H$ is a group homomorphism then $f(e)=e$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $f$ is a homomorphism, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f(e)f(e) &amp;amp;= f(ee) \\&lt;br&gt;
&amp;amp;= f(e) \\&lt;br&gt;
&amp;amp;= f(e)e.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Thus, cancellation on the left yields the equality $f(e)=e$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The next property is just as straightforward — homomorphisms always map inverses to inverses.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $f:G\to H$ is a group homomorphism then $f(x^{-1})=f(x)^{-1}$ for every $x\in G$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $f$ is a homomorphism, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f(x)f(x^{-1}) &amp;amp;= f(xx^{-1}) \\&lt;br&gt;
&amp;amp;= f(e) \\&lt;br&gt;
&amp;amp;= e.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Multiplication on the left by $f(x)^{-1}$ on both sides yields $f(x^{-1})=f(x)^{-1}$, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Both of these properties only serve to reinforce my claim that homomorphisms are structure-preserving maps. I'll shut my smug face now and define two of the most important things ever.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;kernel&lt;/strong&gt; of a group homomorphism $f:G\to H$ is the set $$\ker f=\{x\in G\mid f(x)=e\}.$$ That is, it is the set of elements in $G$ that get mapped to the identity in $H$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;image&lt;/strong&gt; of a group homomorphism $f:G\to H$ is the set $$\im f=f[G]=\{f(x)\in H\mid x\in G\}.$$ That is, it is the set of elements in $H$ that get mapped to by some element in $G$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If $f:G\to H$ is a group homomorphism and $f$ is surjective, it should be clear that $H=\im f$. Similarly, if $f$ is injective then $\ker f = \{e\}$. These facts, along with the next two theorems, are crucial to everything we will do with groups in the future.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $f:G\to H$ is a group homomorphism then its kernel is a subgroup of $G$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; From the lemma at the beginning of this post, it suffices to show that $\ker f$ is nonempty and that $ab^{-1}\in \ker f$ whenever $a,b\in\ker f$.&lt;/p&gt;
&lt;p&gt;The fact that $\ker f$ is nonempty follows immediately from the property that $f(e)=e$, so certainly $e\in\ker f$.&lt;/p&gt;
&lt;p&gt;Next, suppose that $a,b\in\ker f$. That is, $f(a)=f(b)=e$. Then&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f(ab^{-1}) &amp;amp;= f(a)f(b^{-1}) \\&lt;br&gt;
&amp;amp;= f(a)f(b)^{-1}) \\&lt;br&gt;
&amp;amp;= ee^{-1} \\&lt;br&gt;
&amp;amp;= e,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;and thus $ab^{-1}\in\ker f$ by definition. It follows that $\ker f$ is a subgroup of $G$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Along similar lines:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $f:G\to H$ is a group homomorphism then its image is a subgroup of $H$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We will employ the same technique as in the previous proof.&lt;/p&gt;
&lt;p&gt;Certainly $\im f$ is nonempty because $e\in G$ and thus $f(e)=e\in\im g$.&lt;/p&gt;
&lt;p&gt;Next, suppose that $a,b\in\in f$. That is, $a=f(x)$ and $b=f(y)$ for some $x,y\in G$. Certainly $y^{-1}\in G$ and thus $xy^{-1}\in G$ as well, so&lt;br&gt;
$$\begin{align}&lt;br&gt;
ab^{-1} &amp;amp;= f(x)f(y)^{-1} \\&lt;br&gt;
&amp;amp;= f(xy^{-1}) \\&lt;br&gt;
&amp;amp;\in \im f,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In my next few posts, you will begin to understand why it's such a big deal that the kernal and image of a homomorphism are subgroups.&lt;/p&gt;
</content:encoded></item><item><title>Path Connectedness</title><description>Now we can think about a different type of "connectedness." Intuitively, if a space is "connected" you should be able to draw a path between any two points in the space. Otherwise, if there are points in the space that cannot be connected by a path, it is "disconnected."</description><link>http://localhost:2368/path-connectedness/</link><guid isPermaLink="false">5c706452e91ada004c1f7d50</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Wed, 27 Feb 2019 02:47:43 GMT</pubDate><content:encoded>&lt;ol&gt;
&lt;li&gt;&lt;a href="#some-useful-tools"&gt;Some Useful Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#path-connected-spaces"&gt;Path Connected Spaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#a-pathological-example"&gt;A Path-ological Example&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="someusefultoolsanamesomeusefultools"&gt;Some Useful Tools&lt;a name="some-useful-tools"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I defined connectedness in an earlier post, but there's actually a completely different route we could have taken to define it. Recall that a space is connected if it cannot be separated by disjoint neighborhoods. In this post we'll explore an alternative approach to defining connectedness of topological space, called &lt;em&gt;path connectedness&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;But before we do so, we actually need another fact about connectedness. (Recall that $\overline B$ denotes the closure of the set $B$.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ denote a topological space with a connected subsset $A\subseteq X$. If the subset $B\subseteq X$ satisfies $A\subseteq B\subseteq \overline{A}$, then $B$ is connected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We give a proof by contradiction. Suppose $B$ is not connected. Then there exist open sets $U,V\subseteq B$ which separate $B$. Since $A$ is a connected subset of $B$, this means that either $A\subseteq U$ or $A\subseteq V$. Assume without loss of generality that $A\subseteq U$. Then $A\cap V=\varnothing$ because $U\cap V = \varnothing$. Note, however, that $B\cap V\ne\varnothing$. Thus, we may choose $x\in B\cap V$. Certainly $x\in B$ and so it follows that $x\in\overline{A}$ because $B\subseteq\overline{A}$. On the other hand, we have $x\in V$, which is open in $X$ and disjoint from $B$, so $x\notin\overline{A}$. This is a contradiction, since it cannot be that both $x\in\overline{A}$ and $x\notin\overline{A}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What the above theorem really says is that adding limit points to a connected set always results in another connected set. This result will be invaluable to us throughout this post and in the future. Here is just one example of its usefulness:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Corollary.&lt;/strong&gt; All types of intervals in $\mathbb{R}$ (open, half-open and closed) are connected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We've shown previously that open intervals are connected. Therefore, the rest are as well since they can be obtained from open intervals by adding limit points.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Of particular interest to us is the connectedness of the closed unit interval $[0, 1]$.&lt;/p&gt;
&lt;p&gt;The following invaluable lemma allows us to determine when a piecewise function constructed from continuous maps is continuous.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Gluing Lemma.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces with closed subsets $U,V\subseteq X$ for which $X=U\cup V$. If $f_1:U\to Y$ and $f_2:V\to Y$ are continuous functions which agree on their intersection, i.e. $f_1\restriction{U\cap V}=f_2\restriction{U\cap V}$, then there exists a unique continuous function $f:X\to Y$ with $f\restriction{U}=f_1$ and $f\restriction{V}=f_2$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Showing that such a function is unique is fairly straightforward. We simply define $f:X\to Y$ by&lt;/p&gt;
&lt;p&gt;$$f(x) =&lt;br&gt;
\begin{cases}&lt;br&gt;
f_1(x) &amp;amp; \text{if } x\in U, \\&lt;br&gt;
f_2(x) &amp;amp; \text{if } x\in V.&lt;br&gt;
\end{cases}$$&lt;/p&gt;
&lt;p&gt;This function is clearly well defined because $f_1$ and $f_2$ agree on their intersection.&lt;/p&gt;
&lt;p&gt;We will argue next that $f$ is continuous by demonstrating that the preimage of any open set in $Y$ is open in $X$. To this end, suppose $W\subseteq Y$ is open. Then&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f^{-1}[W] &amp;amp;= X \cap f^{-1}[W] \\&lt;br&gt;
&amp;amp;= (U \cup V) \cap f^{-1}[W] \\&lt;br&gt;
&amp;amp;= (U \cap f^{-1}[W]) \cup (V \cap f^{-1}[W]) \\&lt;br&gt;
&amp;amp;= (U \cap f_1^{-1}[W]) \cup (V \cap f_2^{-1}[W]) \\&lt;br&gt;
&amp;amp;= f_1^{-1}[W] \cup f_2^{-1}[W].&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Since $f_1$ and $f_2$ are continuous, certainly $f_1^{-1}[W]$ and $f_2^{-1}[W]$ are open in $U$ and $V$, respectively. Thus $f^{-1}[W]$ is open in $U\cup V = X$ since it is the union of open sets. It follows that $f$ is continuous, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="pathconnectedspacesanamepathconnectedspaces"&gt;Path Connected Spaces&lt;a name="path-connected-spaces"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We can now work toward defining path connectedness. But first we need to define what a path is! The definition may look a bit ominous at first, but it's really just a formal way of saying exactly what you'd expect.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $X$ denote a topological space with points $a,b\in X$. A &lt;strong&gt;path&lt;/strong&gt; in $X$ from $a$ to $b$ is a continuous map $p:[0,1]\to X$ for which $p(0)=a$ and $p(1)=b$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is implicitly understood in such cases that $[0,1]$ is to be viewed as a subspace of $\R$ in the standard topology.&lt;/p&gt;
&lt;p&gt;Basically a path is just a continuous map from the unit interval into the space of interest. Some authors use the word to describe both a path $p$ and its image $p([0,1])\subseteq X$, but I will keep the concepts separate. That is to say, a path is &lt;em&gt;not&lt;/em&gt; the set of points that its image traces out in the codomain. Its image is a useful way to visualize it, however. And of course, since $[0,1]$ is a connected set, so is its image because paths are continuous! This means that the image of a path looks like a connected curve.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/03/path-1.svg" alt="path-1"&gt;&lt;/p&gt;
&lt;p&gt;Now we can think about a different type of &amp;quot;connectedness.&amp;quot; Intuitively, if a space is &amp;quot;connected&amp;quot; you should be able to draw a path between any two points in the space. Otherwise, if there are points in the space that cannot be connected by a path, it is &amp;quot;disconnected.&amp;quot;&lt;/p&gt;
&lt;p&gt;But we've already used these words for a different concept, so we need to make up new ones.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A topological space $X$ is &lt;strong&gt;path connected&lt;/strong&gt; if for any pair of points $a,b\in X$ there is a path $p:[0,1]\to X$ from $a$ to $b$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You may be wondering whether path connectedness and connectedness are actually one and the same. After all, it's hard to visualize a space that is connected in one sense but not the other. It turns out that one direction is easy.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Every path connected topological space is connected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We give a proof by contradiction. Suppose a space $X$ is path connected but not connected. Then there exists a separation of $X$ by nonempty disjoint open sets $U,V\subseteq X$. Choose points $u\in U$ and $v\in V$. Since $X$ is path connected, there exists a path $p:[0,1]\to X$ with $p(0)=u$ and $p(1)=v$. Define $U'=U\cap p([0,1])$ and $V'=V\cap p([0,1])$. We will show that $U'$ and $V'$ form a separation of $p([0,1])$.&lt;/p&gt;
&lt;p&gt;First, we note that $u\in U'$ because $u\in U$ and $p(0)=u$ so $u\in p([0,1])$. Similarly, $v\in V'$ because $v\in V$ and $p(1)=v$ so $v\in p([0,1])$. Thus, $U'$ and $V'$ are nonempty. Next, clearly&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U' \cup V' &amp;amp;= \big(U \cap p([0, 1])\big) \cup \big(V \cap p([0, 1])\big) \\&lt;br&gt;
&amp;amp;= (U \cup V) \cap p([0,1]) \\&lt;br&gt;
&amp;amp;= X \cap p([0,1]) \\&lt;br&gt;
&amp;amp;= p([0,1]).&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;A similar computation yields&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U' \cap V' &amp;amp;= \big(U \cap p([0, 1])\big) \cap \big(V \cap p([0, 1])\big) \\&lt;br&gt;
&amp;amp;= (U \cap V) \cap p([0,1]) \\&lt;br&gt;
&amp;amp;= \varnothing \cap p([0,1]) \\&lt;br&gt;
&amp;amp;= \varnothing.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Finally, $U'$ and $V'$ are both open in the subspace topology on $p([0,1])$ because they are intersections of $p([0,1])$ with open sets in $X$ ($U$ and $V$, respectively). It follows that $U'$ and $V'$ separate $p([0,1])$ and so it is disconnected. However, it is the continuous image of the connected set $[0,1]$, so this is a contradiction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So path connectedness implies connectedness. But as we shall see later on, the converse does not necessarily hold. So the two notions are actually different. It takes more to be a path connected space than a connected one!&lt;/p&gt;
&lt;p&gt;As with any topological concept, we want to show that path connectedness is preserved by continuous maps.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $f:X\to Y$ is a continuous function and $X$ is path connected, then $f[X]\subseteq Y$ is also path connected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose points $a,b\in f[X]$ and pick $a'\in f^{-1}[\{a\}]$ and $b'\in f^{-1}[\{b\}]$. Since $X$ is path connected, there exists some path $p:[0,1]\to X$ from $a'$ to $b'$. We argue that $f\circ p:[0,1]\to Y$ is a path in $Y$ from $a$ to $b$.&lt;/p&gt;
&lt;p&gt;Certainly $f\circ p$ is continuous since it is the composition of continuous maps. Furthermore,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(f\circ p)(0) &amp;amp;= f(p(0)) \\&lt;br&gt;
&amp;amp;= f(a') \\&lt;br&gt;
&amp;amp;= a,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(f\circ p)(1) &amp;amp;= f(p(1)) \\&lt;br&gt;
&amp;amp;= f(b') \\&lt;br&gt;
&amp;amp;= b.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Thus $f\circ p$ is a path from $a$ to $b$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We saw earlier that disconnected spaces could be decomposed into maximal connected subsets. There is also a way of doing this for path disconnected spaces as well.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ denote a topological space and define a relation $\sim$ on $X$ as follows: $a\sim b$ if and only if there exists a path in $X$ from $a$ to $b$. Then $\sim$ is an equivalence relation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We first show reflexivity. For any point $a\in X$, the constant map $p:[0,1]\to X$ given by $p(t)=a$ is certainly a path from $a$ to itself, so $a\sim a$.&lt;/p&gt;
&lt;p&gt;Next we show symmetry. If $a\sim b$ then there exists a path $p:[0,1]\to X$ with $p(0)=a$ and $p(1)=b$. Define $p^{-}:[0,1]\to X$ by $p^{-}(t)=p(1-t)$. We may think of $p^{-}$ as tracing out $p$ in reverse. That $p^{-}$ is well defined follows because $1-t\in [0,1]$ whenever $t\in [0,1]$, so the images of $p$ and $p^{-}$ are actually equal. Next, $p^{-}$ is continuous since it is the composition of the continuous functions $p$ and $1-t$. Finally, $p^{-}(0)=p(1)=b$ and $p^{-}(1)=p(0)=a$, so $p^{-}$ is a path from $b$ to $a$. Thus, $b\sim a$.&lt;/p&gt;
&lt;p&gt;Lastly, we show transitivity. If $a\sim b$ and $b\sim c$ then there exist paths $p_1:[0,1]\to X$ with $p_1(0)=a$ and $p_1(1)=b$ and $p_2:[0,1]\to X$ with $p_2(0)=b$ and $p_2(1)=c$. Define $p:[0,1]\to X$ by&lt;/p&gt;
&lt;p&gt;$$p(t) =&lt;br&gt;
\begin{cases}&lt;br&gt;
p_1(2t) &amp;amp; \text{if } t\in [0,\frac{1}{2}], \\&lt;br&gt;
p_2(2t-1) &amp;amp; \text{if } t\in [\frac{1}{2},1].&lt;br&gt;
\end{cases}$$&lt;/p&gt;
&lt;p&gt;We may think of $p$ as a path which traces out $p_1$ and then $p_2$ at twice the original speed. That $p$ is well defined follows from the fact that $p_1(1)=p_2(0)=b$. By the gluing lemma, this also implies that $p$ is continuous. Furthermore, $p(0)=p_1(0)=a$ and $p(1)=p_2(1)=c$, so $p$ is a path from $a$ to $c$. Thus, $a\sim c$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above result gives us our maximal path-connected subsets:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;path components&lt;/strong&gt; of a topological space are the equivalence classes of the relation $\sim$ defined above.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="apathologicalexampleanameapathologicalexample"&gt;A Path-ological Example&lt;a name="a-pathological-example"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;(I wonder how many topology professors have made that pun with regard to the example I give below? Probably hundreds.)&lt;/p&gt;
&lt;p&gt;I mentioned above that connected spaces are not always path connected. In this section I will give an example of such a space, although the proof that this is the case is not exactly trivial. First, we will need the following result about sequences and limit points.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A sequence $\{x_n\}_{n\in\N}$ in a topological space $X$ is &lt;strong&gt;eventually constant&lt;/strong&gt; if there exists $x \in X$ and $N \in \N$ for which $x_n=x$ whenever $n&amp;gt;N$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that any eventually constant sequence trivially converges to the point $x$ in the above definition.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma.&lt;/strong&gt; Let $X$ denote a topological space and let $A$ be a subset of $X$. If there exists a sequence $\{p_n\}_{n\in\N}$ of points in $A$ that converges to $p\in X$ and which is not eventually constant, then $p$ is a limit point of $A$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose any neighborhood $U$ of $p$. Since $\{p_n\}_{n\in\N}$ converges to $p$, there exists $N_1\in\N$ for which $p_n\in U$ whenever $n&amp;gt;N$. Since $\{p_n\}_{n\in\N}$ is not eventually constant, there exists $N_2\in N$ for which $p_n\neq p$ whenever $n&amp;gt;N_2$. Choose $N=\max\{N_1,N_2\}$. For every $n&amp;gt;N$, we have that $p_n\in U$ and $p_n\ne p$. By definition, it follows that $p$ is a limit point of $A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We are now ready to give an example of a space that is connected but not path connected. Warning: it's pretty gross.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example (The Topologist's Sine Curve).&lt;/strong&gt; Define&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
A &amp;amp;= \Big\{\Big(x,\sin\big(\tfrac{1}{x}\big)\Big)\in\R^2\mid x\in (0,1)\Big\}, \\&lt;br&gt;
B &amp;amp;= \Big\{(0,y)\in\R^2\mid y\in [-1,1]\Big\}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;$A$ is a sort of sine curve which oscillates more and more rapidly as you approach the origin from the right, and which does not include its endpoints. $B$ is just a vertical line. The &lt;strong&gt;topologist's sine curve&lt;/strong&gt; is their union, $X=A\cup B$.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/03/topologists-sine-curve-1.svg" alt="topologists-sine-curve-1"&gt;&lt;/p&gt;
&lt;p&gt;The blue curve in the above graph is the set $A$ and the red curve (which may be too narrow on your display to see clearly) is the $B$. The line $B$ is sort of the natural completion of $A$, in the sense that $A$ never touches is but really looks like it ought to. More formally, every point of $B$ is a limit point of $A$.&lt;/p&gt;
&lt;p&gt;To show this, choose a point $(0,y)\in B$. Certainly $y=\sin\theta$ for some $\theta\in [0,2\pi)$, and thus $y=\sin(\theta+2\pi n)$ for every $n\in \N$. If $x_n=\frac{1}{\theta+2\pi n}$ then $y=\sin\big(\tfrac{1}{x_n}\big)$ for every $n\in \N$. Furthermore, it is clear that $\lim_{n\to\infty}x_n = 0$. Thus,&lt;/p&gt;
&lt;p&gt;$$\lim_{x_n\to 0}\Big(x_n,\sin\big(\tfrac{1}{x_n}\big)\Big) = (0,y).$$&lt;/p&gt;
&lt;p&gt;Since the sequence $\{x_n\}_{n\in\N}$ is not eventually constant, every point $y\in B$ is a limit point of $A$. Since $A$ is connected (it is the continuous image of $(0,1)$), it follows from the lemma at the beginning of this post that the topologist's sine curve $X$ is connected, since it can be obtained from a connected set plus by adding limit points.&lt;/p&gt;
&lt;p&gt;Showing that $X$ is not path connected is a bit trickier. We will proceed by contradiction, supposing that there exists a path $p:[0,1]\to X$ with $p(0)=(0,0)\in B$ and $p(1)\in A$. Since $B$ is closed in $\R^2$, it follows from the continuity of $p$ that $p^{-1}[B]$ is closed in $[0,1]$. Thus, $p^{-1}[B]$ contains a largest element, which we shall denote by $b$. This implies that $p\restriction{[b,1]}:[b,1]\to X$ is continuous with $p\restriction{[b,1]}(b)\in B$. and $p\restriction{[b,1]}(t)\in X-B = A$ for every $t&amp;gt;b$.&lt;/p&gt;
&lt;p&gt;We define $p':[0,1]\to X$ by $p'(t)=p\big((1-b)t+b\big)$. Clearly $p'(0)=p(b)$ and $p'(1)=p(1)$. Also, $p'$ is continuous since it is the composition of continuous functions. Thus, $p'$ is a path in $X$ from $p(b)$ to $p(1)$ with $p'([0,1])=p([b,1])$. Furthermore, $p'(0)\in B$ while $p'(t)\in A$ for every $t\in (0,1]$.&lt;/p&gt;
&lt;p&gt;Now, for any $t\in [0,1]$ we can write $p'(t)=\big(x(t),y(t)\big)$ in terms of its components $x:[0,1]\to [0,1]$ and $y:[0,1]\to [-1,1]$. Clearly both $x$ and $y$ are continuous. Observe that $x(0)=0$ while $x(t)\in (0,1]$ whenever $t\in (0,1]$. In addition, $y(0)=0$ while $y(t)=\sin\big(\frac{1}{x(t)}\big)$ for any $t\in (0,1]$. Choose $n\in\N$ and $u\in\Big(0,x\big(\frac{1}{n}\big)\Big)$ for which $\sin\big(\frac{1}{u}\big)=(-1)^n$. From the intermediate value theorem, there exists $t_n\in\big(0,\frac{1}{n}\big)$ for which $x(t_n)=u$. Since $\lim_{n\to\infty}\frac{1}{n}=0$ and $0&amp;lt;t_n&amp;lt;\frac{1}{n}$ for every $n\in\N$, clearly $\lim_{n\to\infty}t_n=0$. However, $y(t_n)=\sin\big(\frac{1}{x(t_n)}\big)=\sin\big(\frac{1}{u_n}\big)=(-1)^n$, which does not converge. This contradicts the continuity of $y$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are a few other well known examples of spaces like the above that are connected but not path connected. The most prominent is the &lt;strong&gt;topologist's whirlpool&lt;/strong&gt;, which is essentially just the polar form of the topologist's sine curve.&lt;/p&gt;
&lt;p&gt;One might wonder if there is a sufficient additional criterion for a connected space to be path connected? The answer is yes. If a space is connected and &lt;em&gt;locally path connected&lt;/em&gt; then it is path connected. However, I will not define local path connectedness or prove this result, since I do not really care about it and it is not useful to us.&lt;/p&gt;
</content:encoded></item><item><title>Constructing the Rational Numbers (2)</title><description>This is a continuation of Constructing the Rational Numbers (1). Before moving forward with the rest of the construction, I'd like to formally change my notation for rational numbers from that of equivalence classes of ordered pairs of integers to that of fractions.</description><link>http://localhost:2368/constructing-the-rational-numbers-2/</link><guid isPermaLink="false">5c6f446575042202e3d9fce2</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Fri, 22 Feb 2019 00:38:35 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#canonical-form"&gt;Canonical Form&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#where-is-z"&gt;Where is $\Z$?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ordering-the-rationals"&gt;Ordering the Rationals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#filling-the-gaps"&gt;Filling the Gaps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#field-axioms"&gt;Field Axioms&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="canonicalformanamecanonicalform"&gt;Canonical Form&lt;a name="canonical-form"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This is a continuation of &lt;a href="http://localhost:2368/constructing-the-rational-numbers-1/"&gt;Constructing the Rational Numbers (1)&lt;/a&gt;. Before moving forward with the rest of the construction, I'd like to formally change my notation for rational numbers from that of equivalence classes of ordered pairs of integers to that of fractions.&lt;/p&gt;
&lt;p&gt;That is, from here on out the rational number $[(a,b)]$ will simply be written as the &lt;strong&gt;fraction&lt;/strong&gt; $\frac{a}{b}$. And now rational numbers look exactly how you would expect. Yay! Note that the bar doesn't really mean anything yet, this notation is currently just a formalism.&lt;/p&gt;
&lt;p&gt;Remember that there are many choices of representative for each rational number. For instance, $\frac{1}{2}=\frac{2}{4}$ in the same way that $[(1,2)]=[(2,4)]$ because, in the language of my last post, $(1,2)\sim_\Q (2,4)$. Last time we defined this equivalence relation, $\sim_\Q$, which determines whether two fractions are really the same rational number. Let's rephrase this in terms of fractions and without the formality of the equivalence relation, because it's very important:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Rule (Cross Multiplication).&lt;/strong&gt; Two fractions $\frac{a}{b}$ and $\frac{c}{d}$ represent the same rational number if and only if $ad=bc$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We know in our hearts that although there are many fractional representations of each rational, there is always one preferred representation. That is, although $\frac{1}{2}$ and $\frac{2}{4}$ are both valid representations of the same number, we definitely prefer to call it $\frac{1}{2}$ because it's simpler somehow.&lt;/p&gt;
&lt;p&gt;We can easily make this decision rigorous, but first we need to recall the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given two integers $a$ and $b$, their &lt;strong&gt;greatest common divisor&lt;/strong&gt;, written $\gcd(a,b)$, is the largest positive integer which is a factor of both $a$ and $b$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here are some examples, although you have likely seen this concept before.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider $4$ and $6$. We can write $4=2\cdot 2$ and $6=2\cdot 3$. They both have one factor in common: $2$. Thus, $\gcd(4,6)=2$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider $15$ and $30$. We can write $15=3\cdot 5$ and $30=2\cdot 3\cdot 5$. They have two prime factors in common, so their greatest common divisor is the product of these factors: $\gcd(15, 30)=15$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider $0$ and $6$. We can trivially write $0=0\cdot 6$, and so $\gcd(0, 6) = 6$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider $3$ and $7$. We cannot simplify either number further since they are both prime. Clearly they have no factors in common other than $1$, so $\gcd(3, 7)=1$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider $4$ and $9$. Neither is prime, so we can write $4=2\cdot 2$ and $9=3\cdot 3$. There are still no common factors other than $1$, so $\gcd(4,9)=1$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That last example is particularly interesting, since we had two integers which were not prime, and whose greatest common divisor was still 1. We have a special name for this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Two integers $a$ and $b$ are &lt;strong&gt;coprime&lt;/strong&gt; (or &lt;strong&gt;relatively prime&lt;/strong&gt;) if $\gcd(a,b)=1$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I will not prove it, since we are taking properties of the integers for granted, but the greatest common divisor of two integers always exists as long as they are not both zero.&lt;/p&gt;
&lt;p&gt;We now have the tools to choose a preferred representation for each rational number.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A fractional representation $\frac{a}{b}$ for a rational number is in &lt;strong&gt;canonical form&lt;/strong&gt; if $a$ and $b$ are coprime and $b&amp;gt;0$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This definition should hopefully make sense to you. It means, for instance, that $\frac{1}{2}$ is the canonical form for the rational number which is also represented by $\frac{2}{4}$, $\frac{-1}{-2}$, etc.&lt;/p&gt;
&lt;p&gt;However, whenever we make a definition like this it is important to determine two things. Does it always exist? And if it exists, is it unique? In this case, the answer to both questions is yet. There is always exactly one canonical form for each rational number, and so we may refer to it as &lt;em&gt;the&lt;/em&gt; canonical form. Let's prove this!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; Every rational number has a unique canonical form.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose a rational number $q$. We argue first that a canonical form for $q$ exists. Let $\frac{a}{b}$ be some fraction which represents $q$. Note that $\gcd(a,b)$ is guaranteed to exist because $b\neq 0$. Thus there are integers $m$ and $n$ for which&lt;/p&gt;
&lt;p&gt;$$\frac{a}{b} = \frac{m\cdot\gcd(a,b)}{n\cdot\gcd(a,b)}.$$&lt;/p&gt;
&lt;p&gt;We argue that $\frac{a}{b}=\frac{m}{n}$. This is easily done by remembering the above rule for determining whether two fractions represent the same rational number. From the first equality above, we have that&lt;/p&gt;
&lt;p&gt;$$a \cdot n \cdot \gcd(a,b) = b \cdot m \cdot \gcd(a,b).$$&lt;/p&gt;
&lt;p&gt;Using the left cancellation property of the integers, we may cancel $\gcd(a,b)$ from both sides to obtain&lt;/p&gt;
&lt;p&gt;$$an=bm.$$&lt;/p&gt;
&lt;p&gt;Again using the cross multiplication rule for equivalent fractions, we see that $\frac{a}{b}=\frac{m}{n}$. If $n$ is positive, this is certainly a canonical form for $q$. Otherwise, we may easily obtain a canonical form by negating both slots of the fraction: $\frac{-m}{-n}$. It is easy to show from here that $\frac{a}{b}=\frac{-m}{-n}$, so I will not bother with it.&lt;/p&gt;
&lt;p&gt;We have demonstrated that a canonical form for $q$ exists. It remains to show that it is unique. That is, we aim to show that if $\frac{m_1}{n_1}$ and $\frac{m_2}{n_2}$ are both canonical forms for $q$, then $m_1=m_2$ and $n_1=n_2$.&lt;/p&gt;
&lt;p&gt;This is not too difficult. Since both fractions are already in canonical form, we know that $\gcd(m_1,n_1)=1$ and $\gcd(m_2,n_2)=1$. Furthermore, since they both represent $q$, we also know that $\frac{m_1}{n_1} = \frac{m_2}{n_2}$ and thus $m_1n_2=n_1m_2$.&lt;/p&gt;
&lt;p&gt;Certainly this indicates that $m_1$ is a factor of $n_1m_2$. But $m_1$ and $n_1$ are coprime by hypothesis, so it must be the case that $m_1$ is a factor of $m_2$. Reversing the argument, we see that $m_2$ is a factor of $m_1$. This can only be true if $m_1=m_2$. The argument that $n_1=n_2$ is exactly analogous. It follows then that the canonical form is unique, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So now we know that every rational number can be represented in a way that is in this sense &amp;quot;most desirable.&amp;quot; It certainly aligns with the simplification of fractions that we all saw in elementary school.&lt;/p&gt;
&lt;p&gt;Now, remember that we already defined addition and multiplication of rational numbers in my previous post. Let's review those definitions here.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;sum&lt;/strong&gt; of two rational numbers $\frac{a}{b}$ and $\frac{c}{d}$ is the rational number $\frac{ad+bc}{bd}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;product&lt;/strong&gt; of two rational numbers $\frac{a}{b}$ and $\frac{c}{d}$ is the rational number $\frac{ac}{bd}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It's worth mentioning that taking the sum or product of two fractions in canonical form does not always result in a canonical form fraction.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The fraction $\frac{1}{2}$ is certainly in canonical form. However,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\frac{1}{2}+\frac{1}{2} &amp;amp;= \frac{1\cdot 2 + 2\cdot 1}{2\cdot 2} \\&lt;br&gt;
&amp;amp;= \frac{4}{4}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;This is certainly not in canonical form, since $\gcd(4,4)=4$.&lt;/p&gt;
&lt;p&gt;However, this is not really an issue since we can just rewrite it in canonical form post-computation. In this case, the canonical form for the result is $\frac{1}{1}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="whereiszanamewhereisz"&gt;Where is $\Z$?&lt;a name="where-is-z"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Ordinarily, when we are not being as ridiculously pedantic as we are in this post, we would consider the integers to be a subset of the rational numbers. But technically this is not the case of our construction, since our rationals are equivalence classes of pairs of integers and thus not integers themselves. It's our goal, therefore, to identify a subset of rationals that looks and behaves like the integers.&lt;/p&gt;
&lt;p&gt;The above example may have already given this away. Technically the rational number whose canonical form is $\frac{1}{1}$ is not the same thing as the integer $1$. But I mean, come on... they're pretty darn similar. This leads us to the following identification.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;rational integers&lt;/strong&gt; are the set of rational numbers whose canonical form is $\frac{n}{1}$, where $n$ is an integer. We refer to $\frac{n}{1}$ as the &lt;strong&gt;rational integer corresponding to $n$&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I don't actually think &amp;quot;rational integer&amp;quot; is a phrase that anyone ever uses, but it will serve our purposes for this post to distinguish between an integer and its corresponding rational number.&lt;/p&gt;
&lt;p&gt;Rational integers behave just as we would expect under addition and multiplication. That is, their sums and products are also rational integers. Moreover, they are the &lt;em&gt;correct&lt;/em&gt; rational integers. Let's make this a bit more formal:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; If $m$ and $n$ are integers, then the sum of their corresponding rational integers is the rational integer corresponding to their sum. That is,&lt;/p&gt;
&lt;p&gt;$$\frac{m}{1}+\frac{n}{1}=\frac{m+n}{1}.$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; The result actually follows immediately from our definition of rational addition, since&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\frac{m}{1}+\frac{n}{1} &amp;amp;= \frac{m\cdot 1 + 1\cdot n}{1\cdot 1} \\&lt;br&gt;
&amp;amp;= \frac{m+n}{1}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can do exactly the same sort of thing for multiplication:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; If $m$ and $n$ are integers, then the product of their corresponding rational integers is the rational integer corresponding to their product. That is,&lt;/p&gt;
&lt;p&gt;$$\frac{m}{1}\cdot\frac{n}{1}=\frac{mn}{1}.$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Again, the result follows directly from our definition of rational multiplication. Note that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\frac{m}{1}\cdot\frac{n}{1} &amp;amp;= \frac{m\cdot n}{1\cdot 1} \\&lt;br&gt;
&amp;amp;= \frac{mn}{1}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So these rational integers really do correspond to the integers in a one-to-one manner. Since there's no functional difference between rational integers and integers, we might as well just say they're the same thing.&lt;/p&gt;
&lt;h3 id="orderingtherationalsanameorderingtherationals"&gt;Ordering the Rationals&lt;a name="ordering-the-rationals"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;One of the requirements we imposed on our construction last post was that the rationals should be ordered in a way that's compatible with the integers. That means that if $n$ and $m$ are integers with $n &amp;lt; m$, then we definitely want it to be the case that $\frac{n}{1} &amp;lt; \frac{m}{1}$. That is, integers and rational integers should have the same order.&lt;/p&gt;
&lt;p&gt;But in addition to rational integers, all the other rational numbers should be comparable as well. This is done easily enough.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given two rational numbers $\frac{a}{b}$ and $\frac{c}{d}$ in canonical form, their &lt;strong&gt;order&lt;/strong&gt; is determined by saying that $\frac{a}{b} &amp;lt; \frac{c}{d}$ if and only if $ad&amp;lt;bc$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Just as a quick check that this definition of order makes any sense, let's look at a couple simple examples.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the fractions $\frac{1}{2}$ and $\frac{2}{3}$. These are already in canonical form, and we would certainly expect $\frac{1}{2}$ to be less than $\frac{2}{3}$. Let's check that this is indeed the case.&lt;/p&gt;
&lt;p&gt;According to the definition, we just need to make sure that $1 \cdot 3 &amp;lt; 2\cdot 2$. This is obviously true ($3&amp;lt;4$) and so we're done.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the fractions $\frac{0}{1}$ and $\frac{-3}{2}$. Of course we hope it should be the case that $\frac{-3}{2}$ is less than $\frac{0}{1}$.&lt;/p&gt;
&lt;p&gt;Again, plugging these straight into the definition, we see that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
-3\cdot 1 &amp;amp;= -3 \\&lt;br&gt;
&amp;amp;&amp;lt; 0 \\&lt;br&gt;
&amp;amp;= 2\cdot 0.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These sanity checks indicate that we're on the right track with our definition of order. But we need to guarantee that our order extends that of the integers. Let's do that now.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; If $m$ and $n$ are integers with $m&amp;lt;n$ then $\frac{m}{1} &amp;lt; \frac{n}{1}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; There's really not much to prove. Since $m&amp;lt;n$ we certainly have that $m\cdot 1 = 1\cdot n$. By the definition of order, this means that $\frac{m}{1} &amp;lt; \frac{n}{1}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So this order is compatible with the order on the integers. Yay! Now that our rational numbers are ordered, we're allowed to put them on the number line if we so choose.&lt;/p&gt;
&lt;h3 id="fillingthegapsanamefillingthegaps"&gt;Filling the Gaps&lt;a name="filling-the-gaps"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Our motivation for inventing rational numbers was to fill the two types of gaps we identified in the previous post as being missing from the integers. Namely, we required that our rational numbers satisfy the following properties:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;If $a$ and $b$ are integers with $a\ne 0$, there exists a rational number $x$ for which $ax=b$.&lt;/li&gt;
&lt;li&gt;If $p$ and $q$ are rational numbers with $p&amp;lt;q$, there exists a rational number $x$ for which $p&amp;lt;x&amp;lt;q$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;It's actually pretty straightforward to show that our construction guarantees these properties. Let's get straight to work!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; If $a$ and $b$ are integers with $a\ne 0$, there exists a rational number $x$ for which $ax=b$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Technically we should consider the rational integers $\frac{a}{1}$ and $\frac{b}{1}$. We need to show that there is a rational number $\frac{p}{q}$ for which $\frac{a}{1}\cdot\frac{p}{q}=\frac{b}{1}$.&lt;/p&gt;
&lt;p&gt;We argue that taking $p=b$ and $q=a$ will yield the desired result. That is, we only need to verify that $\frac{a}{1}\cdot\frac{b}{a}=\frac{b}{1}$. Notice that, from the definition of rational multiplication,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\frac{a}{1}\cdot\frac{b}{a} &amp;amp;= \frac{ab}{1a} \\&lt;br&gt;
&amp;amp;= \frac{ab}{a}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Since $\gcd(ab, a)=a$, the canonical form for the result is $\frac{b}{1}$, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Neat! We plugged one type of gap and now have solutions to lots of equations. All that's left is to check that we've filled the other type of gap.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; If $p$ and $q$ are rational numbers with $p&amp;lt;q$, there exists a rational number $x$ for which $p&amp;lt;x&amp;lt;q$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose $p=\frac{p_1}{p_2}$ and $q=\frac{q_1}{q_2}$ are in canonical form. We argue that taking $x=\frac{p_1q_2 + p_2q_1}{2p_2q_2}$ will suffice. To see that we did not pull this out of thin air, notice that $x$ is really just the &amp;quot;average&amp;quot; of $p$ and $q$ put into fractional form, and thus we should expect it to sit directly between them on the number line.&lt;/p&gt;
&lt;p&gt;We need to verify that $p&amp;lt;x$ and that $x&amp;lt;q$. To do so, we use the definition of order.&lt;/p&gt;
&lt;p&gt;For the first inequality, note first that since $p&amp;lt;q$, we have that $p_1q_2&amp;lt;p_2q_1$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(p_1)(2p_2q_2) &amp;amp;= (p_2)(2p_1q_2) \\&lt;br&gt;
&amp;amp;&amp;lt; (p_2)(p_1q_2 + p_2q_1),&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;It follows then that $\frac{p_1}{p_2} &amp;lt; \frac{p_1q_2 + p_2q_1}{2p_2q_2}$. That is, $p&amp;lt;x$.&lt;/p&gt;
&lt;p&gt;The proof that $x&amp;lt;q$ is completely analagous to the above.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And there we have it. Our construction of the rational numbers satisfies all the properties we wanted it to!&lt;/p&gt;
&lt;p&gt;So why did we do all this again? The short answer is because we can, and because it's kind of cool. The real answer is that we construct things in mathematics from the ground up so that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We know that the objects we are working with actually exist.&lt;/li&gt;
&lt;li&gt;We know exactly what properties our objects satisfy.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I don't feel comfortable working with anything I can't get a feel for in this way.&lt;/p&gt;
&lt;h3 id="fieldaxiomsanamefieldaxioms"&gt;Field Axioms&lt;a name="field-axioms"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Our rational numbers actually have a few more properties than I've mentioned. Technically, they form what is called an ordered field. I won't prove all of the field axioms here, but from what I've done already and the properties of the integers, you should be able to fill in the gaps pretty easily now. I'll just give you the axioms and then call it quits.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;field&lt;/strong&gt; is a set $\mathbb{F}$ equipped with two operations, addition $+$ and multiplication $\cdot$, which satisfy the following properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Associativity of Addition&lt;/strong&gt;: For all $a,b,c\in\mathbb{F}$, $a+(b+c)=(a+b)+c$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Commutativity of Addition&lt;/strong&gt;: For all $a,b\in\mathbb{F}$, $a+b=b+a$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Additive Identity&lt;/strong&gt;: There exists $0\in\mathbb{F}$ for which $0+a=a$ for any $a\in\mathbb{F}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Additive Inverses&lt;/strong&gt;: For all $a\in\mathbb{F}$, there exists $-a\in\mathbb{F}$ for which $a+(-a)=0$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Associativity of Multiplication&lt;/strong&gt;: For all $a,b,c\in\mathbb{F}$, $a\cdot (b\cdot c)=(a\cdot b)\cdot c$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Commutativity of Multiplication&lt;/strong&gt;: For all $a,b\in\mathbb{F}$, $a\cdot b=b\cdot a$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multiplicative Identity&lt;/strong&gt;: There exists $1\in\mathbb{F}$ with $1\ne 0$ for which $1\cdot a=a$ for any $a\in\mathbb{F}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multiplicative Inverses&lt;/strong&gt;: For all $a\in\mathbb{F}$ with $a\ne 0$, there exists $a^{-1}\in\mathbb{F}$ for which $a\cdot a^{-1}=1$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributivity of Multiplication over Addition&lt;/strong&gt;: For all $a,b,c\in\mathbb{F}$, $a\cdot (b+c) = a\cdot b+a\cdot c$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Furthermore, $\mathbb{F}$ is an &lt;strong&gt;ordered field&lt;/strong&gt; if there is an order $&amp;lt;$ on $\mathbb{F}$ which is compatible with the field structure in the following sense: If $a,b\in\mathbb{F}$ and $a,b\ge 0$, then $a+b\ge 0$ and $a\cdot b\ge 0$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's pretty much it. It's not too hard to show that the rationals $\Q$ form an ordered field under the definitions we gave for addition and multiplication. The additive identity is $\frac{0}{1}$, the multiplicative identity is $\frac{1}{1}$, the additive inverse of $\frac{a}{b}$ is $\frac{-a}{b}$, and the multiplicative inverse of $\frac{a}{b}$ is $\frac{b}{a}$ (provided $a\ne 0$).&lt;/p&gt;
&lt;p&gt;The integers $\Z$ do &lt;em&gt;not&lt;/em&gt; form an ordered field because they are lacking multiplicative inverses. This is precisely the first &amp;quot;gap&amp;quot; that we filled.&lt;/p&gt;
&lt;p&gt;So... yeah.&lt;/p&gt;
</content:encoded></item><item><title>Connectedness</title><description>In this post, I'm going to prove the Intermediate Value Theorem and the One-Dimensional Brouwer Fixed Point Theorem, which are two results that are undeniably and unreasonably useful. In order to prove them, however, we will need to study the notion of connectedness.</description><link>http://localhost:2368/connectedness/</link><guid isPermaLink="false">5c6e5add0ba2bb003f86050b</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Thu, 21 Feb 2019 08:02:07 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#connected-spaces"&gt;Connected Spaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#some-cool-results"&gt;Some Cool Results&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="connectedspacesanameconnectedspaces"&gt;Connected Spaces&lt;a name="connected-spaces"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I decided to postpone the second half of my construction of the rational numbers because I needed to break up the monotony a bit.&lt;/p&gt;
&lt;p&gt;I'm finally about to show you some really cool stuff. In this post, I'm going to prove the Intermediate Value Theorem and the One-Dimensional Brouwer Fixed Point Theorem, which are two results that are undeniably and unreasonably useful. In order to prove them, however, we will need to study the notion of connectedness. I'll probably save path-connectedness for a later time, since this post will be long enough as is.&lt;/p&gt;
&lt;p&gt;Connectedness is pretty much exactly what you'd expect it to be - if something is connected then that means it is all one coherent piece. This is a very hand-wavy statement though, so let's try to come up with a real definition that makes sense in the context of topological spaces. It might be better to start by considering what it is that makes a space disconnected. Let's look at this picture of a space $X$ which we should certainly call disconnected (as a subspace of the plane $\R^2$ with the standard topology):&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/disconnectedspace1.svg" alt="disconnectedspace1"&gt;&lt;/p&gt;
&lt;p&gt;The space $X$ is very clearly comprised of two distinct components that are separated from each other. Now here is the key insight: each of these components is both open and closed in $X$. For this particular example, we can see that each component is open because it is the intersection of $X$ with an open set in $\R^2$ (remember that this is the definition of an open set in the subspace topology).&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/disconnectedspace2.svg" alt="disconnectedspace2"&gt;&lt;/p&gt;
&lt;p&gt;Similarly, each component is closed because it is the intersection of $X$ with a closed set in $\R^2$.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/disconnectedspace3.svg" alt="disconnectedspace3"&gt;&lt;/p&gt;
&lt;p&gt;So in disconnected spaces, there can be numerous sets that are both open and closed. This is something that is not true of connected spaces!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A space $X$ is &lt;strong&gt;connected&lt;/strong&gt; if its only subsets which are both open and closed are $\varnothing$ and $X$ itself.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A space $X$ is &lt;strong&gt;disconnected&lt;/strong&gt; if it is not connected, i.e., there exists a nonempty proper subset $A\subset X$ which is both open and closed in $X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We may occasionally also wish to ask whether subsets of a topological space are connected, and there is an easy way to look at this by using definitions we already have:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A subset $A$ of a topological space $X$ is &lt;strong&gt;connected&lt;/strong&gt; if $A$ is connected when viewed as a subspace of $X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is just one of several equivalent and common ways that we may define connectedness. Let's take a look at two more, since they will be convenient for proofs later in this post. For the first equivalent definition, it is somewhat easier to talk about disconnectedness.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; A topological space $X$ is disconnected if and only if there exist nonempty open sets $U,V\subset X$ for which $U\cap V=\varnothing$ and $U\cup V=X$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose first that $X$ is disconnected. That is, there exists a nonempty proper subset $A\subset X$ which is both open and closed in $X$. Then by definition, $X-A$ is also both open and closed, and $X-A$ is nonempty because $A\ne X$. We observe that $A\cap(X-A)=\varnothing$ and $A\cup(X-A)=X$. Thus, we have demonstrated nonempty open sets $U=A$ and $V=X-A$ for which $U\cap V=\varnothing$ and $U\cup V=X$.&lt;/p&gt;
&lt;p&gt;Suppose next that there exist nonempty open sets $U,V\subset X$ for which $U\cap V=\varnothing$ and $U\cup V=X$. Then $V=X-U$ is also closed since it is the complement of the open set $U$. Note also that $V$ is nonempty by hypothesis, and $V\ne X$ because $U$ is nonempty. It follows that $X$ is disconnected.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Just looking at this new phrasing of disconnectedness, we see that it is immediately applicable to my above example. Clearly that space $X$ is the union of two disjoint, nonempty open sets. Before I move on, let me formally state this new definition in the context of connectedness.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; If a space $X$ is disconnected, then a &lt;strong&gt;separation&lt;/strong&gt; of $X$ is a pair of nonempty open sets $U,V\subset X$ for which $U\cap V=\varnothing$ and $U\cup V=X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Corollary.&lt;/strong&gt; A topological space $X$ is &lt;strong&gt;connected&lt;/strong&gt; if it does not have a separation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The next equivalent definition of connectedness may look a little bit strange to you right now, but it will make our lives a lot easier in two of the proofs that lie ahead. Recall that a constant function $f$ has $f(x_1)=f(x_2)$ for all $x_1,x_2$ in its domain.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; A topological space $X$ is connected if and only if every continuous function $f:X\to\{0,1\}$ is constant, where $\{0,1\}$ is equipped with the discrete topology.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We prove each direction by contraposition. We argue first that if a continuous function $f:X\to\{0,1\}$ is not constant, then $X$ is not connected.&lt;/p&gt;
&lt;p&gt;Suppose there exists a nonconstant continuous function $f:X\to\{0,1\}$. Since $f$ is not constant, it takes on at least two values in its codomain and thus it must be the case that $f[X]=\{0,1\}$, so clearly $f$ is surjective. Define $U=f^{-1}\big[{0}\big]$ and $V=f^{-1}\big[{1}\big]$. Certainly $U\cap V=\varnothing$ and $U\cup V=X$. Also, $U$ and $V$ are nonempty because $f$ is surjective. It follows that $U$ and $V$ form a separation of $X$, and so $X$ is not connected.&lt;/p&gt;
&lt;p&gt;We argue next that if $X$ is not connected, then there exists a continuous function $f:X\to\{0,1\}$ which is not constant.&lt;/p&gt;
&lt;p&gt;Suppose $X$ is not connected. Then there exists a separation $U,V$ of $X$. Define $f:X\to\{0,1\}$ by&lt;/p&gt;
&lt;p&gt;$$f(n) =&lt;br&gt;
\begin{cases}&lt;br&gt;
0 &amp;amp; \text{if } x\in U, \\&lt;br&gt;
1 &amp;amp; \text{if } x\in V.&lt;br&gt;
\end{cases}$$&lt;/p&gt;
&lt;p&gt;This function is nonconstant because both $U$ and $V$ are nonempty. It is obvious that $f^{-1}\big[\{1\}\big]=U$ and $f^{-1}\big[\{0\}\big]=V$, which are both open in $X$. Furthermore, $f^{-1}\big[\{0,1\}\big]=X$ and $f^{-1}[\varnothing]=\varnothing$ are both open in $X$. We have demonstrated that the preimage of every open set in $\{0,1\}$ is open in $X$, and so $f$ is continuous by definition.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's rewrite this as yet another definition of connectedness.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A topological space $X$ is &lt;strong&gt;connected&lt;/strong&gt; if every continuous function $f:X\to\{0,1\}$ is constant.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This definition may seem a little bit stranger than the other two, but it is perfectly natural if we recall that continuous functions map points that are close together to points that are close together. In a connected space, all points are close together and thus a continuous function cannot bridge the gap between $0$ and $1$ - it must stay constant for all values of the domain. Of course, there is nothing special about $\{0,1\}$. We could just has easily used any other discrete two-point space. I'm about to prove a useful theorem whose proof is made much easier by this new definition of connectedness, but first we will need the following lemma.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma.&lt;/strong&gt; Let $X,Y$ denote topological spaces, let $A$ denote a subspace of $X$ and let $f:X\to Y$ be a continuous map. The restriction $f\mid_A:A\to Y$, defined by $f\mid_A(a)=f(a)$ for each $a\in A$, is continuous.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose any open set $U\subseteq Y$. Since $f$ is continuous, $f^{-1}[U]$ is open in $X$. Thus, $f\mid_A^{-1}[U]=A\cap f^{-1}[U]$ is open in $A$ by the definition of the subspace topology, and thus $f\mid_A$ is continuous.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now we have the machinery to prove the following intuitive theorem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ denote a topological space and suppose $\{A_i\}_{i=1}^n$ is a collection of $n$ connected subsets of $X$. If $A_i\cap A_{i+1}$ is nonempty for each $1\leq i&amp;lt; n$, then the set $\bigcup\limits_{i=1}^n A_i$ is connected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We will proceed by induction on $n$, with our base case $n=1$ being too obvious to require proof.&lt;/p&gt;
&lt;p&gt;Suppose $\{A_i\}_{i=1}^{n+1}$ are connected with $A_i\cap A_{i+1}\ne\varnothing$ for every $1\le i&amp;lt; n+1$, and that $\bigcup\limits_{i=1}^n A_i$ is connected. Suppose also that $f:\bigcup\limits_{i=1}^{n+1}A_i\to\{0,1\}$ is continuous. Then $f\mid_{\bigcup\limits_{i=1}^n A_i}:\bigcup\limits_{i=1}^n A_i\to\{0,1\}$ and $f\mid_{A_{n+1}}:A_{n+1}\to\{0,1\}$ are also continuous, and they are thus constant because their domains are connected. Since $A_n\cap A_{n+1}$ is nonempty, certainly $\left(\bigcup\limits_{i=1}^n A_i\right)\cap A_{n+1}$ is nonempty, and so there exists a point $x\in\left(\bigcup\limits_{i=1}^n A_i\right)\cap A_{n+1}$. Thus, because $f$ must be well defined, $f\mid_{\bigcup\limits_{i=1}^n A_i}(x)=f\mid_{A_{n+1}}(x)$. Since $f\mid_{\bigcup\limits_{i=1}^n A_i}$ and $f\mid_{A_{n+1}}$ are both constant and they agree at one point, they must agree at every point. Thus, $f$ is constant and so $\bigcup\limits_{i=1}^{n+1} A_i$ is connected.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The proof is simple but looks a bit ugly rendered this way, for which I apologize. The theorem itself should be fairly obvious, and is in fact quite easy to think of visually. Take the following diagram, for instance:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/connectedsequence.svg" alt="connectedsequence"&gt;&lt;/p&gt;
&lt;p&gt;This is a collection of six connected subsets of $\R^2$, each of which intersects the next in precisely one point. (They may or may not correspond to the six Bagel Bites I am about to enjoy.) From our theorem, it follows that their union is connected. Of course, we could have any number of connected subsets that intersect each other in different ways, as long as the successive pairwise intersections are nonempty. I will not prove it here, but this result also applies for any collection of connected subsets (even uncountably infinite).&lt;/p&gt;
&lt;p&gt;I mentioned a few posts back that much of the study of topology is focused on distinguishing between non-homeomorphic spaces. Connectedness offers a way of doing exactly that, because it is a topological property. What I mean by that is this: if two spaces are homeomorphic and one is connected, then so is the other.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X,Y$ denote topological spaces, let $f:X\to Y$ be a homeomorphism and suppose $X$ is connected. Then $Y$ is connected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We will prove the contrapositive. Suppose $Y$ is not connected, so that there exist open sets $U,V$ that separate $Y$. We argue that $f^{-1}[U]$ and $f^{-1}[V]$ form a separation of $X$.&lt;/p&gt;
&lt;p&gt;Clearly $f^{-1}[U]$ and $f^{-1}[V]$ are open because $f$ is continuous, and they are each nonempty because $f$ is surjective. In addition, because $U\cap V=\varnothing$, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f^{-1}[U]\cap f^{-1}[V]&amp;amp;=f^{-1}[U\cap V] \\&lt;br&gt;
&amp;amp;=f^{-1}[\varnothing] \\&lt;br&gt;
&amp;amp;=\varnothing.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Similarly, because $U\cup V=Y$, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f^{-1}[U]\cup f^{-1}[V]&amp;amp;=f^{-1}[U\cup V] \\&lt;br&gt;
&amp;amp;=f^{-1}[Y] \\&lt;br&gt;
&amp;amp;=X.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;It follows that $f^{-1}[U]$ and $f^{-1}[V]$ form a separation of $X$, so $X$ is not connected.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Really all we used was continuity and surjectivity of $f$, so saying that $f$ was a homeomorphism actually weakened the result a little bit. It's actually true that all continuous functions preserve connectedness! I won't prove it though, because it's really just a teensy modification of the above proof.&lt;/p&gt;
&lt;p&gt;Clearly we can distinguish between topological spaces as follows: if one space is connected and another isn't, then the spaces are not homeomorphic. However, we can also use connectedness in a somewhat trickier way to distinguish between certain connected spaces!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;cutset&lt;/strong&gt; $A$ of a connected topological space $X$ is a subset of $X$ for which $X-A$ is disconnected.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;cutpoint&lt;/strong&gt; $x$ of a connected topological space $X$ is a point of $X$ for which $\{x\}$ is a cutset, i.e., $X-\{x\}$ is disconnected.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Let's look again the connected set $A=\bigcup\limits_{i=1}^6 A_i$ depicted below, and consider the points $x$ and $y$.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/cutset.svg" alt="cutset"&gt;&lt;/p&gt;
&lt;p&gt;Is $x$ a cutpoint of $A$? No, it is not because $$A-\{x\}=(A_1-\{x\})\cup\bigcup\limits_{i=2}^5 A_i\cup(A_6-\{x\}).$$ Each of the sets being unioned are connected and the intersection of each set with the next is nonempty, so $A-\{x\}$ is connected. If we permute the indices of the $A_i$, the same logic tells us that $y$ is not a cutpoint of $A$.&lt;/p&gt;
&lt;p&gt;However, $\{x,y\}$ is a cutset of $A$ (or, equivalently, $y$ is a cutpoint of $A-\{x\}$). That's because&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U &amp;amp;= (A_1-\{x\})\cup(A_2-\{y\}) \\&lt;br&gt;
V &amp;amp;= (A_3-\{y\})\cup\bigcup\limits_{i=4}^5 A_i\cup (A_6-\{x\})&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;is a separation of $A-\{x,y\}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How can we use cutsets to distinguish between connected spaces? I'm glad you ask.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem. Let $f:X\to Y$ be a homeomorphism between connected spaces $X$ and $Y$. If $A$ is a cutset of $X$, then $f[A]$ is a cutset of $Y$.&lt;/p&gt;
&lt;p&gt;Proof. Since $X-A$ is disconnected, there exists a separation $U,V$ of $X-A$. We argue that $f[U]$ and $f[V]$ form a separation of $Y-f[A]$. Because $f$ is a homeomorphism, $f^{-1}$ is continuous and thus $f[U]=(f^{-1})^{-1}[U]$ and $f[V]=(f^{-1})^{-1}[V]$ are open in $Y$. Since $U$ and $V$ are nonempty, certainly $f[U]$ and $f[V]$ are nonempty. Observe that, because $U\cap V=\varnothing$,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f[U]\cap f[V]&amp;amp;=(f^{-1})^{-1}[U]\cap(f^{-1})^{-1}[V]\\&lt;br&gt;
&amp;amp;=(f^{-1})^{-1}[U\cap V]\\&lt;br&gt;
&amp;amp;=f[U\cap V]\\&lt;br&gt;
&amp;amp;=f[\varnothing]\\&lt;br&gt;
&amp;amp;=\varnothing.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Furthermore, because $U\cup V=X-A$ and $f$ is bijective,&lt;/p&gt;
&lt;p&gt;$$\begin{align}f[U]\cup f[V]&amp;amp;=f[U\cup V]\\&lt;br&gt;
&amp;amp;=f[X-A]\\&lt;br&gt;
&amp;amp;=f[X]-f[A]\\&lt;br&gt;
&amp;amp;=Y-f[A].&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Thus, $f[U]$ and $f[V]$ separate $Y-f[A]$. It follows that $f[A]$ is a cutset of $Y$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To see how this can be applied, let's take a look at another example.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The subspace $[0,2\pi)$ of $\R$ and the unit circle $S^1=\{(x,y)\in\R^2\mid x^2+y^2=1\}$ as a subspace of $\R^2$ are not homeomorphic. I have not yet shown that either of these sets is connected, but for now we will take it for granted that they both are. Any point $x\in [0,2\pi)$ with $x\ne 0$ is a cutpoint of $[0,2\pi)$ because $$[0,2\pi)-\{x\}=[0,x)\cup (x,2\pi),$$ which is already expressed as the union of its separation. On the other hand, $S^1$ has no cutpoints. For any $x\in S^1$, the space $S^1-\{x\}$ is homeomorphic to an open interval (which is connected). Any cutset of $S^1$ would therefore consist of at least two points, and so cannot be the homeomorphic image of a cutpoint for $[0,2\pi)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I was just very hand-wavy by claiming that certain sets were connected without proving it. Let me at least prove that $\R$ is connected, from which it will immediately follow that open intervals are connected (because they are homeomorphic to $\R$). The proof will require the following fact about the real numbers, which itself needs to be prefaced with the following definitions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given a subset $A$ of $\R$, a number $x\in\R$ is an &lt;strong&gt;upper bound&lt;/strong&gt; for $A$ if $x\ge a$ for every $a\in A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A subset $A$ of $\R$ is &lt;strong&gt;bounded above&lt;/strong&gt; if an upper bound for $A$ exists.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given a subset $A$ of $\R$, a number $x\in\R$ is the &lt;strong&gt;supremum&lt;/strong&gt; (or &lt;strong&gt;least upper bound&lt;/strong&gt;) of $A$ if $x$ is an upper bound for $A$ and $x\le y$ for every upper bound $y$ of $A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Least Upper Bound Property.&lt;/strong&gt; Any nonempty subset of $\R$ which is bounded above has a supremum.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is one of the defining properties of the real numbers, and it is in fact equivalent to the fact that every Cauchy sequence in $\R$ converges (if you've taken an analysis class, this should seem familiar). We will take the least upper bound property for granted, although it can be proven from the construction of the real numbers as either Dedekind cuts or equivalence classes of rational Cauchy sequences. You don't need to understand any of this to move forward. Just smile and nod while you wait for me to shut up.&lt;/p&gt;
&lt;p&gt;Here's the proof that $\R$ is connected. It's pretty gross.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; The set $\R$ of real numbers, equipped with the standard topology, is connected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We proceed by contradiction, supposing that $\R$ is not connected. This means that there exist open sets $U$ and $V$ which separate $\R$. Choose $u\in U$ and $v\in V$, and assume without loss of generality that $u&amp;lt;v$. Define $U'=[u,v]-V$ and $V'=[u,v]-U$. Using De Morgan's law and the fact that $V\cap U=\varnothing$, it follows that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U'\cup V'&amp;amp;=\big([u,v]-V\big)\cup\big([u,v]-U\big)\\&lt;br&gt;
&amp;amp;=[u,v]-(V\cap U)\\&lt;br&gt;
&amp;amp;=[u,v]-\varnothing\\&lt;br&gt;
&amp;amp;=[u,v].&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;The set $U'$ is nonempty because $u\in U'$ and it is bounded above by $v$, and so $U'$ has a supremum, $s$. Clearly $s\in [u,v]$, so either $s\in U'$ or $s\in V'$.&lt;/p&gt;
&lt;p&gt;Suppose first that $s\in U'$. Then because $U'$ is open in $[u,v]$ and $v\notin U'$, there exists $d\in[u,v]$ for which $[s,d)\subseteq U'$. Then for every $x\in (s,d)$, we have that $x\in U'$ and $x&amp;gt;s$, which is a contradiction because $s$ is the supremum of $U'$.&lt;/p&gt;
&lt;p&gt;Suppose then that $s\in V'$. Then because $V'$ is open in $[u,v]$ and $u\notin V'$, there exists $d\in (u,s)$ for which $(d,s]\subseteq V'$. Then for every $x\in (d,s)$, we have that $x&amp;lt;s$ and $x&amp;gt;y$ for every $y\in U'$, which is again a contradiction because $s$ is the supremum of $U'$.&lt;/p&gt;
&lt;p&gt;Since $s\notin U'$ and $s\notin V'$, it must be true that $s\notin [u,v]$, which contradicts its definition.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="somecoolresultsanamesomecoolresults"&gt;Some Cool Results&lt;a name="some-cool-results"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;It's time for some fun stuff now. Let's start with the intermediate value theorem, which you may have experienced and ignored in a calculus class.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Intermediate Value Theorem.&lt;/strong&gt; Let $X$ denote a connected space and let $f:X\to\R$ be continuous. If $x,y\in f[X]$ and $c\in [x,y]$, then $c\in f[X]$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; It suffices to consider $c\in (x,y)$, because if $x\in\{x,y\}$ there is nothing to prove. Since $X$ is connected and $f$ is continuous, $f[X]$ is connected in $\R$. We proceed by contradiction.&lt;/p&gt;
&lt;p&gt;Suppose $c\notin f[X]$ and define $U'=(-\infty,c)$ and $V'=(c,\infty)$. Clearly $U'\cap V'=\varnothing$ and $f[X]\subseteq U'\cup V'$. We argue that $U=U'\cap f[x]$ and $V=V'\cap f[X]$ form a separation of $f[X]$. Since $x\in U'$ and $y\in V'$, we have that $U$ and $V$ are nonempty. Furthermore,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U\cap V &amp;amp;= (U'\cap f[X])\cap(V'\cap f[X])\\&lt;br&gt;
&amp;amp;= (U'\cap V')\cap f[X]\\&lt;br&gt;
&amp;amp;= \varnothing\cap f[X]\\&lt;br&gt;
&amp;amp;= \varnothing.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;In addition, because $f[X]\subseteq U'\cup V'$, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U\cup V &amp;amp;= (U'\cap f[X])\cup(V'\cap f[X])\\&lt;br&gt;
&amp;amp;= (U'\cup V')\cap f[X]\\&lt;br&gt;
&amp;amp;= f[X].&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Thus, $U$ and $V$ separate $f[X]$, which is a contradiction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This theorem has many important consequences, but I won't spend too long talking about them right now because I want to get to the Brouwer fixed point theorem. We'll see the intermediate value theorem again though, I promise.&lt;/p&gt;
&lt;p&gt;Of course, before I can talk about a fixed point theorem, it would help if I told you what fixed points were.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given a set $X$, a function $f:X\to X$ has a &lt;strong&gt;fixed point&lt;/strong&gt; $x\in X$ if $f(x)=x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So fixed points are points which stay fixed after a function acts on them.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;One-Dimensional Brouwer Fixed Point Theorem.&lt;/strong&gt; Every continuous map $f:[-1,1]\to[-1,1]$ has a fixed point.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We proceed by contradiction. Suppose $f:[-1,1]\to[-1,1]$ is continuous but has no fixed point. That is, $f(x)\ne x$ for every $x\in[-1,1]$. Define $g:[-1,1]\to\{-1,1\}$ by&lt;/p&gt;
&lt;p&gt;$$g(x) =&lt;br&gt;
\begin{cases}&lt;br&gt;
-1 &amp;amp; \text{if } f(x) &amp;gt; x, \\&lt;br&gt;
\phantom{-}1 &amp;amp; \text{if } f(x) &amp;lt; x.&lt;br&gt;
\end{cases}$$&lt;/p&gt;
&lt;p&gt;There is an alternative way of writing this: $$g(x)=\frac{f(x)-x}{\abs{f(x)-x}}.$$ Because $f(x)\ne x$ for every $x\in[-1,1]$, clearly $f(x)-x\ne 0$ and $g$ is thus a continuous function. Because $g$ is continuous and its codomain is a discrete two-point set, it must be that $g$ is constant. However, we also have that $g(-1)=1$ and $g(1)=-1$, and so $g$ is not constant - a contradiction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This might all seem a bit abstract right now, so maybe a diagram will shed some light on why this theorem makes sense.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/brouwer1d.svg" alt="brouwer1d"&gt;&lt;/p&gt;
&lt;p&gt;Here's a graph of some continuous function $f:[-1,1]\to[-1,1]$. Clearly any such function must intersect the diagonal of the square in at least one point. Any such intersection is a fixed point of $f$.&lt;/p&gt;
&lt;p&gt;There is a more general Brouwer fixed point theorem which works for any dimension, although we cannot prove it yet. The beginning of the proof is very similar to the above proof, though, so let's get as far as we can go. (Recall the $\partial A$ denotes the boundary of a set $A$.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Brouwer Fixed Point Theorem.&lt;/strong&gt; Let $B^n$ denote the closed unit ball in $\R^n$. Then every continuous function $f:B^n\to B^n$ has a fixed point.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Beginning of Proof.&lt;/strong&gt; Again, we proceed by contradiction and suppose that $f$ has no fixed points, i.e., $f(x)\ne x$ for every $x\in B^n$. We define a new function $g:B^n\to\partial B^n$ by $$g(x)=\frac{f(x)-x}{d\big(f(x),x\big)},$$ where $d$ is the standard metric.&lt;/p&gt;
&lt;p&gt;We can visualize this function as follows: since every point $x$ is distinct from its image $f(x)$, there is a unique line segment in $B^n$ starting at $f(x)$ and passing through $x$ which intersects the boundary $\partial B^n$ in exactly one point, $g(x)$.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/brouwernd.svg" alt="brouwernd"&gt;&lt;/p&gt;
&lt;p&gt;Since $f(x)\ne x$ for every $x\in B^n$, $d\big(f(x),x)&amp;gt;0$ for every $x\in B^n$ and thus $g$ is continuous. Furthermore, $g(x)=x$ for every $x\in\partial B^n$ This should lead to a contradiction, but we can't show this right now.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The contradiction is that there is no continuous function which maps the ball to its boundary while keeping the boundary fixed. You can probably visualize why this is in your head - any such &amp;quot;retraction&amp;quot; would require tearing a hole in the ball. Unfortunately, we need the idea of homology in order to establish that there is no retraction to the ball's boundary, and we are a long way from that.&lt;/p&gt;
&lt;p&gt;I think there's time for one last proof. Basically, we will prove that fixed point properties are preserved by homeomorphism.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Suppose $X$ is a topological space for which every continuous function $f:X\to X$ has a fixed point, and that $Y$ is a space homeomorphic to $X$. Then every continuous function $g:Y\to Y$ has a fixed point.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Let $h:X\to Y$ be a homeomorphism, and let $g:Y\to Y$ be continuous. By hypothesis, $h$ and $h^{-1}$ are continuous, so $h^{-1}\circ g\circ h:X\to X$ is continuous since it is the composition of continuous functions. Thus, $h^{-1}\circ g\circ h$ has a fixed point. That is, there exists some $x\in X$ for which $\big(h^{-1}\circ g\circ h\big)(x)=x$. But then $\big(g\circ h\big)(x)=h(x)$, and thus $h(x)$ is a fixed point of $g$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I'll leave you with the following interesting fact that might occasionally occur to you as you stir hot beverages: the Brouwer fixed point theorem, together with the theorem above, tells us that some particle of your drink will end up in the same place it began. This is because the space occupied by the liquid (presumably a cylinder) is homeomorphic to the three-dimensional closed unit ball.&lt;/p&gt;
</content:encoded></item><item><title>Constructing the Rational Numbers (1)</title><description>We work with number systems every day, but we just sort of take their existence for granted. However, it is possible to construct all of these number systems from scratch.</description><link>http://localhost:2368/constructing-the-rational-numbers-1/</link><guid isPermaLink="false">5c6ce0a7a4e2f60287eae2a9</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Wed, 20 Feb 2019 05:08:09 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#construction"&gt;The Construction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="introductionanameintroduction"&gt;Introduction&lt;a name="introduction"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;It's been a very long time since I've posted, so I figured I'd kick things off again with one of my favorite topics.&lt;/p&gt;
&lt;p&gt;We work with number systems every day, but we just sort of take their existence for granted. However, it is possible to construct all of these number systems from scratch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The natural numbers are built from sets.&lt;/li&gt;
&lt;li&gt;The integers are built from natural numbers.&lt;/li&gt;
&lt;li&gt;The rational numbers are built from integers.&lt;/li&gt;
&lt;li&gt;The real numbers are built from rational numbers.&lt;/li&gt;
&lt;li&gt;The complex numbers are built from real numbers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We could go on and on, since there are also quaternions, octonions and god knows what else.&lt;/p&gt;
&lt;p&gt;There is an obvious hierarchy here, and if I wanted to do things right I would start off at the very lowest level by constructing the natural numbers. Maybe I'll do a post on each of these constructions at some point, but for now I think I'll start in the middle.&lt;/p&gt;
&lt;p&gt;We are going to assume that we already have the set $\mathbb{Z}$ of integers and all of their properties, and we will work from there. That means we know everything about their arithmetic (addition, subtraction and multiplication), as well as their other properties such as order.&lt;/p&gt;
&lt;p&gt;The first thing to consider whenever we want to construct a new number system is what is missing from what we already have? What is not present in the set of integers that we would like to be there? What sorts of problems can we phrase in terms of integers that don't have integer solutions but should have some sort of solution?&lt;/p&gt;
&lt;p&gt;What immediately springs to mind is the following sort of equation:&lt;/p&gt;
&lt;p&gt;$$2x = 1.$$&lt;/p&gt;
&lt;p&gt;Obviously we want to scream out to the heavens that $x=\frac{1}{2}$. However, there is no such thing as a half in the set of integers. We have no concept of fractions or division, and so the above equation actually has no solution right now.&lt;/p&gt;
&lt;p&gt;So we'd like our rational numbers to be able to fill in this sort of gap and provide answers to equations of the form $ax = b$, where $a$ and $b$ are integers. But there's an even more obvious criterion we would like our new numbers to satisfy.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/integer_line.svg" alt="integer_line"&gt;&lt;/p&gt;
&lt;p&gt;Here's a traditional illustration of the &amp;quot;number line.&amp;quot; It may seem weird to even think about this since it's so ingrained in us after years of doing mathematics, but why do we draw our numbers on a line? Well the reason they can be layed out linearly to begin with is their order: $2$ comes after $1$ and before $3$, etc. But putting them on a line like this suggests something else – that there should be something &lt;em&gt;between&lt;/em&gt; them.&lt;/p&gt;
&lt;p&gt;That is, we would like our new rational numbers to have the property that between any two rational numbers there is another rational number. This is certainly something that the integers don't obey. There is no integer between $1$ and $2$.&lt;/p&gt;
&lt;p&gt;Lastly, we would like our rational numbers to &lt;em&gt;extend&lt;/em&gt; the integers in such a way that we can view the integers as sitting &amp;quot;inside&amp;quot; them, as on the number line. Furthermore, we would like the rational numbers to extend their arithmetic as well. Addition, multiplication and subtraction should all be defined and compatible when we restrict ourselves to talking about integers.&lt;/p&gt;
&lt;p&gt;Let's summarize all of that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Desired Properties of the Rational Numbers&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a subset of the rational numbers which behaves exactly like the integers.&lt;/li&gt;
&lt;li&gt;Rational numbers can be multiplied, added or subtracted in a way that extends the integers and has all the usual properties.&lt;/li&gt;
&lt;li&gt;The rational numbers can be ordered in a way that extends the integers.&lt;/li&gt;
&lt;li&gt;If $a$ and $b$ are integers with $a\ne 0$, there exists a rational number $x$ for which $ax=b$.&lt;/li&gt;
&lt;li&gt;If $p$ and $q$ are rational numbers with $p&amp;lt;q$, there exists a rational number $x$ for which $p&amp;lt;x&amp;lt;q$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;If whatever construction we come up with has all of the above properties, we'll have been successful. With all of that in mind, let's get started.&lt;/p&gt;
&lt;h3 id="theconstructionanameconstruction"&gt;The Construction&lt;a name="construction"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We have a significant advantage here in that we already know exactly what our rational numbers should end up looking like: they should resemble fractions $\frac{p}{q}$ where $p$ and $q$ are integers and $q\ne 0$. We even know what their arithmetic should like like in terms of these fractions. Thus, we are able to draw inspiration from our preconceived notion of what they are and how they should behave. However, we will have to build the concept of &amp;quot;fraction&amp;quot; from the ground up, since it does not exist for us currently.&lt;/p&gt;
&lt;p&gt;We might not have fractions, but we already have the next best thing – &lt;strong&gt;cartesian products&lt;/strong&gt;. Basically a fraction is just a pair of integers, right? So instead of $\frac{p}{q}$, why not just write $(p, q)$? This is actually very close to what we'll end up doing, but it doesn't quite get us where we need to be.&lt;/p&gt;
&lt;p&gt;To see why, recall that any fraction has infinite equivalent representations. For example,&lt;/p&gt;
&lt;p&gt;$$\frac{1}{2}=\frac{2}{4}=\frac{-100}{-200}=\frac{1024}{2048}=\cdots$$&lt;/p&gt;
&lt;p&gt;Unfortunately, our ordered pair idea doesn't allow for this sort of equivalent representation. Certainly $(1, 2)\ne (2, 4)$. They are completely different ordered pairs because their components are different integers! However, we are already equipped with a way to identify these ordered pairs – as a quotient set, by defining an equivalence relation on them.&lt;/p&gt;
&lt;p&gt;At this point, if you have not read my post on &lt;a href="http://localhost:2368/equivalence-relations-and-quotient-sets/"&gt;Equivalence Relations and Quotient Sets&lt;/a&gt;, I would strongly encourage you to pause here and give it a thorough read. It is the main tool we will be using in our construction and is therefore of critical importance to the rest of this post.&lt;/p&gt;
&lt;p&gt;The idea here is to define an equivalence relation $\sim$ on $\mathbb{Z}\times\mathbb{Z}^*$ (the set of ordered pairs of integers whose second component is nonzero) for which&lt;/p&gt;
&lt;p&gt;$$(1, 2) \sim (2, 4) \sim (-100, -200) \sim (1024, 2048) \sim \cdots$$&lt;/p&gt;
&lt;p&gt;and more generally, if $(p, q)\in\mathbb{Z}\times\mathbb{Z}^*$ and $n\in\mathbb{Z}$ then $(p, q) \sim (np, nq)$.&lt;/p&gt;
&lt;p&gt;Let's look back to our intuitive understanding of fractions to see how we might define this equivalence relation. If two fractions are equal, we can &amp;quot;cross multiply&amp;quot; them to obtain an expression purely in terms of integers. That is, if $\frac{a}{b}=\frac{c}{d}$ then $ad=bc$. We will use this to define the following relation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; We define the relation $\sim_\mathbb{Q}$ on the set $\mathbb{Z}\times\mathbb{Z}^*$ as follows:&lt;/p&gt;
&lt;center&gt;$(a, b) \sim_\mathbb{Q} (c, d)$ if and only if $ad=bc$,&lt;/center&gt;&lt;br&gt;
&lt;p&gt;where $a,b,c$ and $d$ are integers with $b,d\ne 0$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the definition above, $\sim_\mathbb{Q}$ is just a symbol meant to distinguish this particular relation. This is not a standard notation or something you will ever need to use again outside of the construction in this post.&lt;/p&gt;
&lt;p&gt;Things are looking good so far. We've managed to rephrase equivalence of fractions solely in terms of ordered pairs of integers. Let's not get too far ahead of ourselves, though. We need to show that this is actually an equivalence relation!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; The relation $\sim_\mathbb{Q}$ is an equivalence relation on the set $\mathbb{Z}\times\mathbb{Z}^*$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We need to show that $\sim_\mathbb{Q}$ is symmetric, reflexive and transitive.&lt;/p&gt;
&lt;p&gt;We argue first that it is symmetric. Choose $a, b\in Z$ with $b\ne 0$. Certainly $ab=ba$ since multiplication of integers is commutative and so $(a, b) \sim_\mathbb{Q} (a, b)$ by the definition of this relation.&lt;/p&gt;
&lt;p&gt;We argue next that it is reflexive. Choose $a,b,c,d\in\mathbb{Z}$ with $b,d\ne 0$ and suppose that $(a,b) \sim_\mathbb{Q} (c,d)$. Then, by definition, we have that $ad=bc$. Again, from the commutativity of integer multiplication, we have that $cb=da$. Thus $(c,d) \sim_\mathbb{Q} (a,b)$.&lt;/p&gt;
&lt;p&gt;Lastly, we argue that it is transitive. Choose $a,b,c,d,e,f\in\mathbb{Z}$ with $b,d,f\ne 0$. Suppose that $(a,b) \sim_\mathbb{Q} (c,d)$ and that $(c,d) \sim_\mathbb{Q} (e,f)$. Then $ad=bc$ and $cf=de$ by definition. Since $cf$ and $de$ are equal, we may multiply the respective sides of the equation $ad=bc$ by these quantities without affecting the equality. That is, $adcf=bcde$. By commutativity, we then have $afdc=bedc$, and so $af=be$. Thus $(a,b) \sim_\mathbb{Q} (e,f)$, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I would be remiss if I failed to mention that in proving the transitivity of $\sim_\mathbb{Q}$ above, we used a property of the integers that might require some explaining. We cancelled the quantity $dc$ from both sides of an equation involving only integers. However, the integers don't have a concept of division! What gives?&lt;/p&gt;
&lt;p&gt;We may not be able to divide integers, but we can still cancel them as we did above. Even though I said before we would assume perfect knowledge of all properties of the integers, I think this one merits special mention since it is not mentioned often outside of a modern algebra course.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Right Cancellation Property of the Integers.&lt;/strong&gt; If $a,b$ and $c$ are integers with $c\ne 0$ and $ac=bc$, then $a=b$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $ac=bc$, we may subtract $bc$ from both sides to obtain the equation $ac-bc=0$. Factoring this yields $(a-b)c=0$. This can only be the case if either $a-b=0$ or $c=0$, since the integers have no zero divisors. However, $c\ne 0$ by hypothesis, and so it must be that $a-b=0$. That is, $a=b$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We used this cancellation property above to cancel the quantity $dc$. There is an analagous left cancellation property, but I think that is obvious and symmetrical enough that I do not need to go into it in detail here.&lt;/p&gt;
&lt;p&gt;Anyway, back to business! We've demonstrated that $\sim_\mathbb{Q}$ is an equivalence relation. This allows us to construct the following quotient set:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; We define the set of rational numbers to be the quotient set&lt;/p&gt;
&lt;p&gt;$$\mathbb{Q}=(\mathbb{Z}\times\mathbb{Z}^*)/\negthickspace\sim_\mathbb{Q}.$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is simultaneously a really beautiful idea and a really ugly expression. And if you're confused by this, let's take a step back and examine what this definition really means.&lt;/p&gt;
&lt;p&gt;Recall that the quotient set defined by an equivalence relation is the set of all of its equivalence classes. What do the equivalence classes look like in this case? Well, they are the sets of all ordered pairs which are equivalent. For instance, the following are all elements of $\mathbb{Q}$:&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
[(1,2)] &amp;amp;= \{(1,2),(-2,4),(3,-6),\ldots\}, \\&lt;br&gt;
[(-3,4)] &amp;amp;= \{(-3,4),(3,-4),(-30,40),\ldots\}, \\&lt;br&gt;
[(5,1)] &amp;amp;= \{(5,1),(15,3),(45,9),\ldots\}, \\&lt;br&gt;
[(0,1)] &amp;amp;= \{(0,1),(0,2),(0,-4),\ldots\}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;And that's because&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(1,2) &amp;amp; \sim_\mathbb{Q} (-2,4) \sim_\mathbb{Q} (3,-6) \sim_\mathbb{Q} \cdots, \\&lt;br&gt;
(-3,4) &amp;amp; \sim_\mathbb{Q} (3,-4) \sim_\mathbb{Q} (-30,40) \sim_\mathbb{Q} \cdots, \\&lt;br&gt;
(5,1) &amp;amp; \sim_\mathbb{Q} (15,4) \sim_\mathbb{Q} (45,9) \sim_\mathbb{Q} \cdots, \\&lt;br&gt;
(0,1) &amp;amp; \sim_\mathbb{Q} (0, 2) \sim_\mathbb{Q} (0, -4) \sim_\mathbb{Q} \cdots.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Essentially all we've done is taken, for instance, all of the pairs which we think should correspond to $\frac{1}{2}$ and we've collapsed them down into a single equivalence class called $[(1,2)]$. In this manner, every rational number is an equivalence class of these ordered pairs of integers.&lt;/p&gt;
&lt;p&gt;Now that we have our rational numbers, we still need to define their arithmetic. We could technically do this however we wanted, but obviously we would like their arithmetic to coincide with our preconceived ideas of fractional arithmetic.&lt;/p&gt;
&lt;p&gt;For example, we could try to define addition as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Incorrect Definition.&lt;/strong&gt; Given two rational numbers $[(a,b)]$ and $[(c,d)]$, we &lt;em&gt;incorrectly&lt;/em&gt; define their &lt;strong&gt;sum&lt;/strong&gt; to be $$[(a,b)] + [(c,d)] = [(a+c,b+d)].$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are two reasons why this is a bad definition. First, because in the language of fractions this would translate to $\frac{a}{b}+\frac{c}{d}=\frac{a+b}{c+d}$, which our elementary school teachers drilled into our heads was &lt;em&gt;WRONG&lt;/em&gt;. (Maybe the phrase &amp;quot;common denominator&amp;quot; is echoing around your head right now.) This definition simply does not correspond to our physical intuition of what should happen when we add fractions.&lt;/p&gt;
&lt;p&gt;But there is an even more fundamental reason why this definition cannot be correct. And it's a little bit subtle, so I'll try to break it down the best that I can.&lt;/p&gt;
&lt;p&gt;To see why this &amp;quot;addition&amp;quot; doesn't even work, let's try to add the rational numbers $[(1,2)]$ and $[(1,4)]$. According to the above definition,&lt;/p&gt;
&lt;p&gt;$$[(1,2)] + [(1,3)] = [(1+1, 2+3)] = [(2,5)].$$&lt;/p&gt;
&lt;p&gt;However, we know that $[(1,2)]=[(2,4)]$. But the above definition gives us&lt;/p&gt;
&lt;p&gt;$$[(2,4)] + [(1,3)] = [(2+1, 4+3)] = [(3,7)].$$&lt;/p&gt;
&lt;p&gt;Obviously $[(2,5)] \ne [(3,7)]$ since $(2,5)\not\sim_\mathbb{Q} (3,7)$, and so we have a serious problem here. The issue is that this notion of addition is not &lt;strong&gt;well defined&lt;/strong&gt;. That is, the result of our addition is different depending on which representative ordered pairs we choose for our equivalence classes. This is unacceptable, because it leads to nonsensical results.&lt;/p&gt;
&lt;p&gt;With that in mind, let's work toward the correct definition. In terms of fractions, we would expect $\frac{a}{b}+\frac{c}{d}=\frac{ad+bc}{bd}$. And in fact, this is exactly how we shall proceed.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given two rational numbers $[(a,b)]$ and $[(c,d)]$, we define their &lt;strong&gt;sum&lt;/strong&gt; to be $$[(a,b)] + [(c,d)] = [(ad+bc,bd)].$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In light of the disaster that was the previous attempt at a definition, we need to verify that this notion of addition is well defined. That is, the result does not depend on our choice of representatives.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; Addition of rational numbers is well defined.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose that $[(a_1,b_1)]=[(a_2,b_2)]$ and $[(c_1,d_1)]=[(c_2,d_2)]$. We need to show that $[(a_1,b_1)]+[(c_1,d_1)]=[(a_2,b_2)]+[(c_2,d_2)]$.&lt;/p&gt;
&lt;p&gt;Since $(a_1,b_1)\sim_\mathbb{Q}(a_2,b_2)$, we have that $a_1b_2=a_2b_1$. Similarly, since $(c_1,d_1)\sim_\mathbb{Q}(c_2,d_2)$, we have that $c_1d_2=c_2d_1$. We note that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(a_1d_1+b_1c_1)b_2d_2 &amp;amp;= a_1b_2d_1d_2 + b_1b_2c_1d_2 \\&lt;br&gt;
&amp;amp;= a_2b_1d_1d_2 + b_1b_2c_2d_1 \\&lt;br&gt;
&amp;amp;= b_1d_1(a_2d_2+b_2c_2).&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;But by definition this means that $(a_1d_1+b_1c_2, b_1d_1) \sim_\mathbb{Q} (a_2d_2+b_2c_2, b_2d_2)$. That is, $[(a_1,b_1)]+[(c_1,d_1)]=[(a_2,b_2)]+[(c_2,d_2)]$, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thank goodness! This definition of addition is both mathematically legal and matches what we would intuitively expect. So let's move on.&lt;/p&gt;
&lt;p&gt;Logically, the next thing to do is work toward defining multiplication of rational numbers. This is very similar to defining addition. Given what we know about fractions, we expect $\frac{a}{b}\cdot\frac{c}{d}=\frac{ac}{bd}$. By now, hopefully you can guess what the definition will look like.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given two rational numbers $[(a,b)]$ and $[(c,d)]$, we define their &lt;strong&gt;product&lt;/strong&gt; to be $$[(a,b)] \cdot [(c,d)] = [(ac,bd)].$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Just like we did for addition, we need to show that multiplication is well defined.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; Multiplication of rational numbers is well defined.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose that $[(a_1,b_1)]=[(a_2,b_2)]$ and $[(c_1,d_1)]=[(c_2,d_2)]$. We need to show that $[(a_1,b_1)] \cdot [(c_1,d_1)] = [(a_2,b_2)] \cdot [(c_2,d_2)]$.&lt;/p&gt;
&lt;p&gt;Since $(a_1,b_1)\sim_\mathbb{Q}(a_2,b_2)$, we have that $a_1b_2=a_2b_1$. Similarly, since $(c_1,d_1)\sim_\mathbb{Q}(c_2,d_2)$, we have that $c_1d_2=c_2d_1$. We note that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
a_1c_1b_2d_2 &amp;amp;= (a_1b_2)(c_1d_2) \\&lt;br&gt;
&amp;amp;= (a_2b_1)(c_2d_1) \\&lt;br&gt;
&amp;amp;= b_1d_1a_2c_2.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;But by definition this means that $(a_1c_1, b_1d_1) \sim_\mathbb{Q} (a_2c_2, b_2d_2)$. That is, $[(a_1,b_1)]\cdot[(c_1,d_1)]=[(a_2,b_2)]\cdot[(c_2,d_2)]$, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This post is getting long, so I'm going to leave it here for now and continue the construction in a later post, along with the verification of our desired properties of the rational numbers. Until next time! :)&lt;/p&gt;
</content:encoded></item><item><title>Sequences, Hausdorff Spaces and Nets</title><description>I'm now going to talk about sequences and nets, which often provide an alternative way of describing topological phenomena. I'll also talk about Hausdorff spaces, which have all sorts of nice properties.</description><link>http://localhost:2368/sequences-hausdorff-spaces-and-nets/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae222</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Thu, 20 Apr 2017 21:11:25 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#sequences"&gt;Sequences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hausdorff-spaces"&gt;Hausdorff Spaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#nets"&gt;Nets&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I'm now going to talk about sequences and nets, which often provide an alternative way of describing topological phenomena. I'll also talk about Hausdorff spaces, which have all sorts of nice properties. I was originally planning to include filters in this discussion as well, but I think if I did that this post might become long enough to break the internet.&lt;/p&gt;
&lt;h3 id="sequencesanamesequences"&gt;Sequences&lt;a name="sequences"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;If you've taken a calculus class (or maybe even if you haven't) then you probably already have some notion of what sequences are. They're basically just lists of elements that go on forever. For instance,&lt;/p&gt;
&lt;p&gt;$$\begin{gather}&lt;br&gt;
\begin{aligned}&lt;br&gt;
(0,1,2,3,4,5,6,7,\dotsc)\\&lt;br&gt;
(1,1,2,3,5,8,13,\dotsc)\\&lt;br&gt;
(\text{cat},\text{cat},\text{cat},\text{cat},\dotsc)&lt;br&gt;
\end{aligned}&lt;br&gt;
\end{gather}$$&lt;/p&gt;
&lt;p&gt;are all sequences. The first two have entries in $\mathbb{N}$ and the third takes values in some set of animals.&lt;/p&gt;
&lt;p&gt;Notice that there is always one entry for each natural number. That is, there is a zeroth entry, a first entry, a second entry, and so on. The order in which these entries appear does matter, so put them in parentheses rather than set brackets to distinguish them from sets. Sequences have two main differences from countable infinite sets: they are ordered, and the same point can appear more than once. This important point leads us to the following rigorous definition of a sequence:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;sequence&lt;/strong&gt; in a topological space $X$ is a function $x:\mathbb{N}\to X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is perhaps a bit confusing to actually think of sequences as functions. The definition above is simply meant to give the &amp;quot;ordered list of points&amp;quot; idea some rigorous footing. We generally write $x_n$, rather than $x(n)$, to denote the $n$th term in a sequence. This means we can write a sequence as $(x_0,x_1,x_2,\dotsc)$. This is sometimes shortened to either $(x_n)_{n=0}^\infty$ or $(x_n)_{n\in\mathbb{N}}$.&lt;/p&gt;
&lt;p&gt;Next, let's talk about convergence. This can be a tricky business, and it is the bane of many Calculus II students' existence. The concept of convergence is not itself terribly complicated — it is the process of figuring out whether a specific sequence converges which can sometimes be unreasonably challenging. To start, let's look at convergence in metric spaces so that we can make use of the familiar notion of distance.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A sequence $(x_n)_{n\in\mathbb{N}}$ in a metric space $X$ converges to a point $x\in X$ if for every real number $\epsilon&amp;gt;0$ there is some natural number $N$ for which $d(x,x_n)&amp;lt; \epsilon$ whenever $n&amp;gt;N$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; If a sequence $(x_n)_{n\in\mathbb{N}}$ converges to a point $x$, we say that $x$ is the &lt;strong&gt;limit&lt;/strong&gt; of that sequence and we write $\lim\limits_{n\to\infty}x_n=x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's a bit of a mouthful, so let's spend a little bit of time making sure we know what we're getting ourselves into. Essentially what I mean when I say that a sequences converges to a point $x$ is that eventually everything in the sequence becomes as close to $x$ as I want. More precisely, given $\epsilon &amp;gt; 0$, I want everything beyond the $N$th entry in the sequence to be within the open ball $B(x,\epsilon)$, where I get to choose $N$. If I can find such an $N$ for every $\epsilon$, then the sequence converges. Generally, $N$ will need to be very large when $\epsilon$ is very close to zero.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the sequence $(x_n)_{n=1}^\infty$ in $\mathbb{R}$ where each $x_n=\frac{1}{n}$. We can visualize this sequence in the following manner:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/1-over-n.svg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Notice that the points in the sequence all lie on the graph of the function $f:\mathbb{R}^+\to\mathbb{R}$ defined by $f(x)=\frac{1}{x}$. This is not surprising, considering we originally defined sequences as functions themselves. That is, this sequence is really the restriction of $f$ to the positive integers, $f\negmedspace\mid_{\mathbb{Z}^+}:\mathbb{Z}^+\to\mathbb{R}$. If you have any experience with this function, you'll believe me when I say that it becomes extremely close to zero and always grows closer to it. It makes sense then that our sequence does the same, so we might guess that it converges to zero. Let's prove this!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; The sequence $(x_n)_{n=1}^\infty$ given by $x_n=\frac{1}{n}$ converges to $0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose $\epsilon&amp;gt;0$ and let $N&amp;gt;\frac{1}{\epsilon}$. If $n&amp;gt;N$, then certainly $n&amp;gt;\frac{1}{\epsilon}$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
d(x_n, 0) &amp;amp;= \vert x_n - 0\vert \\&lt;br&gt;
&amp;amp;= \tfrac{1}{n} \\&lt;br&gt;
&amp;amp;&amp;lt; \epsilon.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You don't really need to remember the proof of this fact, although it's incredibly easy to reproduce — the candidate for $N$ in this case is more obvious than usual. Just remember that $\lim\limits_{n\to\infty}\frac{1}{n}=0$, which should hopefully make a lot of sense to you anyway. This is an important sequence which we will occasional use in the future.&lt;/p&gt;
&lt;p&gt;Also, notice that the sequence we just looked at doesn't actually &lt;em&gt;quite&lt;/em&gt; fit the definition I gave for sequences. That is, it doesn't have an entry for every natural number (in particular, there is no $x_0$). We could easily remedy that by rewriting each term as $\frac{1}{n+1}$ and shifting each entry's index down by one. I chose to write it the way I did because it looks a bit nicer. It is somewhat common to allow sequences to start at any index we like, as we can always translate it into starting at zero using a similar substitution.&lt;/p&gt;
&lt;p&gt;Now, in a calculus or analysis class you would study lots of properties and characteristics of sequences in $\mathbb{R}$ and learn a bunch of tricks to help you show that certain types of sequences in $\mathbb{R}$ converge. However, all of that stuff bores me and I want to talk generally about convergent sequences in topological spaces, not just about $\mathbb{R}$ with the standard topology. This will require a slight reworking of the definition of convergence to eliminate the concept of distance that we have in metric spaces.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A sequence $(x_n)_{n\in\mathbb{N}}$ in a topological space $X$ converges to a point $x\in X$ if for every neighborhood $U$ of $x$, there is a natural number $N$ for which $x_n\in U$ whenever $n&amp;gt;N$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This definition basically replaces open balls with neighborhoods, and shouldn't require too much explanation other than that. It should be clear that this definition, when $X$ is a metric space, is equivalent to the old one because open sets are just unions of open balls.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; If a sequence $(x_n)_{n\in\mathbb{N}}$ in a topological space converges to a point $x$, we say that $x$ is a &lt;strong&gt;limit&lt;/strong&gt; of that sequence and we write $\lim\limits_{n\to\infty}x_n=x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Notice that I've said &amp;quot;a limit,&amp;quot; rather than &amp;quot;the limit&amp;quot; like I did for metric spaces. That's because a convergent sequence in a topological space might actually converge to multiple points. &lt;center&gt;&lt;h1&gt;😱&lt;/h1&gt;&lt;/center&gt; The simplest example of this phenomenon that I can think of is as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Let $X$ be any nonempty set equipped with the trivial topology.&lt;sup class="footnote-ref"&gt;&lt;a href="#fn1" id="fnref1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; Then for any point $x\in X$, the only neighborhood of $x$ is $X$ itself. Certainly for any sequence $(x_n)_{n\in\mathbb{N}}$ in $X$, all terms of the sequence are in $X$. If follows that every sequence in $X$ converges to every point of $X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This might strike you as a bit odd, and I'd agree with you. At the very least, this business of every sequence converging to every point is not very desirable behavior for a topological space. After all, we'd like limits of sequences to be unique. Luckily for us, there is a specific type of space for which this behavior is guaranteed!&lt;/p&gt;
&lt;h3 id="hausdorffspacesanamehausdorffspaces"&gt;Hausdorff Spaces&lt;a name="hausdorff-spaces"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A topological space $X$ is &lt;strong&gt;Hausdorff&lt;/strong&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="#fn2" id="fnref2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; if for every pair of points $x,y\in X$ with $x\ne y$, there exists a neighborhood $U$ of $x$ and a neighborhood $V$ of $y$ such that $U\cap V=\varnothing$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So in a Hausdorff space, distinct points have disjoint neighborhoods. This is clearly not true for spaces with two or more points under the trivial topology, so we're off to a good start. Before I show how this property guarantees uniqueness of limits, I will prove that every metric space is Hausdorff.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ denote a metric space with metric $d:X\times X\to\mathbb{R}$. Then $X$ is Hausdorff when equipped with the topology induced by the metric $d$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose $x,y\in X$ with $x\ne y$. By the definition of a metric, $d(x,y)&amp;gt;0$. Let $r=\frac{d(x,y)}{2}$ and define $U=B(x,r)$ and $V=B(y,r)$. It suffices to show that $U$ and $V$ are disjoint, which we will argue by contradiction.&lt;/p&gt;
&lt;p&gt;Suppose $U\cap V\ne\varnothing$. Then there exists some point $p\in U\cap V$, so $d(x,p)&amp;lt; r$ and $d(y,p)&amp;lt; r$ by the definitions of these open balls. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
d(x,p)+d(y,p) &amp;amp;&amp;lt; 2r \\&lt;br&gt;
&amp;amp;= d(x,y),&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;which violates the triangle inequality. We have reached a contradiction, so the proof is complete.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This tells us right away that things like $\mathbb{R}$ in the standard topology are Hausdorff. Now if we can just show that convergent sequences in Hausdorff spaces have unique limits, then I will definitely have been justified earlier in claiming that metric spaces have unique limits. Let's prove this right now.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ be a nonempty Hausdorff space and let $(x_n)_{n\in\mathbb{N}}$ be a convergent sequences in $X$. Then $(x_n)_{n\in\mathbb{N}}$ has exactly one limit.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $(x_n)_{n\in\mathbb{N}}$ is convergent, we know that it has at least one limit. Thus, it suffices to show that it also has at most one limit. We proceed by contradiction.&lt;/p&gt;
&lt;p&gt;Suppose $(x_n)_{n\in\mathbb{N}}$ converges to both $p_1$ and $p_2$, where $p_1\ne p_2$. Since $X$ is Hausdorff, there exist disjoint neighborhoods $U_1$ of $p_1$ and $U_2$ of $p_2$. From the definition of convergence, we have that $x_n\in U_1$ whenever $n&amp;gt;N_1$ and $x_n\in U_2$ whenever $n&amp;gt;N_2$ for some natural numbers $N_1$ and $N_2$ Let $N=\max\{N_1,N_2\}$. Then clearly $x_n\in U_1\cap U_2$ whenever $n&amp;gt;N$. This is a contradiction, since $U_1$ and $U_2$ are disjoint.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So Hausdorff spaces are desirable in that if a sequence converges, it does so as we'd generally expect it to. I won't go into this in too much detail right now, but all of the thinks we actually think of as &amp;quot;space&amp;quot; are Hausdorff. In fact, the definition of a manifold explicitly requires this property, which we shall see if I ever manage to get that far.&lt;/p&gt;
&lt;p&gt;There are a few more properties of Hausdorff spaces which I'd like to prove before moving on, just because they're interesting. The first is the fact that singleton sets in Hausdorff spaces are closed. Its proof is quite straightforward.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ be a nonempty Hausdorff space. Then for every point $x\in X$, the set $\{x\}$ is closed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $X$ is Hausdorff, for every $y\in X$ with $y\ne x$ there exist disjoint neighborhoods $U_y$ of $x$ and $V_y$ of $y$. It follows from the union lemma that&lt;/p&gt;
&lt;p&gt;$$\bigcup\limits_{y\ne x}V_y = X-\{x\},$$&lt;/p&gt;
&lt;p&gt;and this set is open because it is the union of open sets. Thus, $\{x\}$ is closed because its complement is open.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The next property is a little bit more interesting&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces and suppose $Y$ is Hausdorff. Then the graph of any continuous function $f:X\to Y$, given by&lt;/p&gt;
&lt;p&gt;$$G=\left\{\big(x,f(x)\big)\mid x\in X\right\}$$&lt;/p&gt;
&lt;p&gt;is closed in the product space $X\times Y$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; It suffices to show that $(X\times Y)-G$ is open in $X\times Y$. Choose $(x,y)\in (X\times Y)-G$. Clearly $y\ne f(x)$, so because $Y$ is Hausdorff there exist disjoint neighborhoods $U$ of $y$ and $V$ of $f(x)$. Furthermore, because $f$ is continuous we have that $f^{-1}[V]$ is open in $X$. Notice that $x\in f^{-1}[V]$ by definition.&lt;/p&gt;
&lt;p&gt;Next, choose any point $\big(g,f(g)\big)\in G$, and let us consider separately the cases where $g\in f^{-1}[V]$ and $g\notin f^{-1}[V]$. If $g\in f^{-1}[V]$ then by definition $f(g)\in V$. Thus, $f(g)\notin U$ because $U$ and $V$ are disjoint. It follows that $\big(g,f(g)\big)\notin f^{-1}[V]\times U$. If, on the other hand, $g\notin f^{-1}[V]$ then it follows immediately that $\big(g,f(g)\big)\notin f^{-1}[V]\times U$ from the definition of the Cartesian product.&lt;/p&gt;
&lt;p&gt;Either way, $\big(g,f(g)\big)\notin f^{-1}[V]\times U$ and so we have that $(f^{-1}[V]\times U)\cap G=\varnothing$. Clearly $f^{-1}\times U$ is open as it is the product of open sets. Thus every point $(x,y)\in (X\times Y)-G$ is contained in the open set $f^{-1}[V]\times U$, which is itself contained in $(X\times Y)-G$. It follows that $(X\times Y)-G$ is open in $X\times Y$, so $G$ is closed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a pretty nice result, although it isn't too useful to us right now. At the very least, it tells us that continuous real-valued functions have closed graphs because $\mathbb{R}$ is Hausdorff. The next two theorems should immediately seem useful to you.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Any subspace of a Hausdorff space is Hausdorff.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Let $A$ be a subspace of a Hausdorff space $X$ and choose points $x,y\in A$. Then there exist disjoint neighborhoods in $X$, $U$ of $x$ and $V$ of $y$. It follows that $A\cap U$ is a neighborhood of $x$ in $A$ and $A\cap V$ is a neighborhood of $y$ in $A$. Furthermore,&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
(A\cap U)\cap (A\cap V) &amp;amp;= A\cap (U\cap V) \\&lt;br&gt;
&amp;amp;= A\cap\varnothing \\&lt;br&gt;
&amp;amp;= \varnothing,&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;so $A$ is Hausdorff.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; The product of two Hausdorff spaces is Hausdorff.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Let $X$ and $Y$ denote Hausdorff spaces and choose distinct points $(x_1,y_1)$ and $(x_2,y_2)$ in $X\times Y$. Without loss of generality (the other case is so similar) suppose $x_1\ne x_2$. Then because $X$ is Hausdorff, there exist disjoint neighborhoods $U_1$ of $x_1$ and $U_2$ of $x_2$ in $X$. Note that $U_1\times Y$ and $U_2\times Y$ are both open in $X\times Y$, and that $(x_1,y_1)\in U_1\times Y$ while $(x_2,y_2)\in U_2\times Y$. Furthermore,&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
(U_1\times Y)\cap (U_2\times Y) &amp;amp;= (U_1\cap U_2)\times Y \\&lt;br&gt;
&amp;amp;= \varnothing\times Y \\&lt;br&gt;
&amp;amp;= \varnothing,&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;so $X\times Y$ is Hausdorff.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It can be shown by induction that the product of any finite number of Hausdorff spaces is Hausdorff. It is also possible to show, in fact, that the product of &lt;em&gt;any&lt;/em&gt; collection of Hausdorff spaces is Hausdorff, but I try to avoid talking about infinite Cartesian products unless I have no other choice.&lt;/p&gt;
&lt;p&gt;Given that products and subspaces of Hausdorff spaces inherit Hausdorffness from their parents, you might be tempted to guess that quotients of Hausdorff spaces are Hausdorff. This is wrong in general, although I won't provide a counterexample because this post is already very long and I haven't even started discussing nets yet.&lt;/p&gt;
&lt;p&gt;Unfortunately, before I get to nets I have a few more things about sequences that I would like to talk about. In particular, it would be a shame for me not to prove the following beautiful theorem for you.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces and let $(x_n)_{n\in\mathbb{N}}$ be a sequence which converges to the point $x\in X$. Then for any continuous function $f:X\to Y$, the sequence $\big(f(x_n)\big)_{n\in\mathbb{N}}$ converges to the point $f(x)\in Y$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose any neighborhood $U\subseteq Y$ of $f(x)$. Since $f$ is continuous, $f^{-1}[U]\subseteq X$ is open and clearly $x\in f^{-1}[U]$, so $f^{-1}[U]$ is a neighborhood of $x$. Since $(x_n)_{n\in\mathbb{N}}$ converges to $x$, there exists $N\in\mathbb{N}$ for which $x_n\in f^{-1}[U]$ whenever $n&amp;gt;N$. It follows that $f(x_n)\in U$ whenever $n&amp;gt;N$. Thus, $\big(f(x_n)\big)_{n\in\mathbb{N}}$ converges to $f(x)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This theorem is great because it tells us that continuous functions preserve convergent sequences! It would be even better if the converse was true, because that would give us yet another alternative characterization of continuous functions. Unfortunately, this is not the case without additionally assuming that both spaces are first-countable (a property that I haven't mentioned yet, but that every metric space has). For general spaces, it is also possible for function which aren't continuous to preserve convergent sequences.&lt;/p&gt;
&lt;p&gt;This hints that sequences might not be exactly the right tool to study continuity. The problem is that they are too specific a concept. Let's next look at a generalization of sequences that will solve all of our problems.&lt;/p&gt;
&lt;h3 id="netsanamenets"&gt;Nets&lt;a name="nets"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Before I start trying to explain nets to you, let me state the main theorem we eventually want to prove about them.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces. A function $f:X\to Y$ is continuous if and only if for every net $(x_a)_{a\in A}$ that converges to $x$, the net $\big(f(x_a)\big)_{a\in A}$ converges to $f(x)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In stating this theorem of things to come, I've already given away a fair amount of information about the nature of nets. Namely, the fact that nets look almost exactly like sequences, except perhaps that their entries are indexed over sets other than $\mathbb{N}$. However, nets aren't indexed over just any kind of set — after all, we would still like the entries of a net to progress in some order. Thus, we will define them over sets with a specific type of relation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;preorder&lt;/strong&gt; on a set $X$ is a reflexive and transitive relation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That is, a preorder on $X$ is a relation $\le$ such that $x\le x$ for every $x\in X$, and $x\le z$ whenever $x\le y$ and $y\le z$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;directed set&lt;/strong&gt; is a nonempty set $X$ together with a preorder $\le$ which satisfies the additional property that for any $x,y\in X$, there exists $z\in X$ such that $x\le z$ and $y\le z$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A shorter way of describing this final property of directed sets might be to say that every pair of elements has an upper bound. This ensures that, although some pairs of elements may not be related to each other, they are at least related to some third element. In turn, this guarantees that strange behavior, as in the following example, does not occur.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Just to make sure there's no confusion, this will be an example of a set with a preorder that is &lt;em&gt;not&lt;/em&gt; a directed set, because pairs of elements will not necessarily have upper bounds.&lt;/p&gt;
&lt;p&gt;We will define preorders $\le_1$ on the set $\mathbb{N}\times\{1\}$ and $\le_2$ on the set $\mathbb{N}\times\{2\}$ that act similarly to the standard &amp;quot;less than or equal to&amp;quot; relation on $\mathbb{N}$. Recall that we previously defined $\le$ on $\mathbb{N}$ so that $n\le m$ if and only if $m=n+k$ for some $k\in\mathbb{N}$.&lt;/p&gt;
&lt;p&gt;Notice that every element of $\mathbb{N}\times\{1\}$ is of the form $(n,1)$ for some $n\in\mathbb{N}$. Thus it makes sense to define $\le_1$ using the rule that $(n,1)\le_1 (m,1)$ if and only if $n\le m$. Similarly, we define $\le_2$ using the rule that $(n,2)\le_2 (m,2)$ if and only if $n\le m$.&lt;/p&gt;
&lt;p&gt;It is obvious that both $\le_1$ and $\le_2$ are preorders on their respective sets because they both inherit their reflexivity and transitivity from $\le$.&lt;/p&gt;
&lt;p&gt;Let's use these to define a preorder on $(\mathbb{N}\times\{1\})\cup(\mathbb{N}\times\{2\})$. We can define $\le_3$ on this union using the rule that $n\le_3 m$ if and only if either $n\le_1 m$ or $n\le_2 m$. Using the rigorous set-theoretic definition of relations, we could alternatively define this by $\le_3=\le_1\cup\le_2$. Again, it's easy to see that $\le_3$ is a preorder because it inherits its reflexivity and transitivity from $\le_1$ and $\le_2$.&lt;/p&gt;
&lt;p&gt;Basically what we have is two disjoint copies of things that act identically to $\mathbb{N}$, which have been glued together, but are related to each other in absolutely no way. In particular, if we choose $n_1\in\mathbb{N}\times\{1\}$ and $n_2\in\mathbb{N}\times\{2\}$, there is certainly no element of $(\mathbb{N}\times\{1\})\cup (\mathbb{N}\times\{2\})$ which serves as an upper bound for both $n_1$ and $n_2$. Thus, this example does not constitute a directed set.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; On the other hand, the set $\mathbb{N}$ of natural numbers equipped with $\le$, the standard &amp;quot;less than or equal to&amp;quot; relation, is a directed set. I proved in my post on quotient sets that this relation is reflexive and transitive, so it is certainly a preorder. The fact that all pairs of natural numbers have an upper bound is easy to show. For any $x,y\in\mathbb{N}$, choose $x=\max\{x,y\}$. Then clearly $x\le z$ and $y\le z$. This is a particularly easy example because every natural number is either less than or greater than every other natural number.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Another interesting directed set can be formed as follows. Let $X$ denote any nonempty topological space and pick a point $x\in X$. The set $N_x$ of all neighborhoods of $x$ forms a directed set when equipped with the preorder $\le$ defined by $U\le V$ is and only if $V\subseteq U$.&lt;/p&gt;
&lt;p&gt;This relation is reflexive because for any neighborhood $U$ of $x$, it is clear that $U\subseteq U$ and so $U\le U$.&lt;/p&gt;
&lt;p&gt;It is only a tad more difficult to see that $\le$ is transitive. Suppose we have neighborhoods $U, V$ and $W$ of $x$ for which $U\le V$ and $V\le W$. Then $W\subseteq V\subseteq U$, so certainly $W\subseteq U$. Thus, $U\le Q$.&lt;/p&gt;
&lt;p&gt;Lastly, we need to show that any pair of neighborhoods of $x$ has an upper bound, which in this case simply means they both contain a common neighborhood of $x$. Again, this is easy to show. Choose any two neighborhoods $U$ and $V$ of $x$. Clearly $x\in U\cap V$, and by the definition of a topology $U\cap V$ is open. Thus it is a neighborhood of $x$. It is obvious that $U\cap V\subseteq U$ and $U\cap V\subseteq V$, so $U\le U\cap V$ and $V\le U\cap V$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that we have some examples of directed sets in our arsenal, it's finally time to define nets. You've likely already guessed how we'll proceed.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;net&lt;/strong&gt; in a topological space $X$ is a function $x:A\to X$, where $A$ is any directed set.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Again, we generally write $x_a$ rather than $x(a)$, and we denote a net itself by $(x_a)_{x\in A}$. Since we've already established that $\mathbb{N}$ is a directed set, it should be clear that sequences are a special type of net.&lt;/p&gt;
&lt;p&gt;Convergence of nets is extremely similar to convergence of sequences.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A net $(x_a)_{x\in A}$ in a topological space $X$ &lt;strong&gt;converges&lt;/strong&gt; to a point $x\in X$ if for every neighborhood $U$ of $x$, there exists $b\in A$ for which $x_a\in U$ whenever $a\ge b$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; If a net $(x_a)_{x\in A}$ in a topological space converges to a point $x$, we say that $x$ is a &lt;strong&gt;limit&lt;/strong&gt; of that net and we write $\lim x_a=x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It's fairly easy to come up with a convergent net that is not a sequence, using an example I've already given.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Given a topological space $X$ and a point $x\in X$, let $N_x$ denote the directed set of neighborhoods of $x$ as detailed above. We can construct a net $(x_U)_{U\in N_x}$ by choosing a point $x_U\in U$ for each neighborhood $U$ of $x$. (Notice that this action requires the Axiom of Choice). Intuition tells us that this net should converge to $x$ because the neighborhoods of $x$ get &amp;quot;smaller&amp;quot; the further out we go in our directed set $N_x$. This claim is super easy to verify, so let's just do it.&lt;/p&gt;
&lt;p&gt;Choose any neighborhood $U$ of $x$. From our construction of the net $(x_U)_{U\in N_x}$, it is clear that $x_U\in U$. Furthermore, for any neighborhood $V$ of $x$ with $V\ge U$, we have that $V\subseteq U$ and thus $x_V\in X\subseteq U$. It follows that $(x_U)_{U\in N_x}$ converges to $x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This post is already so ridiculously long that I'm just going to prove the theorem that I promised you and then be done. Unfortunately, the proof is a little bit on the longer side.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces. Then a function $f:X\to Y$ is continuous if and only if for every net $(x_a)_{a\in A}$ that converges to $x$, the net $\big(f(x_a)\big)_{a\in A}$ converges to $f(x)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; The forward direction is practically identical for the analogous result for series. Suppose $f$ is continuous and that the net $(x_a)_{a\in A}$ converges to the point $x\in X$. Choose any neighborhood $U$ of $f(x)$. Since $f$ is continuous, $f^{-1}[U]\subseteq X$ is open and clearly $x\in f^{-1}[U]$, so $f^{-1}[U]$ is a neighborhood of $x$. Thus, there exists $b\in A$ for which $x_a\in f^{-1}[U]$ whenever $a\ge b$. It follows that $f(x_a)\in U$ whenever $a\ge b$, so the net $\big(f(x_a)\big)_{a\in A}$ converges to $f(x)$.&lt;/p&gt;
&lt;p&gt;I will prove the reverse direction by contradiction. Suppose that for every net $(p_a)_{a\in A}$ that converges to $p$, the net $\big(f(p_a)\big)_{a\in A}$ converges to $f(p)$, but that $f$ is not continuous. Then there exists a point $x\in X$ and a neighborhood $V$ of $f(x)$ for which $f^{-1}[V]$ is not a neighborhood of $x$. Thus, we can construct a net $(x_U)_{U\in N_x}$ for which each $x_U\notin f^{-1}[V]$. Clearly each $f(x_U)\notin V$. Choose any neighborhood $W$ of $x$. Then for any neighborhood $T\ge W$, i.e., $T\subseteq W$, and so $x_T\in W$. It follows that $(x_U)_{U\in N_x}$ converges to $x$, and thus $\big(f(x_U)\big)_{U\in N_x}$ converges to $f(x)$. However, the interior of $V$ is a neighborhood of $f(x)$ and thus $f(x_U)$ is eventually in this interior and therefore also in $V$, but this is a contradiction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So continuity is equivalent to the preservation of convergent nets, which is pretty cool. It's also true that being Hausdorff is equivalent to the existence of unique limits for nets, but I'm going to end this post here because it's really just getting ridiculous at this point.&lt;/p&gt;
&lt;hr class="footnotes-sep"&gt;
&lt;section class="footnotes"&gt;
&lt;ol class="footnotes-list"&gt;
&lt;li id="fn1" class="footnote-item"&gt;&lt;p&gt;Recall that in the trivial topology the only open sets are $\varnothing$ and $X$. &lt;a href="#fnref1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn2" class="footnote-item"&gt;&lt;p&gt;Or &lt;strong&gt;separated&lt;/strong&gt;, or $\mathbf{T}_2$. &lt;a href="#fnref2" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content:encoded></item><item><title>Quotient Spaces</title><description>The notion of a quotient space will effectively allow us to glue pieces of topological spaces together. This corresponds to the collapsing of equivalent subsets to points which occurs in quotient sets, as I mentioned in my last post.</description><link>http://localhost:2368/quotient-spaces/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae221</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Sun, 09 Apr 2017 14:16:40 GMT</pubDate><content:encoded>&lt;p&gt;Now that we've defined quotient sets, let's talk about quotient sets of topological spaces. The notion of a quotient space will effectively allow us to glue pieces of topological spaces together. This corresponds to the collapsing of equivalent subsets to points which occurs in quotient sets, as I mentioned in my last post. These are useful tools, so I'm going to jump right into it. It may be a bit difficult to see where I'm going with this at first, but bear with me and hopefully it'll become clear.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $X$ denote a topological space, let $A$ be a set and let $f:X\to A$ be a surjective function. The &lt;strong&gt;quotient topology&lt;/strong&gt; induced by $f$ has as its open sets all sets $U$ such that $f^{-1}[U]$ is open in $X$. We call $f$ a &lt;strong&gt;quotient map&lt;/strong&gt; and $A$ a &lt;strong&gt;quotient space&lt;/strong&gt; when equipped with this topology.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Supposing the quotient topology is truly a topology, we get for free that quotient maps are always continuous, simple from the way they're defined.  However, we still need to verify that quotient topologies satisfy the requirements of a topology. We need to show that $\varnothing$ and $A$ are open, and that unions and finite intersections of open sets are open.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ denote a topological space, let $A$ be a set and let $f:X\to A$ be a surjective function. Then the quotient topology defined above is a topology on $A$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; It is clear that $f^{-1}[\varnothing]=\varnothing$ and so the empty set is open in $A$. Furthermore, since $f$ is surjective, we know that $f^{-1}[A]=X$ is open in $X$ and so $A$ is also open in $A$.&lt;/p&gt;
&lt;p&gt;Next, suppose that $I$ is an indexing set and $U_i\subseteq A$ are such that $f^{-1}[U_i]$ is open in $X$ for each $i\in I$. Then certainly&lt;/p&gt;
&lt;p&gt;$$f^{-1}\left[\bigcup\limits_{i\in I}U_i\right]=\bigcup\limits_{i\in I}f^{-1}[U_i]$$&lt;/p&gt;
&lt;p&gt;is open in $X$ since it is the union of open sets. Thus, $\bigcup\limits_{i\in I}U_i$ is open in A.&lt;/p&gt;
&lt;p&gt;Finally, suppose that $n\in\mathbb{Z}^+$ and that $U_i\subseteq A$ are such that $f^{-1}[U_i]$ is open in $X$ for each $1\le i\le n$. Then&lt;/p&gt;
&lt;p&gt;$$f^{-1}\left[\bigcap\limits_{i=1}^n U_i\right]=\bigcap\limits_{i=1}^n f^{-1}[U_i]$$&lt;/p&gt;
&lt;p&gt;is open in $X$ since it is the intersection of a finite number of open sets. Thus, $\bigcap\limits_{i=1}^n U_i$ is open in $A$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is also an equivalent, more intuitive way to define quotient spaces which will probably look more familiar to you after our discussion on quotient sets last post.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $X$ denote a topological space and let $\sim$ be an equivalence relation on $X$. The &lt;strong&gt;quotient space&lt;/strong&gt; $X/\negthickspace\sim$ is this quotient set where the open sets are sets of equivalence classes whose unions are open in $X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We've established that quotient maps induce a topology in this way, so let's take a look at what they can do for us.&lt;/p&gt;
&lt;p&gt;While it's true that any set $A$ can be turned into a quotient space by defining a suitable surjection as our quotient map, we generally restrict our interest to partitions of $A$. Let's first visualize the construction of a simple quotient space, which you may have seen before. We will then figure out how to formally document this process.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the square $[0,1]^2$ in the standard topology. We can first create a cylinder from the square by gluing two opposite edges together.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/square_to_cylinder.svg" alt="square to cylinder"&gt;&lt;/p&gt;
&lt;p&gt;Next, we can glue the two open circular ends of the cylinder together to form a torus.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/cylinder_to_torus.svg" alt="cylinder to torus"&gt;&lt;/p&gt;
&lt;p&gt;What we've really done here is glue the opposite edges of the square together, one pair at a time. A much more concise diagram representing this act simply identifies opposite edges of the square with each other, and the gluing is implied.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/square_to_torus.svg" alt="square to torus"&gt;&lt;/p&gt;
&lt;p&gt;In such diagrams, it is understood that the two edges with one arrow get glued to each other and the two edges with two arrows get glued to each other. In this case everything is symmetrical so it doesn't matter which pair gets glued first, but there are probably cases in which it does matter. It is conventional to first glue together sides with one arrow, then two arrows and so on.&lt;/p&gt;
&lt;p&gt;It is this last diagram which provides us with something we can really use. This &lt;em&gt;identification&lt;/em&gt; of sides is crucial to defining the torus as a quotient space of the square. What we are really doing here is partitioning the square in such a way that certain pairs of points on the boundary get grouped into the same equivalence class. More precisely, we partition the square into many sets, namely every point in the interior of the square and each pair of opposite points on the square's boundary:&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
B_{x,y} &amp;amp;= \{(x,y)\}\text{ for } (x,y)\in (0,1)^2, \\&lt;br&gt;
C_x     &amp;amp;= \{(x,0),(x,1)\}\text{ for } x\in (0,1), \\&lt;br&gt;
D_y     &amp;amp;= \{(0,y),(1,y)\}\text{ for } y\in (0,1), \\&lt;br&gt;
E        &amp;amp;= \{(0,0),(0,1),(1,0),(1,1)\}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Allow me to explain these choices a little bit. We have defined one set $B_{x,y}$ in the partition for every point $(x,y)$ in the interior of the square. This is because in our quotient space, we do not want the interior of the square to collapse at all, and so every point should be in its own equivalence class. There is one set $C_x$ for every pair of points along the bottom and top of the square, and one set $C_y$ for every pair along the left and right edges. Putting these pairs into the same equivalence classes ensures that they will become one thing, i.e. 'glued together,' in the quotient space. Lastly, $E$ includes the corners of the square separately to avoid double-counting them.&lt;sup class="footnote-ref"&gt;&lt;a href="#fn1" id="fnref1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; This also ensures explicitly that the four corners of the square will all end up being glued together.&lt;/p&gt;
&lt;p&gt;Going back to our original definitions for quotient maps and spaces, we define the set $A$ as the collection of all sets in this partition and the quotient map $f$ as the surjective function mapping each point in the square to the set in $A$ which contains it. Then it is easy to show that the resulting quotient space is (homeomorphic to) the torus $S^1\times S^1$, so we have accomplished what we set out to.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Many, many more examples of quotient spaces can be generated easily in much the same manner. We can define the sphere as a quotient space of pretty much any polygon, for instance. However, it is tedious and difficult to draw diagrams and they can be found all over the internet anyway. This post is also nice and short for a change, so I'm going to stop here. The good news is that I've now introduced most of the common methods for constructing new topological spaces!&lt;/p&gt;
&lt;hr class="footnotes-sep"&gt;
&lt;section class="footnotes"&gt;
&lt;ol class="footnotes-list"&gt;
&lt;li id="fn1" class="footnote-item"&gt;&lt;p&gt;Remember that the sets in a partition must be pairwise disjoint. &lt;a href="#fnref1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content:encoded></item><item><title>Equivalence Relations and Quotient Sets</title><description>Quotient sets of $A$ are comprised not of elements of $A$, but of the equivalence classes they fall into. This gives us a powerful method to collapse a set into a smaller set that is in some way still representative of the original set.</description><link>http://localhost:2368/equivalence-relations-and-quotient-sets/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae220</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Sun, 09 Apr 2017 00:41:51 GMT</pubDate><content:encoded>&lt;p&gt;The next topological construction I'm going to talk about is the quotient space, for which we will certainly need the notion of quotient sets. However, equivalence relations and quotient sets show up all over the place in mathematics and are worth studying on their own because of their tremendous importance and ubiquitousness.&lt;/p&gt;
&lt;p&gt;The first concept I should introduce is that of a relation. A special type of relation, called an equivalence relation, will be vital to all of the content in this post.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;relation&lt;/strong&gt; on a set $A$ is a subset of the Cartesian product $A\times A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So technically any subset of $A\times A$ is a relation on $A$, but most of them are usually boring and have little meaning. Relations are generally used to compare two elements in some way. That is, we use them to determine whether two elements are 'related' in the manner specified. We should take a look at some characteristics that relations may possess which make them more interesting, but first I'd like to give some examples of familiar relations.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Take $A=\mathbb{N}$, the set of natural numbers. The &amp;quot;less than or equal to&amp;quot; relation on $\mathbb{N}$, usually written $\le$, is defined so that $(a,b)\in\le$ if and only if $b=a+x$ for some $x\in\mathbb{N}$. Clearly&lt;/p&gt;
&lt;p&gt;$$\le \; := \{(a,b)\in\mathbb{N}^2\mid b=a+x\text{ for some }x\in\mathbb{N}\}$$&lt;/p&gt;
&lt;p&gt;fits our definition of a relation because it is a subset of $\mathbb{N}\times\mathbb{N}$. However, no one ever writes things this way. Normal people use infix notation. That is, they write $a\le b$ rather than $(a,b)\in\le$. I will pretty much use infix notation for the rest of time since it tends to simplify things a great deal, and it is probably what you're used to seeing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Take $A=\mathbb{Z}$, the set of integers, and choose some $n\in\mathbb{Z}$. We can define a relation $=_n$ on $\mathbb{Z}$ so that for any $a,b\in\mathbb{Z}$, we have that $a=_n b$ if and only if $b-a=kn$ for some $k\in\mathbb{Z}$. We can write this more formally as&lt;/p&gt;
&lt;p&gt;$$=_n \; := \{(a,b)\in\mathbb{Z}^2\mid b-a=kn\text{ for some }k\in\mathbb{Z}\}$$&lt;/p&gt;
&lt;p&gt;but there isn't usually any benefit in doing things that way, and I think it's even a little bit confusing to look at. By the way, this relation is called &lt;strong&gt;congruence modulo $n$&lt;/strong&gt; and it is of tremendous importance to many fields of mathematics. You can bet we'll be seeing this again at some point soon.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; For any set $A$, both $\varnothing$ and $A\times A$ are relations on $A$. The $\varnothing$ relation doesn't relate elements of $A$ to anything, not even themselves. On the other hand, the relation $A\times A$ relates every element to every element of $A$. These are two extreme sorts of relations, but neither is particularly interesting or important.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that I've given you a few examples, hopefully the definition of a relation has had time to sink in and begun to make a bit of sense. As promised I'll now discuss some important qualities that a relation may or may not have.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A relation $\sim$ on a set $A$ is &lt;strong&gt;reflexive&lt;/strong&gt; if $a\sim a$ for every $a\in A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The relation $\le$ on $\mathbb{N}$ is reflexive because every natural number is less than or equal to itself, i.e. $n\le n$ for every $n\in\mathbb{N}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Another example of a reflexive relation is that of congruence modulo $n$. This is because $a-a=0n$ for every $a\in\mathbb{Z}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The reflexive property holds for many important relations, and is in general quite easy to verify. This is because in most relations of interest to mathematicians, elements tend to be related to themselves. In fact, the only relation we discussed above that is not reflexive is the empty relation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A relation $\sim$ on a set $A$ is &lt;strong&gt;symmetric&lt;/strong&gt; if $b\sim a$ whenever $a\sim b$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The relation $=_n$ on $\mathbb{Z}$ of congruence modulo some $n\in\mathbb{Z}$ is symmetric. To see this, suppose $a=_n b$ for some $a,b\in\mathbb{Z}$. Then by definition, $b-a=kn$ for some integer $k$. Negating each side, we see that $a-b=-kn$ and since $-k$ is also an integer it follows that $b=_n a$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The relation $\le$ on $\mathbb{N}$, on the other hand, is not symmetric. We can see this by examining a single counterexample. Clearly $3\le 5$ but it is not true that $5\le 3$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The final property I'm going to talk about is generally the most difficult to demonstrate holds for a particular relation. It sort of resembles the triangle inequality.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A relation $\sim$ on a set $A$ is &lt;strong&gt;transitive&lt;/strong&gt; if $a\sim c$ whenever both $a\sim b$ and $b\sim c$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; It's not too hard to show that the relation $\le$ on $\mathbb{N}$ is transitive. Consider $a,b,c\in\mathbb{N}$ such that $a\le b$ and $b\le c$. Then by definition, $b=a+x$ and $c=b+y$ for some $x,y\in\mathbb{N}$. Substituting the first equation into the second, we see that $c=a+x+y$. And since $x+y\in\mathbb{N}$, it follows that $a\le c$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Next, consider again the relation $=_n$ on $\mathbb{Z}$ of congruence modulo some integer $n$. Suppose $a=_n b$ and $b=_n c$. Then $b-a=k_1 n$ and $c-b=k_2 n$ for some integers $k_1$ and $k_2$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
c-a &amp;amp;= (c-b) + (b-a) \\&lt;br&gt;
&amp;amp;= k_2 n - k_1 n \\&lt;br&gt;
&amp;amp;= (k_2-k_1)n.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Since $k_2-k_1\in\mathbb{Z}$, it follows that $a=_n c$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that we have established these three types of relation, it is a piece of cake to define an equivalence relation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; An &lt;strong&gt;equivalence relation&lt;/strong&gt; is a relation which is reflexive, symmetric and transitive.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; We've established above that congruence modulo $n$ satisfies each of these properties, which automatically makes it an equivalence relation on the integers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The relation &amp;quot;is the same age as&amp;quot; on the set of all people is an equivalence relation. Every person is the same age as him/herself. If person $A$ is the same age as person $B$, then certainly person $B$ is the same age as person $A$. And transitivity also holds, but I'm too lazy to type that one out right now.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The relation &amp;quot;has shaken hands with&amp;quot; on the set of all people is &lt;em&gt;not&lt;/em&gt; an equivalence relation because it is not transitive. For instance, it is entirely possible that Bob has shaken Fred's hand and Fred has shaken hands with the president, yet this does not necessarily mean that Bob has shaken the president's hand.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As you will learn, equivalence relations pop up constantly in every area of mathematics. This is because they give sets a very nice kind of structure. To explain this further, we first need the following concepts:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given an equivalence relation $\sim$ on a set $A$ and an element $a\in A$, the &lt;strong&gt;equivalence class&lt;/strong&gt; of $A$ is the set $[a]=\{x\in A\mid a\sim x\}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given an equivalence relation $\sim$ on a set $A$, the set of equivalence classes corresponding to $\sim$ is called a &lt;strong&gt;quotient set&lt;/strong&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="#fn1" id="fnref1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; and is written $A/\negthickspace\sim$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So quotient sets of $A$ are comprised not of elements of $A$, but of the equivalence classes they fall into. This gives us a powerful method to collapse a set into a smaller set that is in some way still representative of the original set. Hopefully the following example will help make some sense of this.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Let's take another look at the set $\mathbb{Z}$ and the relation $=_3$ of congruence modulo $3$.&lt;sup class="footnote-ref"&gt;&lt;a href="#fn2" id="fnref2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; Under this relation, two integers $a$ and $b$ are related if $b-a=3k$ for some integer $k$. Put more plainly, two integers are congruent if their difference is a multiple of $3$.&lt;/p&gt;
&lt;p&gt;How many equivalence classes does this relation create? It isn't too tough to see that there are three: $[0], [1]$ and $[2]$. This is because all multiples of three, i.e. elements of the form $3k$ for some integer $k$, are congruent. Similarly, all elements of the form $3k+1$ are congruent and all elements of the form $3k+2$ are congruent. And these are all the possible options, really. I have chosen $0, 1$ and $2$ as the &lt;strong&gt;representatives&lt;/strong&gt; of these equivalence classes, but this choice was arbitrary (albeit standard). I could just as easily have named them $[3000], [16]$ and $[-1]$.&lt;/p&gt;
&lt;p&gt;Lastly, we see that the quotient set $\mathbb{Z}/\negthickspace=_3$ is just the set $\big\{[0],[1],[2]\big\}$. That is, all congruent elements are essentially collapsed to a single point in the quotient set.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is one final topic I need to talk about here, which is the fact that equivalence classes form partitions of sets. This is called the Fundamental Theorem of Equivalence Relations, but I'm getting ahead of myself. Before I prove it, I need to tell you what a partition is.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;partition&lt;/strong&gt; $\cal P$ of a set $A$ is a collection of subsets of $A$ which satisfies the following properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The empty set is not in $\cal P$.&lt;/li&gt;
&lt;li&gt;For any two sets $X,Y$ in $\cal P$, either $X$ and $Y$ are disjoint or $X=Y$.&lt;/li&gt;
&lt;li&gt;The union $\bigcup\limits_{X\in\cal P}X$ of all subsets in the partition is equal to $A$ itself.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is all basically a fancy way of saying that a partition is a method of breaking a set into non-overlapping subsets. Now it's time to prove that Fundamental Theorem thingy I mentioned a moment ago.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Fundamental Theorem of Equivalence Relations.&lt;/strong&gt; Let $\sim$ be an equivalence relation on a set $A$. Then the quotient set $A/\negthickspace\sim$ is a partition of $A$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; All we need to do is show that the three properties of partitions hold. Clearly the empty set $\varnothing$ is not in the quotient set $A/\negthickspace\sim$ because the reflexive property of equivalence relations tells us that $x\sim x$, and thus $x\in [x]$ for every $x\in A$.&lt;/p&gt;
&lt;p&gt;Next, suppose that $[x]$ and $[y]$ are equivalences classes in $A/\negthickspace\sim$ and that they are disjoint, i.e. $[x]\cap [y]\ne\varnothing$. Then there exists some $a\in A$ such that $a\in [x]\cap [y]$. That is, $a\in [x]$ and $a\in [y]$, so $a\sim x$ and $a\sim y$. By the symmetric property of equivalence relations, $x\sim a$. And by the transitive property, $x\sim y$. Thus, it follows that $[x]=[y]$.&lt;/p&gt;
&lt;p&gt;Finally, it is clear from the union lemma that the union of all equivalence classes, $\bigcup\limits_{[x]\in A/\sim}[x]$, is the entire set $A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Notice that we used all three properties of equivalence relations (reflexivity, symmetry and transitivity) to prove this result. This indicates that equivalence relations are the only relations which partition sets in this manner. It turns out that this is true, and it's very easy to prove. I won't do that here because this post is already longer than I intended, but I will at least state the theorem.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $\cal P$ be a partition of a set $A$ and define a relation $\sim$ by $x\sim y$ if and only if $x,y\in X$ for some $X\in\cal P$. Then $\sim$ is an equivalence relation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What this theorem really tells us is that every partition is the quotient set of some equivalence relation, and that's a really cool idea.&lt;/p&gt;
&lt;hr class="footnotes-sep"&gt;
&lt;section class="footnotes"&gt;
&lt;ol class="footnotes-list"&gt;
&lt;li id="fn1" class="footnote-item"&gt;&lt;p&gt;More formally, it is called the &lt;strong&gt;quotient set of $A$ modulo $\sim$.&lt;/strong&gt; &lt;a href="#fnref1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn2" class="footnote-item"&gt;&lt;p&gt;It's easy to extend this example to integers other than $3$, but I feel like a more concrete example is useful here. &lt;a href="#fnref2" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content:encoded></item><item><title>Product Spaces</title><description>Next let's talk about an intuitive way to combine topological spaces to create new spaces which inherit certain characteristics from their parents. We've talked about Cartesian products before in the context of set theory, but what happens if we take the Cartesian product of topological spaces?</description><link>http://localhost:2368/product-spaces/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae21f</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Sat, 08 Apr 2017 20:54:38 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#definition"&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples"&gt;Examples&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="definitionanamedefinition"&gt;Definition&lt;a name="definition"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Next let's talk about an intuitive way to combine topological spaces to create new spaces which inherit certain characteristics from their parents. We've talked about Cartesian products before in the context of set theory, but what happens if we take the Cartesian product of topological spaces? What should the topology look like?&lt;/p&gt;
&lt;p&gt;Obviously there are many options (to name a few, the discrete or trivial topologies can be defined on any set), but we would like to choose a topology that is as natural as possible and inherits its properties from the spaces from which it is built. This decision actually has its roots in category theory, but I hope that the choice will make some sense to you nonetheless.&lt;/p&gt;
&lt;p&gt;Let's say we are given topological spaces $X$ and $Y$ and we want to construct a &amp;quot;natural&amp;quot; topology on $X\times Y$. Our first instinct might be to choose as the open sets all products $U\times V$ where $U$ is open in $X$ and $V$ is open in $Y$. But even in $\mathbb{R}\times\mathbb{R}$ we can see that this doesn't result in a topology, since the union of products of open sets isn't necessarily itself a product of open sets, as illustrated below.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/nonono.svg" alt="no no no"&gt;&lt;/p&gt;
&lt;p&gt;So clearly the products of open sets aren't going to form a topology by themselves, since they are not closed under unions. We don't throw out this idea entirely, though. It just so happens that the products of open sets do form a &lt;em&gt;basis&lt;/em&gt; for a topology on $X\times Y$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $(X,{\cal T}_X)$ and $(Y,{\cal T}_Y)$ denote topological spaces. Then ${\cal B} = \{U\times V\mid U\in {\cal T}_X, V\in {\cal T}_V\}$ is a basis for a topology on $X\times Y$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We argue first that for every $(x,y)\in X\times Y$ there exists a basis element $B\in {\cal B}$ with $(x,y)\in B$. Take $B=X\times Y$. Since $x\in X$ and $y\in Y$, clearly $(x,y)\in B$.&lt;/p&gt;
&lt;p&gt;Next, suppose we are given basis elements $B_1, B_2\in{\cal B}$ for which $B_1\cap B_2\ne\varnothing$. Then by definition $B_1=U_1\times V_1$ and $B_2=U_2\times V_2$ for some open sets $U_1, U_2\in{\cal T}_X$ and $V_1, V_2\in{\cal T}_Y$. Note that $U=U_1\cap U_2\in{\cal T}_X$ and $V=V_1\cap V_2\in{\cal T}_Y$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
B_1\cap B_2 &amp;amp;= (U_1\times V_1)\cap (U_2\times V_2) \\&lt;br&gt;
&amp;amp;= (U_1\cap U_2)\times (V_1\cap V_2) \\&lt;br&gt;
&amp;amp;= U\times V \\&lt;br&gt;
&amp;amp;\in {\cal B}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;It follows that $B_1\cap B_2$ is a basis element contained in itself, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that we have a natural basis for a topology on the product of two spaces, defining the product topology is a piece of cake.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let Let $(X,{\cal T}_X)$ and $(Y,{\cal T}_Y)$ denote topological spaces. The &lt;strong&gt;product topology&lt;/strong&gt; on $X\times Y$ is the topology generated by the basis ${\cal B} = \{U\times V\mid U\in {\cal T}_X, V\in {\cal T}_V\}$. We call $X\times Y$ a &lt;strong&gt;product space&lt;/strong&gt; when equipped with this topology.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Just to refresh your memory, the open sets in the topology generated by a basis are the empty set and all unions of basis elements. This also guarantees that the entire space is open as a result of the union lemma, as we saw several posts ago.&lt;/p&gt;
&lt;p&gt;The product topology can easily be extended in the obvious way to the Cartesian product of a finite numbers of sets. This basis even generates a topology for an infinite number of sets, but in that case it is actually not the topology we generally use. For an infinite number of sets, the product topology has a few extra restrictions. The basis we just gave extends to what is called the &lt;strong&gt;box topology&lt;/strong&gt; for an infinite product, and it has some undesirable properties. However, I'm fairly confident that I will never need to talk about infinite products on this blog, so I'm going to leave the discussion at that for now.&lt;/p&gt;
&lt;h3 id="examplesanameexamples"&gt;Examples&lt;a name="examples"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; For our first example, consider $\mathbb{R}$ with the standard topology. What is the product topology on $\mathbb{R}\times\mathbb{R}=\mathbb{R}^2$? Well we know that a basis for this topology is all products of open intervals. If $(a,b)$ and $(c,d)$ are open intervals in $\mathbb{R}$ then $(a,b)\times(c,d)$ can be viewed as an open rectangle in $\mathbb{R}^2$. But open rectangles, just like open balls, generate the standard topology on $\mathbb{R}^2$. So the product topology on $\mathbb{R}^2$ is actually the standard topology, and the same holds for any finite product of $\mathbb{R}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; For our next example, consider the closed interval $[0,1]$ as a subspace of $\mathbb{R}$. The product $[0,1]\times[0,1]$ is just the unit square in $\mathbb{R}^2$. Open sets in the square are unions of products of open sets in $[0,1]$. That is, they are unions of open rectangles.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the unit circle $S^1=\{(x,y)\in\mathbb{R}^2\mid x^2+y^2=1\}$ as a subspace of $\mathbb{R}^2$. Let's begin by visualizing $S^1\times S^1$. Recalling the definition of the Cartesian product, we can think loosely of each point on the first circle $S^1$ as corresponding to an entire circle. We thus obtain the &lt;strong&gt;torus&lt;/strong&gt;, which is a donut-shaped subset of $\mathbb{R}^3$.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/torus-1.svg" alt="torus"&gt;&lt;/p&gt;
&lt;p&gt;Notice that the torus is hollow. If we wanted a solid torus, we would take $S^1\times D^2$, where $D^2=\{(x,y)\in\mathbb{R}^2\mid x^2+y^2\le 1\}$ is the closed unit ball.&lt;/p&gt;
&lt;p&gt;Before we can think about the topology on the torus $S^1\times S^1$, we should first consider the topology on the circle $S^1$. Since it's a subspace of $\mathbb{R}^2$, open sets in the circle are intersections of $S^1$ with open sets in $\mathbb{R}^2$. These open sets basically look like unions of &amp;quot;open intervals&amp;quot; wrapped around the circle. In fact, they are all homeomorphic to open intervals, except for $S^1$ itself (which I will prove when I talk about connectedness).&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/circle_open_sets.svg" alt="circle open sets"&gt;&lt;/p&gt;
&lt;p&gt;Products of these open sets somewhat resemble open rectangles wrapped around the surface of the torus. We'll call them open patches, and the unions of these open patches form the open sets on the torus.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/torus_open_sets.svg" alt="torus open sets"&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Since $S^1\times S^1$ can also be viewed as a subspace of $\mathbb{R}^3$, we could also view the open sets on the torus as intersections of open sets in $\mathbb{R}^3$ with $S^1\times S^1$. This statement may seen obvious, but I haven't proved it yet.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces with $A\subseteq X$ and $B\subseteq Y$. The topology on $A\times B$ as a subspace of $X\times Y$ is the same as the product topology where $A$ is a subspace of $X$ and $B$ is a subspace of $Y$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We argue that any open set in either topology is also open in the other. Choose an open set $U$ in the subspace topology on $A\times B$. By definition, there exists some open set $V$ in $X\times Y$ such that $U=(A\times B)\cap B$. Since it is open in the product topology on $X\times Y$, we have that $V$ must be a union of products of open sets. That is,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
V &amp;amp;= \bigcup\limits_{i\in I}(S_i\times T_i) \\&lt;br&gt;
&amp;amp;= \bigcup\limits_{i\in I}S_i\times\bigcup\limits_{i\in I}T_i,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $I$ is an indexing set such that $S_i$ is open in $X$ and $T_i$ is open in $Y$ for every $i\in I$. But this means that $U$ is open in the product topology on $A\times B$.&lt;/p&gt;
&lt;p&gt;The proof of the reverse direction is completely symmetrical.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So any open set on the torus can also be expressed as the intersection of open balls in $\mathbb{R}^3$ with $S^1\times S^1$. This may or may not be a simpler way of viewing the topology on the torus, depending on the application.&lt;/p&gt;
&lt;p&gt;I would like to conclude with the proof I promised you in my last post, which greatly simplified the task of showing that the $x$-axis as a subspace of $\mathbb{R}^2$ is homeomorphic to $\mathbb{R}$. This proof closely mimics the corresponding proof in my last post, although I have defined the homeomorphism in the opposite direction just to spice things up a bit. Notice first that the $x$-axis may be written as $\mathbb{R}\times\{0\}$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $A$ and $B$ denote topological spaces with $b\in B$ and consider $A\times\{b\}$ as a subspace of $A\times B$ with the product topology. Then $A$ is homeomorphic to $A\times\{b\}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We will argue that $f:A\to A\times\{b\}$ defined by $f(a)=(a,b)$ is a homeomorphism. Certainly $f$ is bijective and its inverse function $f^{-1}:A\times\{b\}\to A$ is given by $f^{-1}\big((a,b)\big)=a$.&lt;/p&gt;
&lt;p&gt;First we'll show that $f$ is continuous. Let $U$ denote an open set in $A\times\{b\}$. We can write $U$ as the intersection of $A\times\{b\}$ with some union of basis elements of $A\times B$, which are themselves products of open sets. That is, for some indexing set $I$,&lt;/p&gt;
&lt;p&gt;$$U=(A\times B)\cap\bigcup\limits_{i\in I}(A_i\times B_i),$$&lt;/p&gt;
&lt;p&gt;where $A_i\subseteq A$ and $B_i\subseteq B$ are open for every $i\in I$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f^{-1}[U] &amp;amp;= f^{-1}\left[(A\times B)\cap\bigcup_{i\in I}(A_i\times B_i)\right]\\&lt;br&gt;
&amp;amp;= f^{-1}[A\times B]\cap f^{-1}\left[\bigcup_{i\in I}(A_i\times B_i)\right]\\&lt;br&gt;
&amp;amp;= f^{-1}[A\times B]\cap \bigcup_{i\in I}f^{-1}[A_i\times B_i]\\&lt;br&gt;
&amp;amp;= A\cap\bigcup_{i\in I}A_i\\&lt;br&gt;
&amp;amp;= \bigcup_{i\in I}A_i,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;which is certainly open in $A$ since it is the union of open sets.&lt;/p&gt;
&lt;p&gt;It is easier to show that $f^{-1}$ is continuous. Let $V$ denote an open set in $A$. Note that $V\times B$ is a basis element for $A\times B$ and is thus open in $A\times B$. Therefore,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(f^{-1})^{-1}[V] &amp;amp;= f[V] \\&lt;br&gt;
&amp;amp;= V\times\{b\} \\&lt;br&gt;
&amp;amp;= (A\times\{b\})\cap(V\times B).&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;This is open in $A\times\{b\}$ because it is the intersection of $A\times\{b\}$ with an open set in $A\times B$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Using this result, we can immediately construct homeomorphisms&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\mathbb{R}&amp;amp;\to\mathbb{R}\times\{b\}\subset\mathbb{R}^2 \\&lt;br&gt;
\mathbb{R}^2&amp;amp;\to\mathbb{R}^2\times\{b\}\subset\mathbb{R}^3 \\&lt;br&gt;
\mathbb{R}^3&amp;amp;\to\mathbb{R}^3\times\{b\}\subset\mathbb{R}^4 \\&lt;br&gt;
&amp;amp;\;\; \vdots \\&lt;br&gt;
\mathbb{R}^n&amp;amp;\to\mathbb{R}^n\times\{b\}\subset\mathbb{R}^{n+1} \\&lt;br&gt;
&amp;amp;\;\; \vdots&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;between $\mathbb{R}^i$ and 'horizontal' hyperplanes in $\mathbb{R}^{i+1}$. It is also possible to show that arbitrary hyperplanes (formally $n$-dimensional subspaces in the linear algebraic sense) of $\mathbb{R}^{i+1}$ are homeomorphic to $\mathbb{R}^i$, but the proof would require a change of basis and that isn't something we have the machinery to get into right now.&lt;/p&gt;
&lt;p&gt;Anyway, that's all the time I have right now and I think I've done enough to introduce product spaces. May this knowledge aid you in your quest and be your savior in many battles.&lt;/p&gt;
</content:encoded></item><item><title>Subspaces</title><description>A topological space is, at its core, just a set with some additional structure. So what if we want to keep the structure, but change the underlying set? There's an easy and somewhat obvious way to do this.</description><link>http://localhost:2368/subspaces/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae21e</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Sat, 08 Apr 2017 16:47:05 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#definition"&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples"&gt;Examples&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="definitionanamedefinition"&gt;Definition&lt;a name="definition"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In this post I'm going to introduce a classic method which allows us to construct new topological spaces from existing ones in a natural way.&lt;/p&gt;
&lt;p&gt;A topological space is, at its core, just a set with some additional structure. So what if we want to keep the structure, but change the underlying set? There's an easy and somewhat obvious way to do this.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $X$ denote a topological space with topology $\cal T$ and suppose $A\subseteq X$. Then ${\cal T}_A=\{A\cap U\mid U\in{\cal T}\}$ is called the &lt;strong&gt;subspace topology&lt;/strong&gt; on $A$. Equipped with this topology, we call $A$ a &lt;strong&gt;subspace&lt;/strong&gt; of $X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In standard English, this says that in the subspace topology on $A$, a set is open if it is the intersection of $A$ with some open set in $X$.&lt;/p&gt;
&lt;p&gt;Now we have to prove that we are justified in calling this thing a topology. We need to show that the empty set and the subspace itself are open, that unions of opens sets are open, and that finite intersections of open sets are open.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ denote a topological space with topology $\cal T$ and suppose $A\subseteq X$. Then ${\cal T}_A=\{A\cap U\mid U\in{\cal T}\}$ is a topology on $A$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Notice first that $\varnothing, X\in\cal T$ by the definition a topology. Certainly $\varnothing=A\cap\varnothing\in{\cal T}_A$, so the empty set is open in the subspace. Similarly, $A=A\cap X\in{\cal T}_A$ because $A\subseteq X$, so $A$ itself is open in the subspace.&lt;/p&gt;
&lt;p&gt;Next, suppose that $I$ is an indexing set such that $V_i\in{\cal T}_A$ is open in the subspace for each $i\in I$. Then for every $i\in I$ we have that $V_i=A\cap U_i$ for some open set $U_i\in{\cal T}$, by the definition of the subspace topology. Since $\bigcup\limits_{i\in I}U_i\in\cal T$ it is clear that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\bigcup\limits_{i\in I}V_i &amp;amp;= \bigcup\limits_{i\in I}(A\cap U_i) \\&lt;br&gt;
&amp;amp;= A\cap \bigcup\limits_{i\in I}U_i \\&lt;br&gt;
&amp;amp;\in{\cal T}_A,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;so we have shown that the union of any collection of open sets in the subspace is also open in the subspace.&lt;/p&gt;
&lt;p&gt;Finally, suppose that $V_i\in{\cal T}_A$ is open in the subspace for $1\leq i\leq n$ for some $n\in\mathbb{Z}^+$. Then for $1\leq i\leq n$, we again have that $V_i=A\cap U_i$ for some open set $U_i\in{\cal T}$, by the definition of the subspace topology. Since $\bigcap\limits_{i=1}^n U_i\in\cal T$, it is clear that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\bigcap\limits_{i=1}^n V_i &amp;amp;= \bigcap\limits_{i=1}^n (A\cap U_i) \\&lt;br&gt;
&amp;amp;= A\cap \bigcap\limits_{i=1}^n U_i \\&lt;br&gt;
&amp;amp;\in {\cal T}_A,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;so we have shown that the intersection of a finite collection of open sets in the subspace is also open in the subspace. It follows that the subspace topology on $A$ is a topology, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I have noticed that people often find the following observation confusing: If $A$ is a subset of a topological space $X$, then the set $A$ is both open and closed in the subspace topology on $A$. This is automatically true because of the definition of a topology. However, this says nothing at all about whether $A$ is open or closed as a &lt;em&gt;subset&lt;/em&gt; of $X$. Similarly, there are plenty of sets that may be open or closed in the subspace topology that may not have been that way in the original topology. Make sure you pay attention to the distinction between subspaces and subsets!&lt;/p&gt;
&lt;p&gt;Next, there's a way to figure out what sets in a subspace are closed which is sometimes more direct than trying to make sure that their complements are open.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $A$ be a subspace of a topological space $X$. Then $U\subseteq A$ is closed in $A$ if and only if $U=A\cap V$ for some closed set $V\subseteq X$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; First, suppose that $U$ is closed in $A$. Then $A-U$ is open by definition, so $A-U=A\cap W$ for some open set $W$ in $X$. Certainly $V=X-W$ is closed in $X$, so&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U &amp;amp;= A-(A-U) \\&lt;br&gt;
&amp;amp;= A-(A\cap W) \\&lt;br&gt;
&amp;amp;= A-W \\&lt;br&gt;
&amp;amp;= A\cap (X-W) \\&lt;br&gt;
&amp;amp;= A\cap V.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Conversely, suppose that $U=A\cap V$ for some closed set $V$ in $X$. Then $X-V$ is open in $X$, so&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
A-U &amp;amp;= A-(A\cap V) \\&lt;br&gt;
&amp;amp;= A\cap (X-V)&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;is open in $A$. Thus, $U$ is closed in $A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I may not have explicitly proved all those set equalities in the past, but if they aren't immediately obvious to you then this might be a good time to go back and get some more practice with set-theoretic proofs.&lt;/p&gt;
&lt;h3 id="examplesanameexamples"&gt;Examples&lt;a name="examples"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Let's look at a few examples of subspaces of topological spaces we are already familiar with.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider $\mathbb{R}$ with the standard topology. Obviously the half-open interval $[0,1)$ is a subset of $\mathbb{R}$. So what is the subspace topology on $[0,1)$?&lt;/p&gt;
&lt;p&gt;Well, we already know it. The open sets are just intersections of $[0,1)$ with any open set in $\mathbb{R}$. Of course, the subspace $[0,1)$ is itself both open and closed.&lt;/p&gt;
&lt;p&gt;Is $(0,1)$ open in the subspace topology? Of course it is. Certainly $(0,1)$ is open in $\mathbb{R}$, and $(0,1)=[0,1)\cap (0,1)$, so it is open in the subspace as well.&lt;/p&gt;
&lt;p&gt;Play around with this a little bit more and you'll notice that the open sets in $[0,1)$ actually look a lot like the open sets in $\mathbb{R}$ which happen to be subsets of $[0,1)$. This is just a testament to the fact that the subspace topology is a very natural object. In fact, whenever I talk about an interval, I'll generally be assuming that it is equipped with the subspace topology induced by the standard topology on $\mathbb{R}$. Even though I won't always explicitly mention this, it will be especially important to remember when I talk about paths and homotopy in the future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's look at another example, shall we?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Consider $\mathbb{R}^2$ in the standard topology, and the set $A=\{(x,0)\in\mathbb{R}^2\mid x\in\mathbb{R}\}$.&lt;sup class="footnote-ref"&gt;&lt;a href="#fn1" id="fnref1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; The open sets in the subspace topology on $A$ are, of course, any sets which can be expressed as the intersection of $A$ with unions of open balls in $\mathbb{R}^2$.&lt;/p&gt;
&lt;p&gt;This looks an awful lot like the standard topology on $\mathbb{R}$, but technically it isn't because $A\ne\mathbb{R}$. In reality, $A$ is just the $x$-axis in the two-dimensional Euclidean plane. So it's basically $\mathbb{R}$, and behaves exactly like it. What we have, then, is that $A$ is homeomorphic to $\mathbb{R}$ with the standard topology.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's prove the assertion I just made. It's going to be a bit of work, but I think it's worth proving things just for fun once in a while. The homeomorphism we will choose in our proof is the obvious choice, but there are actually others that could work just as well.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; Consider $\mathbb{R}$ with the standard topology and $A=\{(x,0)\in\mathbb{R}^2\mid x\in\mathbb{R}\}$ as a subspace of $\mathbb{R}^2$ with the standard topology. The spaces $\mathbb{R}$ and $A$ are homeomorphic.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We argue that the function $f:A\to\mathbb{R}$ defined by $f\big((x,0)\big)=x$ is a homeomorphism. It is trivial to check that $f$ is bijective and that its inverse function $f^{-1}:\mathbb{R}\to A$ is given by $f^{-1}(x)=(x,0)$.&lt;/p&gt;
&lt;p&gt;We will show first that $f^{-1}$ is continuous. Choose an open set $U$ in $A$. By definition, we can write $U$ as the intersection of $A$ with some union of open balls in $\mathbb{R}^2$. That is,&lt;/p&gt;
&lt;p&gt;$$U=A\cap\bigcup\limits_{i\in I} B\big((x_i,y_i),r_i\big)$$&lt;/p&gt;
&lt;p&gt;for some indexing set $I$, where $(x_i,y_i)\in\mathbb{R}^2$ and $r_i\in\mathbb{R}^+$ for each $i\in I$. It follows that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(f^{-1})^{-1}[U] &amp;amp;= f[U] \\&lt;br&gt;
&amp;amp;= f\left[A\cap\bigcup\limits_{i\in I}B\big((x_i,y_i),r_i\big)\right] \\&lt;br&gt;
&amp;amp;= f\left[\bigcup\limits_{i\in I}A\cap B\big((x_i,y_i),r_i\big)\right] \\&lt;br&gt;
&amp;amp;= \bigcup\limits_{i\in I}f\Big[A\cap B\big((x_i,y_i),r_i\big)\Big].&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;A small amount of geometry and the Pythagorean Theorem gets us that for each $i\in I$, the set $f\Big[A\cap B\big((x_i,y_i),r_i\big)\Big]$ is equal to the open interval&lt;/p&gt;
&lt;p&gt;$$\left(x_i-\sqrt{r_i^2-y_i^2}, x_i+\sqrt{r_i^2-y_i^2}\right)$$&lt;/p&gt;
&lt;p&gt;if $r_i&amp;gt;y_i$, and it is empty if $r_i\leq y_i$. The union of open intervals is open in $\mathbb{R}$, so $(f^{-1})^{-1}[U]$ is open in $\mathbb{R}$ and thus $f^{-1}$ is continuous.&lt;/p&gt;
&lt;p&gt;Next, we will show that $f$ is continuous. Choose an open set $U$ in $\mathbb{R}$. By definition, $U$ is a union of open intervals in $\mathbb{R}$ (possibly $\mathbb{R}$ itself, in which case $f^{-1}[U]=f^{-1}[\mathbb{R}]=A$ is certainly open). More precisely, we can write&lt;/p&gt;
&lt;p&gt;$$U=\bigcup\limits_{i\in I}(x_i-r_i,x_i+r_i),$$&lt;/p&gt;
&lt;p&gt;where $I$ is some indexing set such that $x_i\in\mathbb{R}$ and $r_i\in\mathbb{R}^+$ for every $i\in I$. It follows that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f^{-1}[U] &amp;amp;= f^{-1}\left[\bigcup\limits_{i\in I}(x_i-r_i,x_i+r_i)\right] \\&lt;br&gt;
&amp;amp;= f^{-1}\left[\bigcup\limits_{i\in I}\left\{x\in\mathbb{R}\bigg\vert \vert x-x_i\vert&amp;lt; r_i\right\}\right] \\&lt;br&gt;
&amp;amp;= \bigcup\limits_{i\in I}f^{-1}\left[\left\{x\in\mathbb{R}\bigg\vert \vert x-x_i\vert&amp;lt; r_i\right\}\right] \\&lt;br&gt;
&amp;amp;= \bigcup\limits_{i\in I}\left\{(x,0)\in\mathbb{R}^2\bigg\vert\vert x-x_i\vert&amp;lt; r_i\right\} \\&lt;br&gt;
&amp;amp;= \bigcup\limits_{i\in I}\bigg(A\cap B\big((x_i,0),r_i\big)\bigg) \\&lt;br&gt;
&amp;amp;= A\cap \bigcup\limits_{i\in I}B\big((x_i,0),r_i\big).&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;The union of open balls is open, so $f^{-1}[U]$ is the intersection of $A$ with an open set in $\mathbb{R}$ and is thus open in $A$. It follows that $f$ is continuous, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It ended up being a bit long, but not too difficult. And I think this example really gets the point across that even though homeomorphic spaces may not be identical, they behave in essentially the same way. Most people aren't as careful as I just was, and frequently write that $\mathbb{R}$ is a subspace of $\mathbb{R}^2$. This isn't strictly true, but at least you now know how to interpret it.&lt;/p&gt;
&lt;p&gt;Just so you know, we will see another proof of the above proposition when I talk about product spaces. I just really wanted to prove it this way once to give you some geometric intuition behind why it's true.&lt;/p&gt;
&lt;p&gt;The last thing I'm going to do in this post is show that an open interval in $\mathbb{R}$ is homeomorphic to $\mathbb{R}$ itself. This will reinforce the notion that stretching a space does not alter its topological properties.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; Let $a,b\in\mathbb{R}$ with $a&amp;lt; b$. Then $\mathbb{R}$ and the interval $(a,b)$ are homeomorphic.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;quot;Proof.&amp;quot;&lt;/strong&gt; Define $f:(a,b)\to\mathbb{R}$ by&lt;/p&gt;
&lt;p&gt;$$f(x)=\frac{1}{x-a}+\frac{1}{x-b}.$$&lt;/p&gt;
&lt;p&gt;We will argue that $f$ is a homeomorphism. I am not going to check that $f$ is a bijection, although it is obvious just from looking at its graph. This could be done using the quadratic formula to find an explicit inverse function, being careful to restrict $f^{-1}$ so that it is surjective. It could also be done using calculus to show that this function is injective because it is monotonically decreasing and surjective by the intermediate value theorem because it approaches negative infinity at its left endpoint and positive infinity at its right endpoint.&lt;/p&gt;
&lt;p&gt;The function $f$ is clearly continuous because it is a rational function whose denominator is nonzero for every $x\in(a,b)$. Its inverse, $f^{-1}$ is also continuous for the same reason, but since I haven't explicitly computed it I will not show that this is the case.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Okay, so that really wasn't a proof at all. But again, a quick glance at the graph of $f$ should really be enough to convince you that it's a homeomorphism. I could be more precise in giving an argument that any two open intervals are homeomorphic, but I think this post has gone on long enough already, and hopefully this fact is obvious to you. On the other hand, closed intervals are &lt;em&gt;not&lt;/em&gt; homeomorphic to open intervals. We don't have enough tools to prove that at this point, but it will become trivial when I talk about compactness in a later post.&lt;/p&gt;
&lt;hr class="footnotes-sep"&gt;
&lt;section class="footnotes"&gt;
&lt;ol class="footnotes-list"&gt;
&lt;li id="fn1" class="footnote-item"&gt;&lt;p&gt;$(x,0)$ is not an open interval here, but a point in $\mathbb{R}^2$. That is, it's an ordered pair of real numbers. The fact that open intervals and ordered pairs have the same notation is unfortunate, but if we are careful it should always be clear which we are talking about. &lt;a href="#fnref1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content:encoded></item></channel></rss>