<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Algebrology]]></title><description><![CDATA[A gentle introduction to insanity.]]></description><link>https://algebrology.github.io/</link><image><url>https://algebrology.github.io/favicon.png</url><title>Algebrology</title><link>https://algebrology.github.io/</link></image><generator>Ghost 2.14</generator><lastBuildDate>Fri, 09 Aug 2019 19:33:20 GMT</lastBuildDate><atom:link href="https://algebrology.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Dual Spaces]]></title><description><![CDATA[Since I haven't posted for a while, I decided to break up my rants about homology with some posts on linear (and multilinear) algebra. In this post, we will (as usual) deal only with finite dimensional vector spaces.]]></description><link>https://algebrology.github.io/dual-spaces/</link><guid isPermaLink="false">5d4cec2a2864a0008db11a72</guid><category><![CDATA[linear algebra]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Fri, 09 Aug 2019 03:55:00 GMT</pubDate><content:encoded><![CDATA[<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#the-dual-space">The Dual Space</a></li>
<li><a href="#the-double-dual-space">The Double Dual Space</a></li>
</ol>
<hr>
<h3 id="introductionanameintroduction">Introduction<a name="introduction"></a></h3>
<p>Since I haven't posted for a while, I decided to break up my rants about homology with some posts on linear (and multilinear) algebra. In this post, we will (as usual) deal only with finite dimensional vector spaces. Since we care only about abstract properties of vector spaces and not about any specific vector space, I will talk generally about a vector space $V$ of dimension $n$ over a field $\F$ for the remainder of this post.</p>
<p>As we discovered previously, every finite dimensional vector space has a basis. That is, there exists a linearly independent collection $(e_1,e_2,\ldots,e_n)$ of vectors in $V$ for which any vector $v$ in $V$ can be expressed as a linear combination of these basis vectors. That is, for any $v\in V$ there exist scalars $(v^i)_{i=1}^n$ in $\F$ for which</p>
<p>$$v=\sum_{i=1}^n v^i e_i.$$</p>
<p>Note that in the expression above, I have moved the index on $v^i$ into the upper position, whereas in previous posts I would have written the same scalars as $v_i$. There is a good reason for this, and it is commonly seen in physics and differential geometry. The reason will become apparent shortly, but for now just realize that using superscripts for index placement is really no different than using subscripts, and it <strong>does not</strong> represent exponentiation. For instance, $v^2$ represents the second scalar in a list and <strong>not</strong> $v\cdot v$.</p>
<p>Having a basis for our vector space is nice for two main reasons:</p>
<ol>
<li>Any vector can be expressed in terms of the basis because the basis vectors span our vector space.</li>
<li>There is no redundancy in this expression because the basis vectors are linearly independent.</li>
</ol>
<p>Recall also that a linear map $T$ between vector spaces $U$ and $V$ is a function $T:U\to V$ for which</p>
<ol>
<li>$T(u_1+u_2) = T(u_1) + T(u_2)$ for any $u_1, u_2 \in U$.</li>
<li>$T(au) = aT(u)$ for any $a\in F$ and any $u\in U$.</li>
</ol>
<p>We learned that linear maps are completely determined by the way that they act on basis vectors. In fact, we can specify the images of the basis vectors and <em>extend by linearity</em> to obtain a linear map on the whole vector space.</p>
<p>Now let's turn everything on its head.</p>
<h3 id="thedualspaceanamethedualspace">The Dual Space<a name="the-dual-space"></a></h3>
<p>Let's define the most important concept of this post:</p>
<blockquote>
<p><strong>Definition.</strong> Given a vector space $V$ over a field $\F$, its <strong>dual space</strong>, written $V^*$, is the set of all linear maps from $V$ to $\F$.</p>
</blockquote>
<p>Of course, we are now talking about $\F$ as a vector space over itself, or else the idea of a linear map would make no sense.</p>
<p>This definition may seem intimidating at first, but it's really not that complicated. An element of the dual space is just a linear function which eats a vector and returns a scalar. Elements of the dual space are often called <strong>covectors</strong> or <strong>linear functionals</strong>.</p>
<p>Now, the fact that the dual space literally has the word &quot;space&quot; in its name is hopefully suggestive that it is itself a vector space. I suppose there might technically be multiple ways to turn this set into a vector space, but the canonical way is as follows:</p>
<blockquote>
<ul>
<li>The zero vector ${\bf 0}\in V^*$ is the zero map ${\bf 0}:V\to\F$ which maps every vector to the zero element of $\F$. That is, ${\bf 0}(v)=0$ for every $v\in V$.</li>
<li>Vector addition is just function addition. That is, if $s$ and $t$ are maps in the dual space, then $s+t$ is another map defined by $(s+t)(v) = s(v) + t(v)$.</li>
<li>Scalar multiplication is inherited directly. That is, if $a$ is a scalar in $\F$ and $t$ is a map in the dual space, then $at$ is another map defined by $(at)(v) = a\cdot t(v)$.</li>
<li>Additive inverses are given by scalar multiplication by $-1$. That is, if $t$ is a map in the dual space then $-t=(-1)\cdot t$.</li>
</ul>
<p>It is hopefully evident that all of the above maps are linear, and that with these definitions, the dual space satisfies the axioms of a vector space. I will not check these properties here because it is not difficult or instructive to do so.</p>
</blockquote>
<p>Now, the next natural question to ask is how the dual space $V^*$ is related to the original space $V$. The answer is not immediately obvious. There is no canonical mapping which takes any vector and picks out a specific covector which it is related to. That's not to say we can't form a bijection from $V$ to $V^*$, it just wouldn't have much meaning, and there is not an obvious candidate for a <em>favored</em> bijection.</p>
<p>In fact, creating such a bijection would require first choosing a basis for $V$. Mathematicians do not consider this to be a natural correspondence, since it relies on picking some arbitrary basis, so we say there is no <strong>natural</strong> or <strong>canonical</strong> correspondence between $V$ and $V^*$.</p>
<p>However, if we try to figure out the dimension of the dual space, the picture begins to become a little clearer. Before we proceed, we'll need the following definition:</p>
<blockquote>
<p><strong>Definition.</strong> Given $n\in\N$, the <strong>Kronecker delta</strong> is the function $\delta:\Z_n\times \Z_n\to\R$ defined by</p>
<p>$$\delta^i_j =<br>
\begin{cases}<br>
0 &amp; \text{if } i\ne j, \\<br>
1 &amp; \text{if } i = j.<br>
\end{cases}$$</p>
</blockquote>
<p>The weird upper and lower index notation in place of a more traditional notation for function arguments, such as $\delta(i, j)$, does have a purpose which will soon become apparent.</p>
<p>Alright, we're now well armed to make a very bold claim:</p>
<blockquote>
<p><strong>Theorem.</strong> If $(e_i)_{i=1}^n$ is a basis for a finite dimensional vector space $V$, then $(e^i)_{i=1}^n$ is a basis for $V^*$, where each basis covector $e^i:V\to\F$ is defined on basis vectors by $$e^i(e_j)=\delta^i_j$$ and defined on all of $V$ by extending by linearity.</p>
<p><strong>Aside.</strong> Before we try to prove this, let's take a look at what these basis covectors really are. Since $(e_i)_{i=1}^n$ is a basis for $V$, we can write any vector $v\in V$ as $v=\sum_{j=1}^n v^j e_j$. Applying the $i$th basis covector to $v$ and using its linearity, we get</p>
<p>$$\begin{align}<br>
e^i(v) &amp;= e^i\left(\sum_{j=1}^n v^j e_j\right) \\<br>
&amp;= \sum_{j=1}^n v^j e^i(e_j) \\<br>
&amp;= \sum_{j=1}^n v^j \delta^i_j \\<br>
&amp;= v^i.<br>
\end{align}$$</p>
<p>That is, $e^i$ is the linear map which picks out only the $i$th component of $v$ and discards the rest. It is in some sense a projection map onto the $i$th coordinate.</p>
<p>In order to understand why the complicated-looking sum above broke down into such a simple expression, recall the defining property of the Kronecker delta function. For almost every $j$, the Kronecker delta was identically zero and so those $v^j$ terms do not contribute to the sum. The only term for which it wasn't zero was the $i$th term, and so we were left with only $v^i$. This ability of the Kronecker delta function to simplify ugly sums is almost magical, and we will see it over and over again.</p>
<p><strong>Proof.</strong> In order to show that $(e^i)_{i=1}^n$ is a basis for $V^*$, we need to show that it is linearly independent and that it spans $V^*$.</p>
<p>We argue first that it is linearly independent. Suppose $\sum_{i=1}^n a_i e^i=0$ for some scalars $(a_i)_{i=1}^n$. Then for any vector $v=\sum_{j=1}^n v^j e_j$ in $V$,</p>
<p>\begin{align}<br>
\left(\sum_{i=1}^n a_i e^i\right)(v) &amp;= \left(\sum_{i=1}^n a_i e^i\right)\left(\sum_{j=1}^n v^j e_j\right) \\<br>
&amp;= \sum_{i=1}^n\sum_{i=j}^n a_i v^j e^i(e_j) \\<br>
&amp;= \sum_{i=1}^n\sum_{i=j}^n a_i v^j \delta^i_j \\<br>
&amp;= \sum_{i=1}^n a_i v^i \\<br>
&amp;= 0.<br>
\end{align}</p>
<p>Since the $v^i$ were arbitrary, the only way this can only be true is if the scalars $(a_i)_{i=1}^n$ are identically zero. Thus, $(e^i)_{i=1}^n$ are linearly independent.</p>
<p>We argue next that the covectors $(e^i)_{i=1}^n$ span the dual space. To this end, suppose $s$ is any covector in $V^*$. For any vector $v=\sum_{j=1}^n v^j e_j$, we have from the linearity of $s$ that</p>
<p>\begin{align}<br>
s(v) &amp;= s\left(\sum_{j=1}^n v^j e_j\right) \\<br>
&amp;= \sum_{j=1}^n v^j s(e_j).<br>
\end{align}</p>
<p>Define $s_j = s(e_j)$ for each $1\le j \le n$. Then</p>
<p>\begin{align}<br>
\left(\sum_{j=1}^n s_j e^j\right)(v) &amp;= \left(\sum_{j=1}^n s_j e^j\right)\left(\sum_{i=1}^n v^i e_i\right) \\<br>
&amp;= \sum_{j=1}^n\sum_{i=1}^n s_j v^i e^j(e_i) \\<br>
&amp;= \sum_{j=1}^n\sum_{i=1}^n s_j v^i \delta^j_i \\<br>
&amp;= \sum_{j=1}^n s_j v^j \\<br>
&amp;= \sum_{j=1}^n v^j s(e_j) \\<br>
&amp;= s(v).<br>
\end{align}</p>
<p>We've thus shown that any covector can be written as a linear combination of the covectors $(e^i)_{i=1}^n$, and thus they span the dual space.</p>
<p>It follows that $(e^i)_{i=1}^n$ forms a basis for $V^*$, as desired.</p>
</blockquote>
<p>We call the basis defined in the proof above the <strong>dual basis</strong>, and it is the basis we usually work with when talking about the dual space. However, it is of course not the <em>only</em> basis we could choose. It is just particularly convenient for our purposes because of the way things tend to simplify via the Kronecker delta.</p>
<p>Hidden in the result above is the fact that $\dim V^*=\dim V$. That's because we've exhibited a basis for $V^*$ consisting of $n$ covectors, which is the definition of the dimension of a vector space. So the dual space always has the same dimension as the original vector space (as long as its finite dimensional)! Which is pretty cool I guess. 😎</p>
<h3 id="thedoubledualspaceanamethedoubledualspace">The Double Dual Space<a name="the-double-dual-space"></a></h3>
<p>The following definition is exactly as you might expect.</p>
<blockquote>
<p><strong>Definition.</strong> Given a vector space $V$ over a field $\F$ and its dual space $\dual{V}$, its <strong>double dual space</strong>, written $\ddual{V}$, is the dual space of $\dual{V}$.</p>
</blockquote>
<p>By now, things may seem hopelessly abstract. If the dual space was the space of all linear functions from $V$ to $\F$, that would make the double dual the space of all linear functions from the space of linear functions from $V$ to $\F$ into $\F$. As if that wasn't complicated enough, there's no end in sight. Am I ever going to stop? Or am I going to next construct the triple dual space, the quadruple dual space, ad infinitum?</p>
<p>It turns out we don't need to keep going, because as we will soon see, $\ddual{V}$ is essentially just $V$.</p>
<blockquote>
<p><strong>Theorem.</strong> Every finite dimensional vector space $V$ over a field $\F$ is canonically isomorphic to its double dual space $\ddual{V}$.</p>
<p><strong>Proof.</strong> Recall first that <em>canonically isomorphic</em> means we can find a isomorphism which does not depend on a choice of basis. So proving the &quot;canonical&quot; part consists only of not choosing a basis to arrive at the result.</p>
<p>Since the dual space of any finite dimensional vector space shares its dimension, it follows that</p>
<p>$$\dim \ddual{V} = \dim \dual{V} = \dim V.$$</p>
<p>Thus, the rank-nullity theorem tells us that a linear map $T:V\to \ddual{V}$ is an isomorphism if it is injective, which greatly simplifies the proof.</p>
<p>Define a map $T:V\to \ddual{V}$ by</p>
<p>$$\big(T(v)\big)(s) = s(v)$$</p>
<p>for any vector $v\in V$ and any covector $s\in \dual{V}$.</p>
<p>Let's pause to make some sense of this. Since $T$ takes $V$ into its double dual, the image $T(v)$ of any vector $v\in V$ will be a linear map in $\ddual{V}$, which itself takes a covector $s\in\dual{V}$. That's why we're defining $T(v)$ by how it acts on a covector $s$.</p>
<p>We will argue that $T$ is an isomorphism. Let's show first that $T$ is a linear map. To this end, suppose $v, v_1, v_2 \in V$ and $a \in \F$. Then because of the linearity of $s$,</p>
<p>$$\begin{align}<br>
\big(T(v_1 + v_2)\big)(s) &amp;= s(v_1 + v_2) \\<br>
&amp;= s(v_1) + s(v_2) \\<br>
&amp;= \big(T(v_1)\big)(s) + \big(T(v_2)\big)(s),<br>
\end{align}$$</p>
<p>and</p>
<p>$$\begin{align}<br>
\big(T(av)\big)(s) &amp;= s(av) \\<br>
&amp;= as(v) \\<br>
&amp;= a\big(T(v)\big)(s).<br>
\end{align}$$</p>
<p>We'll show next that $T$ is injective (and thus bijective by our earlier dimension argument). We will do so by showing that its kernel is trivial. So suppose $v\in\ker(T)$. Then by definition,</p>
<p>$$\begin{align}<br>
\big(T(v)\big)(s) &amp;= s(v) \\<br>
&amp;= 0<br>
\end{align}$$</p>
<p>for all covectors $s\in\dual{V}$. It follows then that $v=0$, since the above holds for any choice of covector $s$, including isomorphisms which themselves must have trivial kernels.</p>
<p>We have shown that $T$ is a bijective linear map, and we have done so without explicitly choosing a basis for any of the vector spaces involved, so it follows that $T$ is a canonical isomorphism, as desired.</p>
</blockquote>
<p>So it turns out that $V$ and $\ddual{V}$ can be used almost interchangeably. But using the double dual space gives us a nice kind of duality (pardon the pun), in that we can think of covectors as maps which act on vectors, and we can think of vectors as maps which act on covectors. Physicists often do this without realizing that they are technically working with cocovectors instead of vectors, but that's fine because the isomorphism makes it work.</p>
<p>I'll leave this here for now. Next time I'll talk about multilinear maps and tensor products!</p>
]]></content:encoded></item><item><title><![CDATA[Simplicial Homology]]></title><description><![CDATA[Homology groups are topological invariants which, informally, give information about the types of holes in a topological space. They are not the only such invariant in algebraic topology, but they are particularly nice to work with since they are always abelian and easy to compute.]]></description><link>https://algebrology.github.io/simplicial-homology/</link><guid isPermaLink="false">5cb3f9f47f36fe003eea102b</guid><category><![CDATA[algebraic topology]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Mon, 15 Apr 2019 04:04:40 GMT</pubDate><content:encoded><![CDATA[<p>Homology groups are topological invariants which, informally, give information about the types of holes in a topological space. They are not the only such invariant in algebraic topology, but they are particularly nice to work with since they are always abelian and easy to compute.</p>
<p>For now, we will restrict our discussion to homology of simplicial complexes. Since many topological spaces of interest can be triangularized into a simplicial complex, this is robust enough for a first treatment of homology. I will talk about the more general theory of singular homology in a later post.</p>
<p>I actually gave the definition of simplicial homology groups in my last post, but it was not motivated by anything in particular. Because the motivation for the construction of homology groups can be difficult to understand, we'll work through the concept by looking at a rather in-depth example. As we will see, the natural and intuitive steps we take in this example generalize to a broader theory of homology.</p>
<p>Let's work with the same simplicial complex we looked at in my last post:</p>
<p><img src="https://algebrology.github.io/content/images/2019/04/simplicial-complex-4.svg" alt="simplicial-complex-4"></p>
<p>We'll call the complex $X$, to match our notation from last time. The $v_i$ are $0$-simplices, the $e_i$ are $1$-simplices and $f$ is a $2$-simplex. We will be working with the chain groups of $X$ as in the previous post. We define $C_0(X)$ as the free abelian group generated by the $v_i$, $C_1(X)$ as the free abelian group generated by the $e_i$, and $C_2(X)$ as the free abelian group generated by $f$.</p>
<p>Notice that elements of $C_0(X)$ are integral linear combinations of the vertices $v_i$. A typical element $\sigma_0 \in C_0(X)$ can be expressed as</p>
<p>$$\sigma_0 = \sum_{i=0}^3 a_iv_i = a_0v_0 + a_1v_1 + a_2v_2 + a_3v_3,$$</p>
<p>where the $a_i$ are integers. We refer to the elements of $C_0(X)$ as $0$-chains.</p>
<p>Similarly, the elements of $C_1(X)$, which we call $1$-chains, are integral linear combinations of the edges $e_i$. A typical element $\sigma_1 \in C_1(X)$ can be written</p>
<p>$$\sigma_1 = \sum_{i=0}^4 b_ie_i = b_0e_0 + b_1e_1 + b_2e_2 + b_3e_3 + b_4e_4,$$</p>
<p>where the $b_i$ are integers. Since the edges are oriented, remember that we think of the negative of an edge as that same edge oriented in the opposite direction. There are only two possible orientations for each edge, so this works out nicely.</p>
<p>Lastly, the elements of $C_2(X)$, the $2$-chains, are integral multiples of the face $f$. A typical element $\sigma_2 \in C_2(X)$ is of the form</p>
<p>$$\sigma_2 = cf,$$</p>
<p>where $c$ is an integer. We will assume that $f=[v_0, v_1, v_2]$ is oriented clockwise.</p>
<p>Last time, we defined the boundary maps between chain groups for any simplicial complex. Let's look at the boundaries of the simplices in $X$. These are determined by the homomorphisms</p>
<p>$$C_2(X)\xrightarrow{\partial_2} C_1(X)\xrightarrow{\partial_1}C_0(X)\xrightarrow{\partial_0}0.$$</p>
<p>Recall the definition of the boundary of an $n$-simplex:</p>
<p>$$\partial_n([v_0, v_1,\ldots, v_n])=\sum_{i=0}^n(-1)^i[v_0,\ldots,v_{i-1},v_{i+1},\ldots,v_n].$$</p>
<p>We see immediately that</p>
<p>$$\partial_0([v_i]) = 0$$</p>
<p>for every $1$-simplex $v_i$. This should make intuitive sense, since a vertex shouldn't have any boundary. This means that $\partial_0$ is the zero map, (as it must be since it's codomain is the trivial group).</p>
<p>Next we see that</p>
<p>$$\begin{align}<br>
\partial_1(e_0) &amp;= \partial_1([v_0, v_1]) = v_1 - v_0 \\[.5em]<br>
\partial_1(e_1) &amp;= \partial_1([v_1, v_2]) = v_2 - v_1 \\[.5em]<br>
\partial_1(e_2) &amp;= \partial_1([v_2, v_0]) = v_0 - v_2 \\[.5em]<br>
\partial_1(e_3) &amp;= \partial_1([v_3, v_2]) = v_2 - v_3 \\[.5em]<br>
\partial_1(e_4) &amp;= \partial_1([v_0, v_3]) = v_3 - v_0.<br>
\end{align}$$</p>
<p>Since homomorphisms between free abelian groups are uniquely determined by the way they act on generators, we immediately know how to calculate the boundary $\partial_1$ of any $1$-chain in $X$. If</p>
<p>$$\sigma_1 = \sum_{i=0}^4 b_ie_i$$</p>
<p>then from the properties of homomorphisms we see that</p>
<p>$$\begin{align}<br>
\partial_1(\sigma_1) &amp;= \partial_1\left(\sum_{i=0}^4 b_ie_i\right) \\<br>
&amp;= \sum_{i=0}^4 b_i \partial_1(e_i) \\[1em]<br>
&amp;= b_0(v_1-v_0) + b_1(v_2-v_1) + b_2(v_0-v_2) + b_3(v_2-v_3) + b_4(v_3-v_0) \\[1em]<br>
&amp;= (-b_0+b_2-b_4)v_0 + (b_0-b_1)v_1 + (b_1-b_2+b_3)v_2 + (-b_3+b_4)v_3.<br>
\end{align}$$</p>
<p>Of particular interest to us are cycles in $C_1(X)$. As you might expect, a cycle is a $1$-chain which starts where it began and traverses the edges in a natural order. For instance, we would like</p>
<p>$$\begin{align}<br>
\sigma_1 &amp;= e_0 + e_1 + e_2, \\[1em]<br>
\sigma_2 &amp;= -2e_2 -2e_4 -2e_3<br>
\end{align}$$</p>
<p>to be considered cycles, since they are natural paths around the simplex and they form closed loops. However,</p>
<p>$$\begin{align}<br>
\sigma_3 &amp;= e_0 + e_1, \\[1em]<br>
\sigma_4 &amp;= 3e_1 -e_4<br>
\end{align}$$</p>
<p>should not be, as $\sigma_3$ does not form a closed loop and $\sigma_4$ is not even a natural path.</p>
<p>How should we algebraically determine which chains are cycles? To answer this question, let's take a look at the boundaries of the $1$-chains we just discussed. Remember that chain groups are abelian, so we can rearrange terms. This is extremely important!</p>
<p>$$\begin{align}<br>
\partial_1(\sigma_1) &amp;= \partial(e_0 + e_1 + e_2) \\[.5em]<br>
&amp;= \partial(e_0) + \partial(e_1) + \partial(e_2) \\[.5em]<br>
&amp;= (v_1-v_0) + (v_2-v_1) + (v_0-v_2) \\[.5em]<br>
&amp;= (v_0 - v_0) + (v_1 - v_1) + (v_2 - v_2) \\[.5em]<br>
&amp;= 0, \\[2em]<br>
\partial_1(\sigma_2) &amp;= \partial(-2e_2 -2e_4 -2e_3) \\[.5em]<br>
&amp;= -2\partial(e_2) -2\partial(e_4) + -2\partial(e_3) \\[.5em]<br>
&amp;= -2(v_0-v_2) -2(v_3-v_0) -2(v_2-v_3) \\[.5em]<br>
&amp;= -2(v_0 - v_0) -2(v_2 - v_2) -2(v_3 - v_3) \\[.5em]<br>
&amp;= 0, \\[2em]<br>
\partial_1(\sigma_3) &amp;= \partial(e_0 + e_1) \\[.5em]<br>
&amp;= \partial(e_0) + \partial(e_1)\\[.5em]<br>
&amp;= (v_1-v_0) + (v_2-v_1)\\[.5em]<br>
&amp;= -v_0 + (v_1-v_1) + v_2 \\[.5em]<br>
&amp;= -v_0 + v_2, \\[2em]<br>
\partial_1(\sigma_4) &amp;= \partial(3e_1 -e_4) \\[.5em]<br>
&amp;= 3\partial(e_1) -\partial(e_4) \\[.5em]<br>
&amp;= 3(v_2-v_1) -(v_3-v_0) \\[.5em]<br>
&amp;= -v_0 -3v_1 + 3v_2 -v_3. \\[.5em]<br>
\end{align}$$</p>
<p>So It looks like the $1$-chains that ought to be cycles have a boundary of $0$, whereas the others have nonzero boundary. This should make some sense, since remember that the boundary of a $1$-simplex is its second vertex minus its first vertex. So if the edges of a chain follow a natural path, i.e., the chain is of the form</p>
<p>$$\sigma = [v_0, v_1] + [v_1, v_2] + [v_2, v_3] + \cdots + [v_{n-2}, v_{n-1}] + [v_{n-1}, v_n]$$</p>
<p>then the terms in its boundary will telescope and only the extreme points will remain. That is,</p>
<p>$$\begin{align}<br>
\partial_1(\sigma) &amp;= \partial_1([v_0, v_1] + [v_1, v_2] + [v_2, v_3] + \cdots +[v_{n-2}, v_{n-1}] + [v_{n-1}, v_n]) \\[.5em]<br>
&amp;= \partial([v_0, v_1]) + \partial([v_1, v_2]) + \partial([v_2, v_3]) + \cdots + \partial([v_{n-2}, v_{n-1}]) + \partial([v_{n-1}, v_n]) \\[.5em]<br>
&amp;= (v_1-v_0) + (v_2-v_1) + (v_3-v_2) + \cdots + (v_{n-1}-v_{n-2}) + (v_n-v_{n-1}) \\[.5em]<br>
&amp;= -v_0 + (v_1-v_1) + (v_2-v_2) + \cdots + (v_{n-1}-v_{n-1}) + v_n\\[.5em]<br>
&amp;= -v_0 + v_n.<br>
\end{align}$$</p>
<p>If it happens that the chain is also closed, then its first and last vertices are the same. That is, $v_0=v_n$, and so $\partial_1(\sigma)=0$. That means that any cycle is in the kernel of the boundary map! It thus makes sense to make the following definition:</p>
<blockquote>
<p><strong>Definition.</strong> If $X$ is a simplicial complex with chain groups and boundary operators</p>
<p>$$\cdots\xrightarrow{\partial_{n+1}} C_n(X)\xrightarrow{\partial_n} C_{n-1}(X)\xrightarrow{\partial_{n-1}}\cdots\xrightarrow{\partial_2}C_1(X)\xrightarrow{\partial_1}C_0(X)\xrightarrow{\partial_0}0,$$</p>
<p>then for each natural number $n$, the <strong>group of $n$-cycles</strong> in $X$ is defined as</p>
<p>$$Z_n(X)=\ker\partial_n.$$</p>
</blockquote>
<p>This subgroup consists of all $n$-chains whose boundary is zero. That is, precisely the chains which we called cycles. Since each cycle group $Z_n(X)$ is the kernel of a homomorphism, it is always a normal subgroup of $C_n(X)$.</p>
<p>Remember that the whole point of this endeavor is to identify the holes in our simplicial complex $X$. Intuitively, just from looking at the diagram of $X$, it seems there should only be one hole: that empty region bounded by the edges $e_2$, $e_4$ and $e_3$.</p>
<p>How do we instinctively know this? Because $e_2+e_3+e_4$ is a a cycle in $Z_1(X)$ but it is not the boundary of any $2$-chain in $X$. Those edges could feasibly form the boundary of a $2$-simplex, but there is simply not a $2$-simplex sitting there. That tells us there's a hole in our complex.</p>
<p>On the other hand, $e_0+e_1+e_2$ is a cycle in $Z_1(X)$ and it is the boundary of the $2$-chain $f$, so there is no hole there.</p>
<p>What I'm saying is that <strong>holes in $X$ correspond to cycles which are not also the boundary of anything in $X$.</strong></p>
<p>How do we know which chains are also boundaries? Well that's easy! Any boundary of a simplex in $X$ is the image of that simplex under the boundary map.</p>
<blockquote>
<p><strong>Definition.</strong> If $X$ is a simplicial complex with chain groups and boundary operators</p>
<p>$$\cdots\xrightarrow{\partial_{n+1}} C_n(X)\xrightarrow{\partial_n} C_{n-1}(X)\xrightarrow{\partial_{n-1}}\cdots\xrightarrow{\partial_2}C_1(X)\xrightarrow{\partial_1}C_0(X)\xrightarrow{\partial_0}0,$$</p>
<p>then for each natural number $n$, the <strong>group of $n$-boundaries</strong> in $X$ is defined as</p>
<p>$$B_n(X)=\im\partial_{n+1}.$$</p>
</blockquote>
<p>This subgroup consists of all $n$-chains which are the image of an $(n+1)$-chain under the $(n+1)$th boundary map. That is, $B_n(X)$ consists of all boundaries of the $(n+1)$-chains in $X$.</p>
<p>We showed last time that the boundary of a boundary is always zero, i.e., $(\partial_{n}\circ \partial_{n+1})(\sigma)=0$ for all $n$. We also showed that this implies that $\im\partial_{n+1}\subseteq\ker\partial_n$. Or, in the terminology of this post, $B_n(X)\subseteq Z_n(X)$.</p>
<p>This should make intuitive sense, since every boundary of a simplex should be a cycle but not all cycles are necessarily boundaries. In fact, it is precisely the cycles that are <em>not</em> boundaries that we are interested in. Since each chain group is free abelian, this implies that $B_n(X)$ is a normal subgroup of $Z_n(X)$. We're finally ready to define homology groups!</p>
<blockquote>
<p><strong>Definition.</strong> If $X$ is a simplicial complex with chain groups and boundary operators</p>
<p>$$\cdots\xrightarrow{\partial_{n+1}} C_n(X)\xrightarrow{\partial_n} C_{n-1}(X)\xrightarrow{\partial_{n-1}}\cdots\xrightarrow{\partial_2}C_1(X)\xrightarrow{\partial_1}C_0(X)\xrightarrow{\partial_0}0,$$</p>
<p>then for each natural number $n$, the <strong>$n$th simplicial homology group</strong> of $X$ is defined as the quotient group</p>
<p>$$H_n(X)=\frac{Z_n(X)}{B_n(X)} = \frac{\ker\partial_n}{\im\partial_{n+1}}.$$</p>
</blockquote>
<p>In the case of our example, $Z_1(X)$ is the free abelian group generated by the two basic cycles in $X$. That is, any cycle in $Z_1(X)$ is an integral combination of $e_0+e_1+e_2$ and $e_2+e_4+e_3$. Since it is generated by two elements, $\rank Z_1(X)=2$.</p>
<p>On the other hand, $B_1(X)$ is the free abelian group generated by boundaries of the $2$-simplices in $X$. But there is only one $2$-simplex, namely $f$. The boundary of $f$ is the chain $e_0+e_1+e_2$, so any boundary in $B_1(X)$ is an integral multiple of this chain. Since it is generated by a single element, $\rank B_1(X)=1$.</p>
<p>This implies that $\rank H_1(X)=2-1=1$. This coincides with our intuitive idea that there is one hole in the complex $X$. In general, the rank of the $n$-th homology group tells us how many $n$-dimensional holes are in a simplicial complex.</p>
<p>The elements of the $1$st homology group $H_1(X)$ are the cosets of $B_1(X)$. That is, any element $\sigma\in H_1(X)$ is of the form</p>
<p>$$\sigma = B_1(X) + \gamma$$</p>
<p>where $\gamma$ is any $1$-cycle in $Z_1(X)$. If $\gamma$ is a multiple of the chain $e_0+e_1+e_2$ then $\gamma\in B_1(X)$ and so we have the identity coset. Otherwise, $\gamma$ contains some multiple of the other cycle, $e_2+e_4+e_3$, and so $B_1(X) + \gamma$ is not the identity. This is why the rank of $H_1(X)$ is $1$ in this case — because the only distinct cosets are the identity and $B_1(X) + e_2+e_4+e_3$.</p>
<p>I'll leave you with this for now. Next time I'll discuss an algorithm for actually computing homology groups of simplicial complexes!</p>
]]></content:encoded></item><item><title><![CDATA[Simplicial Complexes and Boundary Maps]]></title><description><![CDATA[The idea behind our definitions is that lots of topological spaces can be "triangularized" in such a way that they look sort of like a bunch of "triangles" glued together.]]></description><link>https://algebrology.github.io/simplicial-complexes-and-boundary-maps/</link><guid isPermaLink="false">5c96fb2d7f36fe003eea0f77</guid><category><![CDATA[algebraic topology]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Sun, 14 Apr 2019 03:48:00 GMT</pubDate><content:encoded><![CDATA[<ol>
<li><a href="#linear-maps">Simplicial Complexes</a></li>
<li><a href="#chain-groups">Chain Groups</a></li>
<li><a href="#boundary-maps">Boundary Maps</a></li>
</ol>
<hr>
<h3 id="simplicialcomplexesanamesimplicialcomplexes">Simplicial Complexes<a name="simplicial-complexes"></a></h3>
<p>Let's define the types of topological spaces that are of interest to us in this post. The idea behind our definitions is that lots of topological spaces can be &quot;triangularized&quot; in such a way that they look sort of like a bunch of &quot;triangles&quot; glued together.</p>
<blockquote>
<p><strong>Definition.</strong> For any $n\in\N$, the <strong>standard $n$-simplex</strong> is the subset of $\R^{n+1}$ given by</p>
<p>$$\Delta^n =\bset{(x_1,x_2,\ldots,x_{n+1})\in\R^{n+1}\mid x_i\ge 0 \text{ for all } i \text{ and } \sum_{i=1}^{n+1} x_i = 1}.$$</p>
</blockquote>
<p>That is, $\Delta^n$ is the set of points in $\R^{n+1}$ whose components are all positive and sum to $1$. Clearly in $\R$ the only such point is $1$ itself, and so</p>
<p>$$\Delta^0=\set{1}.$$</p>
<p>In $\R^2$ the points which satisfy these conditions are precisely the points which lie on the line segment connecting the point $(1,0)$ to the point $(0,1)$, including these two points. For instance, the points $\left(\frac{1}{3}, \frac{2}{3}\right)$ and $\left(\frac{1}{2}, \frac{1}{2}\right)$ certainly satisfy the requirements and sit on this line segment. It follows that $\Delta^1$ is a line segment in $\R^2$.</p>
<p><img src="https://algebrology.github.io/content/images/2019/04/delta_1-1.svg" alt="delta_1-1"></p>
<p>The simplex $\Delta^2$ is a triangle in $\R^3$ which contains its interior and is bounded by the line segments connecting the points $(1,0,0)$, $(0,1,0)$ and $(0,0,1)$.</p>
<p><img src="https://algebrology.github.io/content/images/2019/04/delta_2.svg" alt="delta_2"></p>
<p>$\Delta^3$ is a solid tetrahedron in $\R^4$ (although we can embed it in $\R^3$ to visualize it). By solid, I mean that it contains its entire interior.</p>
<p><img src="https://algebrology.github.io/content/images/2019/04/delta_3.svg" alt="delta_3"></p>
<p>Higher-dimensional simplices cannot be visualized so easily. However, they are still relatively easy objects to manipulate formally. And they follow a very interesting pattern:</p>
<ul>
<li>The $1$-simplex is a line with two $0$-simplices at either end.</li>
<li>The $2$-simplex is a triangle whose edges are three $1$-simplices and whose vertices are three $0$-simplices.</li>
<li>The $3$-simplex is a tetrahedron whose faces are four $2$-simplices, whose edges are six $1$-simplices, and whose vertices are four $0$-simplices.</li>
</ul>
<p>Recall Pascal's triangle:</p>
<p><img src="https://algebrology.github.io/content/images/2019/04/pascal.svg" alt="pascal"></p>
<p>Each element in the triangle (that isn't a $1$) is the sum of the two values directly above it. Equivalently, we can find the value in the $n$th row and $k$th column using the binomial formula:</p>
<p>$${n \choose k} = \frac{n!}{k!(n-k)!}.$$</p>
<p>Pascal's triangle is intimately related to the number of different dimensional faces of a simplex. The number of $k$-faces of an $n$-simplex is the value in column $(k+1)$ and row $(n+1)$ of the triangle, which you can verify quite easily for the simplices given above. For higher dimensional simplices, you'll just have to take my word for it for now.</p>
<p>So far we've only looked at standard simplices, but we can just as easily talk about any point as a non-standard $0$-simplex, any line as a non-standard $1$-simplex, any triangle as a non-standard $2$-simplex, any tetrahedron as a non-standard $3$-simplex, etc. The preferred way to define these arbitrary simplices would be as the images of standard simplices under bijective affine transformations. (An affine transformation is basically just a linear transformation plus a translation, so that it is not necessarily origin-preserving.) However, we will work with them intuitively as follows:</p>
<blockquote>
<p><strong>Definition.</strong> Given a set of affine-independent vertices $v_0,v_2,\ldots,v_n$ in $\R^{n+1}$, the <strong>(oriented) $n$-simplex</strong> corresponding to these vertices is the convex hull of the vertices,</p>
<p>$$\bset{a_0v_0 + a_1v_1 + \cdots + a_nv_n\in\R^{n+1} \mid a_i\ge 0 \text{ for all } i \text{ and } \sum_{i=1}^{n+1} a_i = 1},$$</p>
<p>and is denoted by</p>
<p>$$[v_0, v_1, \ldots, v_n].$$</p>
</blockquote>
<p>We're now ready to define simplicial complexes. The basic idea is that they are built from simplices which have been glued together in a particularly nice way.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>simplicial complex</strong> $X$ is a finite collection of simplices for which</p>
<ol>
<li>If $s$ is a simplex in $X$ then every face of $s$ is also in $X$.</li>
<li>If $s_1$ and $s_2$ are simplices in $X$ then either they are disjoint or their intersection $s_1 \cap s_2$ is a face of both $s_1$ and $s_2$.</li>
</ol>
</blockquote>
<p>This definition ensures that the gluing of simplices is done in such a way that all the vertices and faces line up nicely. And of course, we make a simplicial complex $X$ into a topological space by taking the union of all its simplices and considering it as a topological subspace of $\R^{n+1}$, where $n$ is the maximum dimension of any simplex in $X$.</p>
<blockquote>
<p><strong>Example.</strong> Here's an example of a simplicial complex which we'll be revisiting later. It should technically live in $\R^3$ since $f$ is a $2$-simplex, but this one just happens to be flat enough to embed in $\R^2$.</p>
<p><img src="https://algebrology.github.io/content/images/2019/04/simplicial-complex-3.svg" alt="simplicial-complex-3"></p>
<p>Notice that the edges have arrows. This is because they are oriented simplices. That is, $[v_0, v_1]$ is a different simplex than $[v_1, v_0]$, and the arrows are visual indicators of the orientation. Technically $f$ is oriented as well, but I have not added a visual indication of this fact. We will treat $f$ as the simplex $[v_0, v_1, v_2]$.</p>
<p>With all of that in mind, this complex consists of the following simplices:</p>
<p><strong>0-simplices</strong>:</p>
<div style="margin-left:1.5em; line-height: 2.5em; margin-top:-1.5em">
$v_0$
<p>$v_1$<br>
$v_2$<br>
$v_3$</p>
</div>
<p><strong>1-simplices</strong>:</p>
<div style="margin-left:1.5em; line-height: 2.5em; margin-top:-1.5em">
$e_0 = [v_0, v_1]$
<p>$e_1 = [v_1, v_2]$<br>
$e_2 = [v_2, v_0]$<br>
$e_3 = [v_3, v_2]$<br>
$e_4 = [v_0, v_3]$</p>
</div>
<p><strong>2-simplices</strong>:</p>
<div style="margin-left:1.5em; line-height: 2.5em; margin-top:-1.5em; margin-bottom:2em">
$f=[v_0, v_1, v_2]$
</div>
</blockquote>
<h3 id="chaingroupsanamechaingroups">Chain Groups<a name="chain-groups"></a></h3>
<p>In an earlier post, I developed the machinery of free abelian groups. Now let's finally put that to use.</p>
<blockquote>
<p><strong>Definition.</strong> Let $X$ denote a simplicial complex. For each $n\in\N$, the <strong>group of simplicial $n$-chains</strong>, written $C_n(X)$, is the free abelian group with basis all $n$-simplices in $X$. The elements of $C_n(X)$ are called <strong>simplicial $n$-chains</strong> in $X$.</p>
</blockquote>
<p>Stated more plainly, an $n$-chain in a simplicial complex $X$ is a formal integral combination of $n$-simplices in $X$. For example, in the simplicial complex pictured above, we may talk about $1$-chains such as</p>
<p>$$3e_0 + 2e_2-e_3.$$</p>
<p>Since our simplices are oriented, it makes sense to define the negative of a simplex to be that simplex with its verticed traversed in the opposite order. That is,</p>
<p>$$-[v_0,v_1,\ldots,v_n] = [v_n,\ldots, v_1, v_0].$$</p>
<p>We may thus write the chain groups for our simplicial complex as follows:</p>
<p>$$\begin{align}<br>
C_0 &amp;= \bset{\sum_{i=0}^4 a_iv_i \mid a_i\in\Z}, \\[1em]<br>
C_1 &amp;= \bset{\sum_{i=0}^3 b_ie_i \mid b_i\in\Z}, \\[1em]<br>
C_2 &amp;= \bset{cf \mid c\in\Z}, \\[1em]<br>
C_n &amp;= \set{0} \text{ for } n\ge 3.<br>
\end{align}$$</p>
<p>It is natural to associate chains in $C_1$ to actual paths in the simplex $X$. For instance, the chain $\sigma = e_0 + e_1 + e_2$ traces out a path all the way around $f$ from $v_0$ to $v_1$ to $v_2$ and back to $v_0$.</p>
<p>Its negative, $-\sigma = -e_2-e_1-e_0$ traces out the same path but in the opposite direction.</p>
<p>Its double, $2\sigma = 2e_0 + 2e_1 + 2e_2$ traces out the same path twice in a row. That is, it loops around $f$ two times instead of just once.</p>
<p>It might seem natural to view the chain $\sigma$ as the <em>boundary</em> of the simplex $f$. Let's work toward making this idea more formal.</p>
<h3 id="boundarymapsanameboundarymaps">Boundary Maps<a name="boundary-maps"></a></h3>
<p>First lets define the boundary of a $1$-simplex, since it is the easiest to visualize. Since a $1$-simplex is essentially just a closed line segment, its boundary should consist solely of its endpoints, which are $0$-simplices. However, the directedness of the edges indicates that we should add/subtract the endpoints rather than take their union.</p>
<p>Thus, we define the boundary of the $1$-simplex $[v_0, v_1]$ to be the $0$-chain $v_1-v_0$.</p>
<p>Similarly, we would like the boundary of the $2$-simplex $[v_0, v_1, v_2]$ to be the sum of its edges $[v_0, v_1]$, $[v_1, v_2]$ and $[v_2, v_0]$. So we define the boundary of $[v_0, v_1, v_2]$ as</p>
<p>$$[v_0, v_1] + [v_1, v_2] + [v_2, v_0] = [v_1, v_2] - [v_0, v_2] + [v_0, v_1].$$</p>
<p>Note that the expressions on both sides are completely equal, just with the order permuted and $[v_2, v_0]$ negated twice. The reason for this rearrangement will become apparent shortly.</p>
<p>It is <em>very</em> important to note that the boundary of an $n$-simplex is always an $(n-1)$-chain!</p>
<p>In general, we will define boundary homomorphisms not just on simplices but between chain groups. First we will describe how they act on generators and then we will extend by linearity to a homomorphism on the chain groups.</p>
<p>Let $X$ be a simplicial complex and let $B_n(X)$ denote the set of $n$-simplices in $X$. That is, $B_n(X)$ is a basis for the free abelian group $C_n(X)$, the group of $n$-chains in $X$. Define a function $\partial'_n:B_n(X)\to C_{n-1}(X)$ as follows: For any oriented $n$-simplex $\sigma=[v_0, v_1,\ldots, v_n]$, we let</p>
<p>$$\partial'_n(\sigma) = \partial'_n([v_0, v_1,\ldots, v_n])=\sum_{i=0}^n(-1)^i[v_0,\ldots,v_{i-1},v_{i+1},\ldots,v_n].$$</p>
<p>That is, an alternating sum of $n-1$-simplices where the $i$th term is missing the $i$th vertex.</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>boundary map</strong> of dimension $n$ is the unique homomorphism $\partial_n:C_n(X)\to C_{n-1}(X)$ given by extending the function $\partial'_n$ by linearity.</p>
</blockquote>
<p>Notice that these boundary maps form a sequence</p>
<p>$$\cdots\xrightarrow{\partial_{n+1}} C_n(X)\xrightarrow{\partial_n} C_{n-1}(X)\xrightarrow{\partial_{n-1}}\cdots\xrightarrow{\partial_2}C_1(X)\xrightarrow{\partial_1}C_0(X)\xrightarrow{\partial_0}0.$$</p>
<p>Of course, $\partial_0$ is the zero map, since $0$ is the trivial group.</p>
<p>It is easy to see that these boundary maps are all well defined and that $\partial_n(-\sigma)=-\partial_n(\sigma)$ for any $n$-simplex $\sigma$. Thus, boundary maps are not affected by the orientation of simplices in a chain, as long as the orientations are consistent.</p>
<p>Next, we will prove an extremely important and useful result concerning the structure of chain groups and boundary maps — namely, the boundary of a boundary is always zero.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ be a simplicial complex with $\sigma\in X$ an $n$-simplex. Then $(\partial_{n}\circ \partial_{n+1})(\sigma)=0$.</p>
<p><strong>Proof.</strong> The proof is a direct computation.</p>
<p>\begin{align*}<br>
\big(\partial_{n-1}\circ\partial_n\big)\left([v_0,\dotsc,v_n]\right)&amp;=\sum_{i=0}^n(-1)^i\partial_{n-1}[v_0,\dotsc,v_{i-1},v_{i+1},\dotsc,v_n]\\<br>
&amp;= \sum_{j&lt;i}(-1)^i(-1)^j[v_0,\dotsc,v_{j-1},v_{j+1},\dotsc,v_{i-1},v_{i+1},\dotsc,v_n]\\<br>
&amp;\phantom{==} +\sum_{j&gt;i}(-1)^i(-1)^{j-1}[v_0,\dotsc,v_{i-1},v_{i+1},\dotsc,v_{j-1},v_{j+1},\dotsc,v_n]\\<br>
&amp;=0<br>
\end{align*} because the terms in the last two summations cancel in pairs.</p>
</blockquote>
<p>Recall that the kernel and image of any group homomorphism are subgroups of the domain and codomain, respectively. The result above actually implies that the image of any boundary map is a subgroup of the kernel of the next boundary map!</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ be a simplicial complex. Then $\im\partial_{n+1}\subseteq\ker\partial_n$ for any natural number $n$.</p>
<p><strong>Proof.</strong> Choose any $\beta\in\im\partial_{n+1}$. Then $\beta=\partial_{n+1}(\sigma)$ for some chain $\sigma\in C_{n+1}$. But then</p>
<p>$$\begin{align}<br>
\partial_n(\beta) &amp;= \partial_n(\partial_{n+1}(\sigma)) \\<br>
&amp;=0<br>
\end{align}$$</p>
<p>from the above theorem, and so $\beta\in\ker\partial_n$. It follows that $\im\partial_{n+1}\subseteq\ker\partial_n$.</p>
</blockquote>
<p>In addition to the kernel and image being subgroups, because we are working strictly with abelian groups this implies that they are normal subgroups. In particular, since  $\im\partial_{n+1}\subseteq\ker\partial_n$, it follows that $\im\partial_{n+1}$ is a normal subgroup of $\ker\partial_n$. This means that we can actually form quotient groups $\frac{{\ker\partial_n}}{\im\partial_{n+1}}$ for every natural number $n$. This will become extremely important when we talk about homology in the next post. In fact, this is the definition of a homology group! But its motivation will become more clear later.</p>
]]></content:encoded></item><item><title><![CDATA[Matrices for Linear Maps]]></title><description><![CDATA[If you already knew some linear algebra before reading my posts, you might be wondering where the heck all the matrices are. The goal of this post is to connect the theory of linear maps and vector spaces to the theory of matrices and computation.]]></description><link>https://algebrology.github.io/matrices-for-linear-maps/</link><guid isPermaLink="false">5c933214bdf47d003ee073df</guid><category><![CDATA[linear algebra]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Thu, 21 Mar 2019 07:06:11 GMT</pubDate><content:encoded><![CDATA[<p>If you already knew some linear algebra before reading my posts, you might be wondering where the heck all the matrices are. The goal of this post is to connect the theory of linear maps and vector spaces to the theory of matrices and computation.</p>
<p>The key observation is that linear maps between finite-dimensional vector spaces can be completely described solely in terms of how they act on basis vectors. Let's explore this idea in a more precise setting.</p>
<p>Suppose $T:U\to V$ is a linear map with $\dim U = n$ and $\dim V = m$. Let $(e_1,\ldots,e_n)$ be a basis for $U$ and let $(f_1,\ldots,f_m)$ be a basis for $V$. Then for any vector $u\in U$, we may express $u$ uniquely as a linear combination of basis vectors,</p>
<p>$$u=\sum_{i=1}^n a_i e_i,$$</p>
<p>where $a_i\in\F$ for $1\le i\le n$. Since $T$ is a linear map, this means that</p>
<p>$$\begin{align}<br>
Tu &amp;= T\left(\sum_{i=1}^n a_i e_i\right) \\<br>
&amp;= \sum_{i=1}^n a_i Te_i. \tag{1}<br>
\end{align}$$</p>
<p>We can go further and decompose each $Te_i\in V$ as a linear combination of basis vectors in $V$,</p>
<p>$$Te_i = \sum_{j=1}^m b_{j,i} f_j,$$</p>
<p>where $b_{j,i}\in\F$ for $1\le j\le m$. Thus, we may rewrite equation $(1)$ as</p>
<p>$$Tu = \sum_{i=1}^n \sum_{j=1}^m a_i b_{j,i} f_j.$$</p>
<p>Since the scalars $a_i$ depend only on the input vector $u$ and the basis vectors $f_j$ are fixed, this means that the way $T$ acts on $u$ is determined entirely by the scalars $b_{j,i}$.</p>
<p>To put it another way, say we've chosen bases for $U$ and $V$. Then <em>everything</em> about the linear map is encoded in a collection of $m\times n$ scalars, which describe how the map acts on basis vectors in $U$ in terms of basis vectors in $V$.</p>
<p>If we were, for instance, to write the scalars $b_{j,i}$ in the following form:</p>
<p>$$\begin{bmatrix}<br>
b<br>
\end{bmatrix}_{j,i} = \begin{bmatrix}<br>
b_{1,1} &amp; b_{1,2} &amp; \cdots &amp; b_{1,n} \\<br>
b_{2,1} &amp; b_{2,2} &amp; \cdots &amp; b_{2,n} \\<br>
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\<br>
b_{m,1} &amp; b_{m,2} &amp; \cdots &amp; b_{m,n}<br>
\end{bmatrix},$$</p>
<p>then we would have a compact and easy to read way of describing the linear map $T$.</p>
<p>This is what we call an <strong>$\mathbf{m\times n}$ matrix</strong>. In this case, $[b_{j,i}]$ is the matrix corresponding to the linear transformation $T$ with respect to the bases $(e_1,\ldots,e_n)$ for $U$ and $(f_1,\ldots,f_m)$ for $V$. We sometimes write $\M(T)$ denote the matrix for $T$, assuming we have already established which bases we are using for our domain and codomain.</p>
<p>It can be a bit confusing at first trying to remember what the shape of a linear map's matrix should be. The number of columns corresponds to the dimension of the domain, and the number of rows corresponds to the dimension of the codomain. The $i$th column of the matrix is really just the components of $Te_i$. That is, we may think of the matrix $\M(T)$ as</p>
<p>$$\M(T) = \begin{bmatrix}<br>
\mid &amp; \mid &amp;        &amp; \mid \\<br>
Te_1   &amp; Te_2   &amp; \cdots &amp; Te_n \\<br>
\mid &amp; \mid &amp;        &amp; \mid<br>
\end{bmatrix}.$$</p>
<p>The scalar $b_{j,i}$ in the $j$th row and $i$th column of $\M(T)$ is thus the coefficient of the $j$th basis vector $f_j$ for $V$ in the representation of $Te_i$.</p>
<p>If $T:\F_1^n\to\F_2^m$ is a linear map between vector spaces $\F_1^n$ and $\F_2^m$, where $\F_1$ and $\F_2$ are fields, then unless otherwise specified it should be assumed that its matrix $\M(T)$ is with respect to the standard bases on $\F_1^n$ and $\F_2^m$. Otherwise, it does not make sense to talk about a matrix for a linear map without first explicitly choosing bases for the vector spaces involved. <strong>This is very important.</strong></p>
<blockquote>
<p><strong>Example.</strong> Recall the zero map we defined last time, $0:U\to V$ where $U(u)=0$ for every $u\in U$. Suppose $(e_1,\ldots,e_n)$ is any basis for $U$ and $(f_1,\ldots,f_m)$ is any basis for $V$.</p>
<p>To compute the matrix for the zero map with respect to these bases, all we need to do is figure out how it acts on basis vectors. Choose and $i$ for which $1\le i\le n$. Then</p>
<p>$$\begin{align}<br>
0(e_i) &amp;= \sum_{j=1}^m b_{j,i} f_j \\<br>
&amp;= 0<br>
\end{align}$$</p>
<p>for some scalars $b_{j,i}\in\F$. But since $(f_1,\ldots,f_m)$ is a basis for $V$, it is linearly independent and so this is only possible if $b_{j,i}=0$ for $1\le j\le m$. That means the matrix for the zero map looks like this:</p>
<p>$$\M(0) = \begin{bmatrix}<br>
0 &amp; 0 &amp; \cdots &amp; 0 \\<br>
0 &amp; 0 &amp; \cdots &amp; 0 \\<br>
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\<br>
0 &amp; 0 &amp; \cdots &amp; 0<br>
\end{bmatrix}.$$</p>
<p>And it looks like this for any choice of bases for $U$ and $V$, since there's only one way to write the zero vector in terms of any basis.</p>
<p>We call this the <strong>zero matrix</strong> of dimension $m\times n$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> Recall the identity map we defined last time, $I:V\to V$ where $Iv=v$ for every $v\in V$. Suppose $(e_1,\ldots,e_n)$ is any basis for $V$.</p>
<p>How does $I$ act on basis vectors? Choose any $i$ with $1\le i\le n$. Then</p>
<p>$$\begin{align}<br>
Ie_i &amp;= \sum_{j=1}^n b_{j,i} e_j \\<br>
&amp;= e_i.<br>
\end{align}$$</p>
<p>Since $(e_1,\ldots,e_n)$ is linearly independent, this is only possible if $b_{j,i} = 0$ for every $j\ne i$ and $b_{i,i} = 1$. That is,</p>
<p>$$b_{j,i} = \begin{cases}<br>
0 &amp; \text{if } j\ne i, \\<br>
1 &amp; \text{if } j=i.<br>
\end{cases} \tag{2}$$</p>
<p>This means that the matrix for the identity map is a square matrix which looks like this:</p>
<p>$$\M(I) = \begin{bmatrix}<br>
1 &amp; 0 &amp; \cdots &amp; 0 \\<br>
0 &amp; 1 &amp; \cdots &amp; 0 \\<br>
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\<br>
0 &amp; 0 &amp; \cdots &amp; 1<br>
\end{bmatrix},$$</p>
<p>with $1$s along the main diagonal and $0$s everywhere else. And it looks like this for any choice of basis for $V$, as long as we use <em>the same</em> basis for both copies of $V$ (the domain and the codomain). Otherwise, all bets are off.</p>
<p>We call this the <strong>identity matrix</strong> of dimension $n\times n$.</p>
<p>We also give a name to formula $(2)$ for the scalars $b_{j,i}$. The <strong>Kronecker delta</strong> is a function of two variables defined by</p>
<p>$$\delta_{j,i} = \begin{cases}<br>
0 &amp; \text{if } j\ne i, \\<br>
1 &amp; \text{if } j=i.<br>
\end{cases}$$</p>
<p>We can rewrite the identity map in terms of the Kronecker delta, if we want:</p>
<p>$$\M(I) = \begin{bmatrix}<br>
\delta_{1,1} &amp; \delta_{1,2} &amp; \cdots &amp; \delta_{1,n} \\<br>
\delta_{2,1} &amp; \delta_{2,2} &amp; \cdots &amp; \delta_{2,n} \\<br>
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\<br>
\delta_{n,1} &amp; \delta_{n,2} &amp; \cdots &amp; \delta_{n,n}<br>
\end{bmatrix}.$$</p>
</blockquote>
<p>There are a number of important operations we can define on matrices.</p>
<blockquote>
<p><strong>Definition.</strong> If $\begin{bmatrix} a \end{bmatrix}_{j,i}$ and $\begin{bmatrix} b \end{bmatrix}_{j,i}$ are $m\times n$ matrices, their <strong>sum</strong> is defined component-wise:</p>
<p>$$\begin{align}<br>
\begin{bmatrix} a \end{bmatrix}_{j,i} + \begin{bmatrix} b \end{bmatrix}_{j,i} &amp;= \begin{bmatrix}<br>
a_{1,1} &amp; a_{1,2} &amp; \cdots &amp; a_{1,n} \\<br>
a_{2,1} &amp; a_{2,2} &amp; \cdots &amp; a_{2,n} \\<br>
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\<br>
a_{m,1} &amp; a_{m,2} &amp; \cdots &amp; a_{m,n}<br>
\end{bmatrix} + \begin{bmatrix}<br>
b_{1,1} &amp; b_{1,2} &amp; \cdots &amp; b_{1,n} \\<br>
b_{2,1} &amp; b_{2,2} &amp; \cdots &amp; b_{2,n} \\<br>
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\<br>
b_{m,1} &amp; b_{m,2} &amp; \cdots &amp; b_{m,n}<br>
\end{bmatrix} \\[1em]<br>
&amp;= \begin{bmatrix}<br>
a_{1,1} + b_{1,1} &amp; a_{1,2} + b_{1,2} &amp; \cdots &amp; a_{1,n} + b_{1,n} \\<br>
a_{2,1} + b_{2,1} &amp; a_{2,2} + b_{2,2} &amp; \cdots &amp; a_{2,n} + b_{2,n} \\<br>
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\<br>
a_{m,1} + b_{m,1} &amp; a_{m,2} + b_{m,2} &amp; \cdots &amp; a_{m,n} + b_{m,n}<br>
\end{bmatrix}.<br>
\end{align}$$</p>
</blockquote>
<p>It is easy to show that with this definition and the normal operation of addition of functions, $M(T_1 + T_2) = M(T_1) + M(T_2)$ for any linear maps $T_1,T_2:U\to V$.</p>
<p>Addition of matrices is not defined for matrices of different sizes, since this would be like trying to add functions with different domains and codomains.</p>
<blockquote>
<p><strong>Definition.</strong> If $\begin{bmatrix} a \end{bmatrix}_{j,i}$ is an $m\times n$ matrix, its <strong>scalar product</strong> with $k\in\F$ is defined component-wise:</p>
<p>$$\begin{align}<br>
k\begin{bmatrix} a \end{bmatrix}_{j,i} &amp;= k\begin{bmatrix}<br>
a_{1,1} &amp; a_{1,2} &amp; \cdots &amp; a_{1,n} \\<br>
a_{2,1} &amp; a_{2,2} &amp; \cdots &amp; a_{2,n} \\<br>
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\<br>
a_{m,1} &amp; a_{m,2} &amp; \cdots &amp; a_{m,n}<br>
\end{bmatrix} \\[1em]<br>
&amp;= \phantom{k}\begin{bmatrix}<br>
ka_{1,1} &amp; ka_{1,2} &amp; \cdots &amp; ka_{1,n} \\<br>
ka_{2,1} &amp; ka_{2,2} &amp; \cdots &amp; ka_{2,n} \\<br>
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\<br>
ka_{m,1} &amp; ka_{m,2} &amp; \cdots &amp; ka_{m,n}<br>
\end{bmatrix}.<br>
\end{align}$$</p>
</blockquote>
<p>So far we have seen a lot of matrices but I have not shown you what they are useful for. The rest of this post will address their uses.</p>
<p>Suppose we have a linear map $T:U\to V$ and bases $(e_1,\ldots,e_n)$ for $U$ and $(f_1,\ldots,f_m)$ for $V$. Then for any $u\in U$, we may write</p>
<p>$$u = \sum_{i=1}^n u_i e_i,$$</p>
<p>where the scalars $u_i\in\F$ are unique. We can define a new sort of matrix — the <strong>matrix of the vector $u$</strong>, as follows:</p>
<p>$$\M(u) = \begin{bmatrix}<br>
u_1 \\<br>
u_2 \\<br>
\vdots \\<br>
u_n<br>
\end{bmatrix}.$$</p>
<p>The natural question to ask is this: given the matrix $M(T)$ for the linear map $T$ and the matrix $M(u)$ for the vector $u$, what does the matrix $M(Tu)$ for the vector $Tu\in V$ look like? All we need to do is calculate its components in terms of the basis $(f_1,\ldots,f_m)$,</p>
<p>$$Tu = \sum_{i=1}^n \sum_{j=1}^m u_i b_{j,i} f_j.$$</p>
<p>So the matrix for $Tu$ is</p>
<p>$$\begin{align}<br>
\M(Tu) &amp;= \begin{bmatrix}<br>
(Tu)_1 \\<br>
(Tu)_2 \\<br>
\vdots \\<br>
(Tu)_m<br>
\end{bmatrix} \\[1em]<br>
&amp;= \begin{bmatrix}<br>
\sum_{i=1}^n u_i b_{1,i} \\<br>
\sum_{i=1}^n u_i b_{2,i} \\<br>
\vdots \\<br>
\sum_{i=1}^n u_i b_{m,i}<br>
\end{bmatrix}.<br>
\end{align}$$</p>
<p>Observe that all of the information about $\M(Tu)$ is encoded in the matrices $\M(T)$ and $\M(u)$. So we make the following definition:</p>
<blockquote>
<p><strong>Definition.</strong> If $\begin{bmatrix} b \end{bmatrix}_{j,i}$ is an $m\times n$ matrix for a linear map and $\begin{bmatrix} u \end{bmatrix}_{i}$ is an $n\times 1$ matrix for a vector, we define <strong>matrix-vector</strong> multiplication as follows:</p>
<p>$$\begin{align}<br>
\begin{bmatrix} b \end{bmatrix}_{j,i} \begin{bmatrix} u \end{bmatrix}_{i} &amp;= \begin{bmatrix}<br>
b_{1,1} &amp; b_{1,2} &amp; \cdots &amp; b_{1,n} \\<br>
b_{2,1} &amp; b_{2,2} &amp; \cdots &amp; b_{2,n} \\<br>
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\<br>
b_{m,1} &amp; b_{m,2} &amp; \cdots &amp; b_{m,n}<br>
\end{bmatrix} \begin{bmatrix}<br>
u_1 \\<br>
u_2 \\<br>
\vdots \\<br>
u_n<br>
\end{bmatrix} \\[1em]<br>
&amp;= \begin{bmatrix}<br>
\sum_{i=1}^n u_i b_{1,i} \\<br>
\sum_{i=1}^n u_i b_{2,i} \\<br>
\vdots \\<br>
\sum_{i=1}^n u_i b_{m,i}<br>
\end{bmatrix} \\[1em]<br>
&amp;= \begin{bmatrix} \sum_{i=1}^n u_i b_{j,i} \end{bmatrix}_j.<br>
\end{align}$$</p>
</blockquote>
<p>By definition then, we have that $\M(Tu)=\M(T)\M(u)$. This means if we have a matrix representation of a linear map and a matrix representation of a vector, both in terms of some bases, then we work entirely in terms of matrices. Incidentally, this is why it is common to write $Tu$ to mean $T(u)$ — because the action of the linear map $T$ on the vector $u$ can be calculated via multiplication of their matrices.</p>
<p>Now, suppose we have three vector spaces $U$, $V$ and $W$, of dimensions $n$, $m$ and $l$, respectively. Let $T:U\to V$ and $S:V\to W$ be linear maps, and let $(e_1,\ldots,e_n)$ be a basis for $U$, $(f_1,\ldots,f_m)$ be a basis for $V$, and $(g_1,\ldots,g_l)$ be a basis for $W$. We have already seen that if</p>
<p>$$u=\sum_{i=1}^n u_i e_i$$</p>
<p>and</p>
<p>$$Te_i = \sum_{i=1}^m b_{j,i}f_j$$</p>
<p>then</p>
<p>$$Tu = \sum_{i=1}^n \sum_{i=1}^m u_i b_{j,i} f_j.$$</p>
<p>Let's take this one step further, and compute $S(Tu)$. Suppose $S$ acts on each basis vector $f_j$ as follows:</p>
<p>$$Sf_j = \sum_{k=1}^l c_{k,j}g_k.$$</p>
<p>Then</p>
<p>$$\begin{align}<br>
S(Tu) &amp;= S\left(\sum_{i=1}^n \sum_{i=1}^m u_i b_{j,i} f_j\right) \\<br>
&amp;= \sum_{i=1}^n \sum_{i=1}^m u_i b_{j,i} Sf_j \\<br>
&amp;= \sum_{i=1}^n \sum_{i=1}^m \sum_{k=1}^l u_i b_{j,i} c_{k,j} g_k.<br>
\end{align}$$</p>
<p>We thus make the following definition:</p>
<blockquote>
<p><strong>Definition.</strong> If $\begin{bmatrix} b \end{bmatrix}_{j,i}$ is an $m\times n$ matrix and $\begin{bmatrix} c \end{bmatrix}_{k,j}$ is an $l\times m$ matrix, we defined <strong>matrix-matrix</strong> multiplication as follows:</p>
<p>$$\begin{align}<br>
\begin{bmatrix} c \end{bmatrix}_{k,j} \begin{bmatrix} b \end{bmatrix}_{j,i} &amp;= \begin{bmatrix}<br>
c_{1,1} &amp; c_{1,2} &amp; \cdots &amp; c_{1,m} \\<br>
c_{2,1} &amp; c_{2,2} &amp; \cdots &amp; c_{2,m} \\<br>
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\<br>
c_{l,1} &amp; c_{l,2} &amp; \cdots &amp; c_{l,m}<br>
\end{bmatrix}\begin{bmatrix}<br>
b_{1,1} &amp; b_{1,2} &amp; \cdots &amp; b_{1,n} \\<br>
b_{2,1} &amp; b_{2,2} &amp; \cdots &amp; b_{2,n} \\<br>
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\<br>
b_{m,1} &amp; b_{m,2} &amp; \cdots &amp; b_{m,n}<br>
\end{bmatrix} \\[1em]<br>
&amp;= \begin{bmatrix}<br>
\sum_{j=1}^m b_{j,1}c_{1,j} &amp; \sum_{j=1}^m b_{j,2}c_{1,j} &amp; \cdots &amp; \sum_{j=1}^m b_{j,n}c_{1,j} \\<br>
\sum_{j=1}^m b_{j,1}c_{2,j} &amp; \sum_{j=1}^m b_{j,2}c_{2,j} &amp; \cdots &amp; \sum_{j=1}^m b_{j,n}c_{2,j} \\<br>
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\<br>
\sum_{j=1}^m b_{j,1}c_{l,j} &amp; \sum_{j=1}^m b_{j,2}c_{l,j} &amp; \cdots &amp; \sum_{j=1}^m b_{j,n}c_{l,j}<br>
\end{bmatrix} \\[1em]<br>
&amp;= \begin{bmatrix} \sum_{j=1}^m b_{j,i} c_{k,j} \end{bmatrix}_{k,i}.<br>
\end{align}$$</p>
<p>Note that the product is an $l\times n$ matrix.</p>
</blockquote>
<p>By definition then, we have that $\M(S)\M(T)=\M(S\circ T)$. Which is super awesome! Matrix multiplication corresponds to function composition of linear maps. And of course the dimensions of the product should be $l\times n$, since $S\circ T:U\to W$. We often write $S\circ T$ as simply $ST$, since this is reminiscent of multiplication.</p>
<p>I'm going to end here for now because all of these indices are making my head spin. But we'll see matrices again all over the place.</p>
]]></content:encoded></item><item><title><![CDATA[Linear Maps]]></title><description><![CDATA[Just as we decided to study continuous functions between topological spaces and homomorphisms between groups, much of linear algebra is dedicated to the study of linear maps between vector spaces.]]></description><link>https://algebrology.github.io/linear-maps/</link><guid isPermaLink="false">5c918ffabdf47d003ee07357</guid><category><![CDATA[linear algebra]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Wed, 20 Mar 2019 08:49:00 GMT</pubDate><content:encoded><![CDATA[<ol>
<li><a href="#linear-maps">Linear Maps</a></li>
<li><a href="#basic-properties">Basic Properties</a></li>
<li><a href="#rank-and-nullity">Rank and Nullity</a></li>
</ol>
<hr>
<p>Sooo somewhere along the line I dropped the arrow notation for vectors. I was using $\v$ instead of $v$ to denote vectors to keep things clean and avoid confusion, but it was inevitable that this would eventually stop happening because it was a lot of work on my end.</p>
<h3 id="linearmapsanamelinearmaps">Linear Maps<a name="linear-maps"></a></h3>
<p>Just as we decided to study continuous functions between topological spaces and homomorphisms between groups, much of linear algebra is dedicated to the study of linear maps between vector spaces.</p>
<p>If you've been paying close attention, you may have noticed that vector spaces are really just groups to which we have added scalar multiplication. In the same vein, linear maps are really just homomorphisms with an additional property which makes them compatible with scalar multiplication.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>linear map</strong> (or <strong>linear transformation</strong>) between two vector spaces $U$ and $V$ is a function $T:U\to V$ for which the following properties hold:</p>
<div style="margin-left:1.5em">
<p><strong>Additivity</strong><br>
For any vectors $u_1,u_2\in U$, we have that $T(u_1+u_2) = T(u_1) + T(u_2)$.</p>
<p><strong>Homogeneity</strong><br>
For any vector $u\in U$ and any scalar $a\in\F$, we have that $T(au)=aT(u)$.</p>
</div>
<p>It is conventional to write $Tu$ to mean $T(u)$, when no confusion arises from this shorthand. The reason for this notation will become apparent shortly.</p>
</blockquote>
<p>It is important to note that on the left side of each equation in this definition, the vector addition and scalar multiplication are being done in $U$. That is, we are applying $T$ to the sum $u_1+u_2\in U$ and to the scalar product $au\in U$. On the right, the addition and multiplication are being done in $V$. That is, we are first applying $T$ to $u_1$ and $u_2$ separately and adding the result in $V$, or applying it to $u$ and multiplying the result in $V$. So linear maps are precisely those functions for which it does not matter whether we add/multiply before or after applying the function.</p>
<blockquote>
<p><strong>Example.</strong> We define a &quot;zero map&quot; $0:U\to V$ between any two vector spaces by $0(u)=0$ for all vectors $u\in U$. This is obviously a linear map because</p>
<p>$$\begin{align}<br>
0(u_1 + u_2) &amp;= 0 \\<br>
&amp;= 0 + 0 \\<br>
&amp;= 0(u_1) + 0(u_2),<br>
\end{align}$$</p>
<p>which shows that it satisfies additivity, and</p>
<p>$$\begin{align}<br>
0(au) &amp;= 0 \\<br>
&amp;= a0 \\<br>
&amp;= a0(u),<br>
\end{align}$$</p>
<p>which shows that it satisfies homogeneitry.</p>
</blockquote>
<p>Note that I am making no distinction between the zero vector in the domain and the zero vector in the codomain. Nonetheless, it is important to realize that they may be different objects. For example, in the case of a linear map from $\R^n$ to $\P(\F)$, the zero vector in $\R^n$ is the $n$-tuple $(0,0,\ldots,0)$ and the zero vector in $\P(\F)$ is the zero polynomial $p(x)=0$.</p>
<p>What's even worse though it that I'm also using the symbol $0$ to represent the zero map. Maybe I just worship chaos.</p>
<blockquote>
<p><strong>Example.</strong> The identity map $I:U\to V$ between any two vector spaces, defined as usual by $Iu=u$ for all vectors $u\in U$, is a linear map. To see that it is additive, note that</p>
<p>$$\begin{align}<br>
I(u_1+u_2) &amp;= u_1+u_2 \\<br>
&amp;= Iu_1 + Iu_2.<br>
\end{align}$$</p>
<p>To see that it is homogeneous, note that</p>
<p>$$\begin{align}<br>
I(au) &amp;= au \\<br>
&amp;= aIu.<br>
\end{align}$$</p>
</blockquote>
<p>The identity map is <em>super</em> important in linear algebra.</p>
<blockquote>
<p><strong>Example.</strong> The map $T:\R^2\to\R$ defined by $T(x,y)=x+y$ for any vector $(x,y)\in\R^2$ is a linear map.</p>
<p>This map is additive because</p>
<p>$$\begin{align}<br>
T((x_1,y_1)+(x_2,y_2)) &amp;= T(x_1+x_2,y_1+y_2) \\<br>
&amp;= x_1+x_2+y_1+y_2 \\<br>
&amp;= x_1+y_1+x_2+y_2 \\<br>
&amp;= T(x_1,y_1) + T(x_2,y_2).<br>
\end{align}$$</p>
<p>It is homogeneous because</p>
<p>$$\begin{align}<br>
T(a(x,y)) &amp;= T(ax,ay) \\<br>
&amp;= ax+ay \\<br>
&amp;= a(x+y) \\<br>
&amp;= aT(x,y).<br>
\end{align}$$</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> The map $T:\R^3\to\P_2(\R)$ defined by $T(a,b,c)=a+bx+(a-b)x^2$ is a linear map.</p>
<p>It's additive because</p>
<p>$$\begin{align}<br>
T((a_1,b_1)+(a_2,b_2)) &amp;= T(a_1+a_2,b_1+b_2) \\<br>
&amp;= a_1+a_2 + (b_1+b_2)x + (a_1+a_2 - b_1-b_2)x^2\\<br>
&amp;= a_1 + b_1x + (a_1-b_1)x^2 + a_2 + b_2x + (a_2-b_2)x^2\\<br>
&amp;= T(a_1,b_1) + T(a_2,b_2).<br>
\end{align}$$</p>
<p>And it's homogeneous because</p>
<p>$$\begin{align}<br>
T(k(a,b)) &amp;= T(ka,kb) \\<br>
&amp;= ka + kbx + (ka-kb)x^2 \\<br>
&amp;= k(a+bx+(a-b)x^2) \\<br>
&amp;= kT(a,b).<br>
\end{align}$$</p>
</blockquote>
<h3 id="basicpropertiesanamebasicproperties">Basic Properties<a name="basic-properties"></a></h3>
<p>What follows are some basic properties of linear maps.</p>
<p>The first result says that linear maps take the zero vector in the domain to the zero vector in the codomain.</p>
<blockquote>
<p><strong>Theorem.</strong> If $T:U\to V$ is a linear map, then $T(0)=0$.</p>
<p><strong>Proof.</strong> We proceed directly via the following computation:</p>
<p>$$\begin{align}<br>
T(0) &amp;= T(0) + 0 \\<br>
&amp;= T(0) + T(0) - T(0) \\<br>
&amp;= T(0 + 0) - T(0) \\<br>
&amp;= T(0) - T(0) \\<br>
&amp;= 0.<br>
\end{align}$$</p>
</blockquote>
<p>Linear maps do a lot more than that though. They also map subspaces to subspaces!</p>
<blockquote>
<p><strong>Theorem.</strong> If $T:U\to V$ and $W$ is a subspace of $U$ then $T[W]$ is a subspace of $V$.</p>
<p><strong>Proof.</strong> We just need to show that $T[W]$ contains the zero vector and is closed under vector addition and scalar multiplication.</p>
<p>Since $W$ is a subspace, it contains the zero vector in $U$. Thus, since $T(0)=0$, it follows that $T[W]$ contains the zero vector in $V$.</p>
<p>Next, suppose $v_1,v_2\in T[W]$. Then $v_1=Tw_1$ and $v_2=Tw_2$ for some vectors $w_1,w_2\in W$. But since $W$ is a subspace, it contains $w_1+w_2$. Thus,</p>
<p>$$\begin{align}<br>
v_1 + v_2 &amp;= Tw_1 + Tw_2 \\<br>
&amp;= T(w_1 + w_2) \\<br>
&amp;\in T[W].<br>
\end{align}$$</p>
<p>Lastly, suppose $v\in T[W]$ and $a\in\F$. Then $v=Tw$ for some $w\in W$. But since $W$ is a subspace, it contains $aw$. Thus,</p>
<p>$$\begin{align}<br>
av &amp;= aTw \\<br>
&amp;= T(aw) \\<br>
&amp;\in T[W].<br>
\end{align}$$</p>
<p>It follows that $T[W]$ is a subspace of $V$, as desired.</p>
</blockquote>
<p>The kernel of any linear transformation is a subspace of the domain.</p>
<blockquote>
<p><strong>Theorem.</strong> If $T:U\to V$ then $\ker T$ is a subspace of $U$.</p>
<p><strong>Proof.</strong> Again, we need to show that $\ker T$ contains the zero vector and is closed under vector addition and scalar multiplication.</p>
<p>Since $T(0)=0$, certainly $0\in\ker T$ by definition.</p>
<p>Next, suppose $u_1,u_2\in\ker T$. Then $Tu_1=0$ and $Tu_2=0$. Thus,</p>
<p>$$\begin{align}<br>
T(u_1 + u_2) &amp;= Tu_1 + Tu_2 \\<br>
&amp;= 0 + 0 \\<br>
&amp;= 0,<br>
\end{align}$$</p>
<p>and so $u_1+u_2\in\ker T$.</p>
<p>Lastly, suppose that $u\in\ker T$ and $a\in\F$. Then $Tu=0$, and therefore</p>
<p>$$\begin{align}<br>
T(au) &amp;= aTu \\<br>
&amp;= a0 \\<br>
&amp;= 0,<br>
\end{align}$$</p>
<p>so $au\in\ker T$. It follows that $\ker T$ is a subspace of $U$.</p>
</blockquote>
<p>For some reason in linear algebra it is common to refer to the kernel of a linear transformation as its <strong>null space</strong>, and to write $\ker T$ as $\null T$. I personally find this annoying since in every other context we refer to fiber of $0$ as the kernel. But you should at least be aware that this convention exists and is prevalent in the literature.</p>
<p>In the same vein, the image of any linear transformation is a subspace of the codomain. Actually we've already done all the work to show this.</p>
<blockquote>
<p><strong>Theorem.</strong> If $T:U\to V$ then $\im T$ is a subspace of $V$.</p>
<p><strong>Proof.</strong> Since linear maps take subspaces to subspaces, and $U$ is a subspace of itself, it follows that $\im T$ is a subspace of $V$.</p>
</blockquote>
<h3 id="rankandnullityanamerankandnullity">Rank and Nullity<a name="rank-and-nullity"></a></h3>
<p>Now we will explore the interesting relation between the dimensions of the kernel and image of a linear map. It will take some time to build up to the &quot;big result&quot; of this section, but it is well worth the wait.</p>
<p>To understand this connection, we begin by considering what injective and surjective linear maps look like.</p>
<blockquote>
<p><strong>Theorem.</strong> A linear map $T:U\to V$ is injective if and only if $\ker T = \set{0}$.</p>
<p><strong>Proof.</strong> Suppose first that $\ker T=\set{0}$, and suppose further that $Tu_1=Tu_2$. Then</p>
<p>$$\begin{align}<br>
0 &amp;= Tu_1 - Tu_2 \\<br>
&amp;= T(u_1-u_2),<br>
\end{align}$$</p>
<p>and so $u_1-u_2\in\ker T$. But $\ker T$ contains only one element — the zero vector. So $u_1-u_2=0$, meaning $u_1=u_2$. It follows that $T$ is injective.</p>
<p>Suppose conversely that $T$ is injective. We know that $T(0)=0$. Furthermore, since $T$ is injective, at most one vector in the domain can be mapped to any vector in the codomain. This implies that $0$ is the only vector which gets mapped to zero. Hence, $\ker T=\set{0}$.</p>
</blockquote>
<p>Injective linear maps have an extremely special property. Namely, they preserve linear independence!</p>
<blockquote>
<p><strong>Theorem.</strong> Let $T:U\to V$ denote an injective linear map, and suppose $(u_1,\ldots,u_n)$ is a linearly independent list of vectors in $U$. Then $(Tu_1,\ldots,Tu_n)$ is a linearly independent list of vectors in $V$.</p>
<p><strong>Proof.</strong> Suppose we have scalars $a_i\in\F$ for $1\le i\le n$ such that</p>
<p>$$\begin{align}<br>
\sum_{i=1}a_iTu_i &amp;= T\left(\sum_{i=1}^n a_i u_i\right) \\<br>
&amp;= 0.<br>
\end{align}$$</p>
<p>Since $T$ is injective, we know that $\ker T=\set{0}$ and therefore</p>
<p>$$\sum_{i=1}^n a_i u_i = 0.$$</p>
<p>Since $(u_1,\ldots,u_n)$ is linearly independent, this implies $a_i=0$ for $1\le i\le n$. Thus, $(Tu_1,\ldots,Tu_n)$ is linearly independent.</p>
</blockquote>
<p>The next theorem is probably pretty obvious. It just says that if the domain of a linear map is finite-dimensional then so is its image.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $T:U\to V$ denote a linear map. If $U$ is finite-dimensional, then $\im T$ is a finite-dimensional subspace of $V$.</p>
<p><strong>Proof.</strong> We have already shown that $\im T$ is a subspace of $V$. To show that it is finite-dimensional, we must demonstrate that it is spanned by a finite list of vectors in $V$. Let $(e_1,\ldots,e_n)$ be any basis for $U$. We claim that</p>
<p>$$\im T = \span(Te_1,\ldots,Te_n).$$</p>
<p>To see this, choose any vector $v\in\im T$. Then $v=Tu$ for some $u\in U$. We may write $u$ as a linear combination of basis vectors</p>
<p>$$u = \sum_{i=1}^n a_i e_i,$$</p>
<p>where $a_i\in\F$ for $i\le i\le n$. Thus,</p>
<p>$$\begin{align}<br>
v &amp;= Tu \\<br>
&amp;= T\left(\sum_{i=1}^n a_i e_i\right) \\<br>
&amp;= \sum_{i=1}^n a_i Te_i.<br>
\end{align}$$</p>
<p>We have expressed $v$ as a linear combination of the vectors $Te_1,\ldots,Te_n$. It follows that</p>
<p>$$\im T = \span(Te_1,\ldots,Te_n).$$</p>
<p>Since $\im T$ is spanned by a finite list of vectors, it is finite-dimensional.</p>
</blockquote>
<p>We now come to the main theorem of this post, which states that the dimension of the domain of a linear map is always equal to the sum of the dimensions of its kernel and image.</p>
<blockquote>
<p><strong>Rank-Nullity Theorem.</strong> Suppose $U$ is a finite-dimensional vector space, $V$ is any vector space, and $T:U\to V$ is a linear map. Then</p>
<p>$$\dim U = \dim\ker T + \dim\im T.$$</p>
<p><strong>Proof.</strong> Let $n=\dim\ker T$, and choose any basis $(e_1,\ldots,e_n)$ for $\ker T$. Then we can extend this list to a basis $(e_1,\ldots,e_n,f_1,\ldots,f_m)$ for $U$ by adding $m=\dim U - n$ linearly independent vectors. We argue that $\dim\im T=m$ by showing that $(Tf_1,\ldots,Tf_m)$ is a basis for $\im T$.</p>
<p>Certainly $\im T = \span(Tf_1,\ldots,Tf_m)$ since $\span(Te_1,\ldots,Te_n)=\set{0}$ as all the $e_i$ are in the kernel of $T$. It suffices to show then that $(Tf_1,\ldots,Tf_m)$ is linearly independent.</p>
<p>To this end, suppose we are given $a_i$ for $1\le i\le m$ such that</p>
<p>$$\sum_{i=1}^m a_i Tf_i = 0.$$</p>
<p>Then since $T$ is a linear map, we have that</p>
<p>$$T\left(\sum_{i=1}^m a_i f_i\right) = 0.$$</p>
<p>It follows that</p>
<p>$$\sum_{i=1}^m a_i f_i\in\ker T,$$</p>
<p>and so we can rewrite this uniquely in terms of basis vectors for $\ker T$:</p>
<p>$$\sum_{i=1}^m a_i f_i = \sum_{i=1}^n b_i e_i, \tag{1}$$</p>
<p>where $b_i\in\F$ for $i\le 1\le n$. Since $(e_1,\ldots,e_n,f_1,\ldots,f_m)$ is a basis for $U$, it is linearly independent. Thus, equation $(1)$ implies that $a_i=0$ for $1\le i\le m$ and $b_i=0$ for $1\le i\le n$. Thus, $(Tf_1,\ldots,Tf_m)$ is linearly independent and so it is a basis for $\im T$. It follows that $\dim\im T = m$, and thus</p>
<p>$$\dim U = \dim\ker T + \dim\im T.$$</p>
</blockquote>
<p>Essentially what this means is that with any linear map, every dimension of the domain is accounted for. By this, I really mean that every one-dimensional subspace is either squashed down to zero and thus in the kernel, or its linear dependence is preserved and it gets mapped to a unique one-dimensional subspace of the codomain.</p>
<p>The reason this is called the rank-nullity theorem is because we make the following definitions:</p>
<blockquote>
<p><strong>Definition.</strong> Let $T:U\to V$ denote a linear map. The <strong>rank</strong> of $T$ is</p>
<p>$$\text{rank }T = \dim\im T.$$</p>
<p>Similarly, the <strong>nullity</strong> of $T$ is</p>
<p>$$\text{nullity }T = \dim\ker T.$$</p>
</blockquote>
<p>The rank-nullity theorem has an interesting implication.</p>
<blockquote>
<p><strong>Theorem.</strong> If $U$ and $V$ are finite-dimensional vector spaces with $\dim U=\dim V$ and $T:U\to V$ is a linear map. Then $T$ is injective if and only if it is surjective.</p>
<p><strong>Proof.</strong> Suppose first that $T$ is injective, so that $\ker T=\set{0}$. Then $\dim\ker T = 0$, so from the rank-nullity theorem it follows that</p>
<p>$$\begin{align}<br>
\dim\im T &amp;= \dim U \\<br>
&amp;= \dim V,<br>
\end{align}$$</p>
<p>and thus $\im T = V$ because the only subspace of a finite-dimensional vector space whose dimension is the same as the parent space is the parent space itself. Thus, $T$ is surjective.</p>
<p>Suppose next that $T$ is surjective. Then</p>
<p>$$\begin{align}<br>
\dim\im T &amp;= \dim V \\<br>
&amp;= \dim U.<br>
\end{align}$$</p>
<p>From the rank-nullity theorem, it follows that $\dim\ker T = 0$. The only subspace with dimension zero is the trivial subspace, and so $\ker T = \set{0}$. Thus, $T$ is injective.</p>
</blockquote>
<p>So for two finite-dimensional vector spaces of the same dimension, injective and surjective linear maps are actually the same thing. Remember that a function that is both injective and surjective is called bijective and has an inverse function. As you shall see, all kinds of interesting results follow from this observation.</p>
]]></content:encoded></item><item><title><![CDATA[Bases and Dimension of Vector Spaces]]></title><description><![CDATA[Bases for vector spaces are similar to bases for topological spaces. The idea is that a basis is a small, easy to understand subset of vectors from which it is possible to extrapolate pretty much everything about the vector space as a whole.]]></description><link>https://algebrology.github.io/bases-and-dimension-of-vector-spaces/</link><guid isPermaLink="false">5c914d58bdf47d003ee072b9</guid><category><![CDATA[linear algebra]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Tue, 19 Mar 2019 20:45:57 GMT</pubDate><content:encoded><![CDATA[<ol>
<li><a href="#bases">Bases</a></li>
<li><a href="#dimension">Dimension</a></li>
<li><a href="#some-useful-theorems">Some Useful Theorems</a></li>
</ol>
<hr>
<h3 id="basesanamebases">Bases<a name="bases"></a></h3>
<p><a href="https://algebrology.github.io/vector-spaces-2">Previously</a>, I discussed span and linear independence. Essentially, the span of a list of vectors is the set of all possible linear combinations of those vectors, and a list of vectors is linearly independent if none of them can be expressed as a linear combination of the others.</p>
<p>Now I'll use these ideas to introduce one of the most important concepts in linear algebra:</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>basis</strong> for a vector space $V$ is a linearly independent list of vectors which spans $V$.</p>
</blockquote>
<p>This definition, combined with what we have already discussed, tells us three things:</p>
<ol>
<li>Every vector in $V$ can be expressed as a linear combination of basis vectors, since a basis must span $V$.</li>
<li>This representation is unique, since the basis elements are linearly independent.</li>
<li>$V$ is the direct sum of the spans of the basis vectors.</li>
</ol>
<p>Stated more explictly, if $(e_1,\ldots,e_n)$ is a basis for a vector space $V$, then for every vector $v\in V$ there is a unique set of scalars $a_i\in\F$ for which</p>
<p>$$v=\sum_{i=1}^n a_ie_i.$$</p>
<p>Bases for vector spaces are similar to bases for topological spaces. The idea is that a basis is a small, easy to understand subset of vectors from which it is possible to extrapolate pretty much everything about the vector space as a whole.</p>
<p>Here are some examples and non-examples.</p>
<blockquote>
<p><strong>Example.</strong> The list $\big((1,0), (0,1)\big)$ is a basis for the vector space $\R^2$, since the list is linearly independent and for any vector $(x,y)\in\R^2$, we may write</p>
<p>$$(x,y) = x(1,0)+y(0,1).$$</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> The list $\big((1,0), (1,1)\big)$ is also a basis for $\R^2$. To see that it is linearly independent, we must show that</p>
<p>$$a(1,0) + b(1,1) = (0,0) \tag{1}$$</p>
<p>only if $a=b=0$. Since in this vector space addition and scalar multiplication are done component-wise, equation $(1)$ reduces to the system of equations:</p>
<p>$$\begin{align}<br>
a + b &amp;= 0 \tag{2}\\<br>
\phantom{a +} b &amp;= 0. \tag{3}<br>
\end{align}$$</p>
<p>Equation $(2)$ implies that $a=-b$, from which equation $(3)$ implies that $a=b=0$, as desired.</p>
<p>To see that our list spans $\R^2$, we note that any vector $(x,y)\in\R^2$ can be written</p>
<p>$$(x,y) = (x-y)(1,0) + y(1,1).$$</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> The list $\big((1,0),(-1,0)\big)$ is not a basis for $\R^2$ because it is linearly dependent and it does not span $\R^2$. To see that it is linearly dependent, note that</p>
<p>$$(1,0)+(-1,0)=(0,0).$$</p>
<p>That is, we have written the zero vector as a linear combination of vectors in the list with nonzero coefficients.</p>
<p>To see that it does not span $\R^2$, note that no vector in $\R^2$ with a nonzero second component can ever be written as a scalar multiple of these vectors, since both have a second component of zero.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> The list $\big((1,0),(0,1),(1,1)\big)$ is not a basis for $\R^2$ because it is linearly dependent. We can see this immediately from the linear dependence lemma, since it has length three and we have already exhibited a linearly dependent list of length two which spans $\R^2$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> The list $\big(1,x,x^2,x^3)$ is a basis for $\P_3(\F)$, the vector space of polynomials over a field $\F$ with degree at most three. To justify this claim, note that this list is certainly linearly independent, and any polynomial of degree less than or equal three can be written</p>
<p>$$p(x)=a+b+cx^2+dx^3$$</p>
<p>for some choice of $a,b,c,d\in\F$.</p>
</blockquote>
<p>The next theorem is fairly obvious, but without it we would be pretty much lost. Recall that a vector space is finite-dimensional if it is spanned by a finite list of vectors. For this next proof we will use this definition, as well the linear dependence lemma.</p>
<blockquote>
<p><strong>Theorem.</strong> Every finite-dimensional vector space has a basis.</p>
<p>Let $V$ denote a finite-dimensional vector space. Then $V$ is spanned by some finite list of vectors, not all zero, $(v_1,\ldots,v_n)$. If this list is linearly independent, then we are done. Otherwise, we can use the linear dependence lemma to remove a vector from this list and produce a new list of length $n-1$ which still spans $V$. We continue this process until we are left with a linearly independent list, which will take at most $n-1$ steps, since any list containing one nonzero vector is automatically linearly independent. This resulting list is, by definition, a basis for $V$.</p>
</blockquote>
<p>This argument only works for finite-dimensional vector spaces, since it relies on the fact that we only have to apply the linear dependence lemma a finite number of times. Infinite-dimensional vector spaces, as I've mentioned before, are a lot grosser. It is possible to show that infinite-dimensional vector spaces are guaranteed to have bases if we accept the axiom of choice. Since most mathematicians much prefer to have bases for their vector spaces, this is just one more point in favor of accepting the axiom of choice. Luckily for us, we aren't currently interested in infinite-dimensional vector spaces and so we can simply ignore this icky business.</p>
<h3 id="dimensionanamedimension">Dimension<a name="dimension"></a></h3>
<p>Next comes a super important result which will finally settle the question I posed last time about how to define the dimension of a vector space. Its proof will employ the theorem I proved at the end of my last post, that the length of a list of spanning vectors is never shorter than any linearly independent list.</p>
<blockquote>
<p><strong>Theorem.</strong> All bases of a finite-dimensional vector space have the same length.</p>
<p><strong>Proof.</strong> Let $V$ denote a finite dimensional vector space, and suppose $(e_1,\ldots,e_n)$ and $(f_1,\ldots,f_m)$ are both bases for $V$. Since $(e_1,\ldots,e_n)$ is linearly independent and $(f_1,\ldots,f_m)$ spans $V$, we have that $n\le m$. Similarly, since $(f_1,\ldots,f_m)$ is linearly independent and $(e_1,\ldots,e_n)$ spans $V$, we have that $m\le n$. It follows that $m=n$, as desired.</p>
</blockquote>
<p>What we have really shown is that the length of a basis for a finite-dimensional vector space is an invariant of that space! And it's a particularly special invariant:</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>dimension</strong> of a finite-dimensional vector space is the length of any basis for that space.</p>
<p>If the dimension of a vector space $V$ is $n$, we write</p>
<p>$$\dim V=n.$$</p>
</blockquote>
<p>As a special case, recall that we defined $\span()=\set{0}$. That means that $\dim\set{0}=0$.</p>
<p>We've already shown that dimension is well-defined, since all bases for a vector space have the same length. But does this definition coincide with what we expect? For instance, we would certainly expect that $\dim\R^n=n$ for any positive integer $n$. And it turns out that this always the case, as we can see by the following definition.</p>
<blockquote>
<p><strong>Definition.</strong> Given a positive integer $n$, the <strong>standard basis</strong> for $\F^n$ is the list containing the vectors</p>
<p>$$\begin{eqnarray}<br>
e_1 &amp;=&amp; (1,0,\ldots,0), \\<br>
e_2 &amp;=&amp; (0,1,\ldots,0), \\<br>
&amp;\;\vdots&amp; \\<br>
e_n &amp;=&amp; (0,0,\ldots,1).<br>
\end{eqnarray}$$</p>
<p>That is, $e_i$ is the vector whose $i$th component is $1$ and every other component is zero.</p>
</blockquote>
<p>It should be fairly obvious that for any $n$, the standard basis for $\F^n$ is, in fact, a basis. It is certainly linearly independent and it is easy to write any vector in $\F^n$ as a linear combination of basis vectors, implying it also spans $\F^n$. Since there are $n$ basis vectors in the standard basis, it follows that $\dim\F^n=n$, just like we would expect.</p>
<h3 id="someusefultheoremsanamesomeusefultheorems">Some Useful Theorems<a name="some-useful-theorems"></a></h3>
<p>This first result tells use how to calculate the dimension of a sum of two subspaces. Recall that we previously showed that the intersection of two subspaces is itself a subspace.</p>
<blockquote>
<p><strong>Theorem.</strong> If $V_1$ and $V_2$ are subspaces of a finite-dimensional vector space, then</p>
<p>$$\dim(V_1+V_2) = \dim V_1 + \dim V_2 - \dim(V_1\cap V_2).$$</p>
<p><strong>Proof.</strong> Since $V_1\cap V_2$ is a subspace of a finite-dimensional vector space, it is also finite-dimensional. Thus it has a basis, which we will denote $(e_1,\ldots,e_n)$.</p>
<p>This basis for $V_1\cap V_2$ is linearly independent, and thus we may extend it to a basis for $V_1$ by adding $m_1 = \dim V_1 - n$ new linearly independent vectors. That is, there is some basis $(e_1,\ldots,e_n,f_1,\ldots,f_{m_1})$ for $V_1$, and certainly $\dim V_1 = m_1+n$.</p>
<p>We may similarly extend $(e_1,\ldots,e_n)$ to a basis for $V_2$ by adding $m_2 = \dim V_2 - n$ new linearly independent vectors. That is, there is some basis $(e_1,\ldots,e_n,g_1,\ldots,g_{m_2})$ for $V_2$, and certainly $\dim V_2 = m_2+n$.</p>
<p>We argue that $(e_1,\ldots,e_n,f_1,\ldots,f_{m_1},g_1,\ldots,g_{m_2})$ is a basis for $V_1+V_2$. Certainly</p>
<p>$$V_1,V_2\subseteq\span(e_1,\ldots,e_n,f_1,\ldots,f_{m_1},g_1,\ldots,g_{m_2})$$</p>
<p>and thus</p>
<p>$$V_1+V_2=\span(e_1,\ldots,e_n,f_1,\ldots,f_{m_1},g_1,\ldots,g_{m_2}).$$</p>
<p>To see that this list is linearly independent, suppose that</p>
<p>$$\sum_{i=1}^n a_i e_i + \sum_{i=1}^{m_1} b_i f_i + \sum_{i=1}^{m_2} c_i g_i = 0 \tag{4}$$</p>
<p>for some scalars $\set{a_i}_{i=1}^n$, $\set{b_i}_{i=1}^{m_1}$ and $\set{c_i}_{i=1}^{m_2}$. We can rewrite $(4)$ as</p>
<p>$$\sum_{i=1}^{m_2} c_i g_i = -\sum_{i=1}^n a_i e_i - \sum_{i=1}^{m_1} b_i f_i,$$</p>
<p>from which we see that $\sum_{i=1}^{m_2} c_i g_i\in V_1\cap V_2$. Since $(e_1,\ldots,e_n)$ is a basis for $V_1\cap V_2$, it follows that we may write it uniquely as a linear combination of basis vectors,</p>
<p>$$\sum_{i=1}^{m_2} c_i g_i = \sum_{i=1}^n d_i e_i, \tag{5}$$</p>
<p>for some scalars ${d_i}_{i=1}^n$. But $(e_1,\ldots,e_n,g_1,\ldots,g_{m_2})$ is linearly independent, so it follows from equation $(5)$ that $c_i=0$ for all $i$. This means we may rewrite equation $(4)$ as</p>
<p>$$\sum_{i=1}^n a_i e_i + \sum_{i=1}^{m_1} b_i f_i = 0.$$</p>
<p>Since $(e_1,\ldots,e_n,f_1,\ldots,f_{m_1})$ is linearly independent, it follows that $a_i=0$ for all $1\le i\le n$ and $b_i=0$ for all $1\le i\le m_1$. Since all coefficients in equation $(4)$ have been shown to be zero, it follows that $(e_1,\ldots,e_n,f_1,\ldots,f_{m_1},g_1,\ldots,g_{m_2})$ is a basis for $V_1+V_2$.</p>
<p>It follows then that</p>
<p>$$\begin{align}<br>
\dim(V_1+V_2) &amp;= n + m_1 + m_2 \\<br>
&amp;= (n+m_1) + (n+m_2) - n \\<br>
&amp;= \dim V_1 + \dim V_2 - \dim(V_1\cap V_2),<br>
\end{align}$$</p>
<p>completing the proof.</p>
</blockquote>
<p>If you think that proof was gross, try doing it for sums of three or more subspaces. Or don't, because as far as I know <a href="https://mathoverflow.net/questions/23478/examples-of-common-false-beliefs-in-mathematics/23501#23501">there is no general formula for such a thing</a>.</p>
<p>The next result ties together our work from last post regarding direct sums, and follows immediately from the theorem we just proved. Recall that if a sum of two subspaces is direct, their intersection is trivial.</p>
<blockquote>
<p><strong>Theorem.</strong> If $U_1,\ldots,U_n$ are subspaces of a finite-dimensional vector space $V$ for which</p>
<p>$$V=\bigoplus_{i=1}^n U_i,$$</p>
<p>then</p>
<p>$$\dim V = \sum_{i=1}^n \dim U_i.$$</p>
<p><strong>Proof.</strong> We first rewrite the sum as</p>
<p>$$V = (\cdots((U_1\oplus U_2) \oplus U_3) \oplus \cdots \oplus U_n).$$</p>
<p>We work outward, first considering the subspace $W_2 = U_1\oplus U_2$ and then the subspace $W_3 = W_2 \oplus U_3$, then the subspace $W_4 = W_3 \oplus U_4$, etc. This takes $n-1$ iterations, but eventually we reach $W_n = V$.</p>
<p>But $U_1\cap U_2=\set{0}$ since the sum is direct, and so by the previous theorem we see that</p>
<p>$$\begin{align}<br>
\dim W_1 &amp;= \dim (U_1 + U_2) \\<br>
&amp;= \dim U_1 + \dim U_2 - \dim (U_1\cap U_2) \\<br>
&amp;= \dim U_1 + \dim U_2 - \dim\set{0} \\<br>
&amp;= \dim U_1 + \dim U_2 - 0 \\<br>
&amp;= \dim U_1 + \dim U_2.<br>
\end{align}$$</p>
<p>And in general, since $W_j = W_{j-1} \oplus U_j$,</p>
<p>$$\begin{align}<br>
\dim W_j &amp;= \dim (W_{j-1} + U_j) \\[1em]<br>
&amp;= \dim W_{j-1} + \dim U_j \\<br>
&amp;= \sum_{i=1}^{j-1} \dim U_i + \dim U_j.<br>
\end{align}$$</p>
<p>Setting $j=n$, the result follows.</p>
</blockquote>
<p>Next time I will introduce linear maps, which are (besides vector spaces themselves) the primary objects of study in linear algebra.</p>
]]></content:encoded></item><item><title><![CDATA[Vector Spaces (2) - Direct Sums, Span and Linear Independence]]></title><description><![CDATA[I left off last time with an example of a sum of subspaces with a rather important property. Namely, every vector in the sum had a unique representation as a sum of vectors in the component subspaces.]]></description><link>https://algebrology.github.io/vector-spaces-2/</link><guid isPermaLink="false">5c8d3e3bbdf47d003ee07244</guid><category><![CDATA[linear algebra]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Sat, 16 Mar 2019 18:24:28 GMT</pubDate><content:encoded><![CDATA[<ol>
<li><a href="#direct-sums">Direct Sums</a></li>
<li><a href="#span-and-linear-combinations">Span and Linear Combinations</a></li>
<li><a href="#linear-independence">Linear Independence</a></li>
</ol>
<hr>
<h3 id="directsumsanamedirectsums">Direct Sums<a name="direct-sums"></a></h3>
<p>I left off <a href="https://algebrology.github.io/vector-spaces-1">last time</a> with an example of a sum of subspaces with a rather important property. Namely, every vector in the sum had a unique representation as a sum of vectors in the component subspaces. Not all sums have this property, but it is so important that it deserves special consideration.</p>
<blockquote>
<p><strong>Definition.</strong> Let $U$ and $W$ be subspaces of a vector space $V$ for which $V=U+W$. This sum is a <strong>direct sum</strong> if for every $\v\in V$, the representation</p>
<p>$$\v=\u+\w,$$</p>
<p>where $\u\in U$ and $\w\in W$, is unique. If a sum is direct, it is expressed symbolically as</p>
<p>$$ V=U\oplus W.$$</p>
<p>Similarly, if $U_1,U_2,\ldots,U_n$ are subspaces of $V$ for which</p>
<p>$$\begin{align}<br>
V &amp;= \sum_{i=1}^n U_i \\<br>
&amp;= U_1 + U_2 + \cdots + U_n,<br>
\end{align}$$</p>
<p>then this sum is direct if for every $\v\in V$, the representation</p>
<p>$$\begin{align}<br>
\v &amp;= \sum_{i=1}^n \u_i \\<br>
&amp;= \u_1 + \u_2 + \cdots + \u_n,<br>
\end{align}$$</p>
<p>where $\u_i\in U_i$ for every $i$, is unique. Such a direct sum is expressed symbolically as</p>
<p>$$\begin{align}<br>
V &amp;= \bigoplus_{i=1}^n U_i \\<br>
&amp;= U_1 \oplus U_2 \oplus \cdots \oplus U_n.<br>
\end{align}$$</p>
</blockquote>
<p>Notice that a direct sum of linear subspaces is not really its own thing. It is a normal sum which happens to also have the property of being direct. You do not start with two subspaces and take their direct sum. You take the sum of subspaces, and that sum may happen to be direct.</p>
<p>We have already seen an example of a sum which is direct. You may find it more enlightening if we exhibit an example of a sum which is <em>not</em> direct.</p>
<blockquote>
<p><strong>Example.</strong> Consider the subspaces</p>
<p>$$\begin{align}<br>
U &amp;= \set{(x,y,0)\in\R^3\mid x,y\in\R}, \\<br>
W &amp;= \set{(0,y,z)\in\R^3\mid y,x\in\R}<br>
\end{align}$$</p>
<p>of the vector space $\R^3$. Clearly $\R^3=U+W$. The vector $(1,1,1)\in\R^3$ can be expressed as</p>
<p>$$(1,1,1)=(1,1,0)+(0,0,1),$$</p>
<p>where $(1,1,0)\in U$ and $(0,0,1)\in W$. However, it can also be expressed as</p>
<p>$$(1,1,1)=(1,2,0)+(0,-1,1),$$</p>
<p>where $(1,2,0)\in U$ and $(0,-1,1)\in W$. In fact, there are infinitely many ways to decompose this into such a sum — the numbers in the second components just need to add up to $1$. Since the representation of $(1,1,1)$ as a sum of a vector in $U$ and a vector in $W$ is not unique, it follows that the sum $R^3=U+W$ is not direct.</p>
</blockquote>
<p>It is sometimes difficult to determine whether a sum is direct using only the definition. This would require some sort of insight as to why it must be direct, or the demonstration of a counterexample. This next proposition gives us a somewhat simplified method for checking whether a sum of subspaces is direct.</p>
<blockquote>
<p><strong>Proposition.</strong> Suppose $U_1,U_2,\ldots,U_n$ are subspaces of a vector space $V$ for which</p>
<p>$$V=\sum_{i=1}^n U_i.$$</p>
<p>Then</p>
<p>$$V=\bigoplus_{i=1}^n U_i$$</p>
<p>if and only if the choices of $\u_i\in U_i$ which satisfy</p>
<p>$$\0=\u_1+\u_2+\cdots+\u_n$$</p>
<p>are the vectors $\u_1=\u_2=\cdots=\u_n=\0$.</p>
<p><strong>Proof.</strong> First, suppose that</p>
<p>$$V=\bigoplus_{i=1}^n U_i,$$</p>
<p>and for each $i$ choose $\u_i\in U_i$ so that</p>
<p>$$\0=\u_1+\u_2+\cdots+\u_n.$$</p>
<p>Because the above sum is direct, this representation of $\0$ must be unique. It must be the case then that $\u_1=\u_2=\cdots=\u_n=\0$.</p>
<p>Suppose next that the only way to satisfy</p>
<p>$$\0=\u_1+\u_2+\cdots+\u_n$$</p>
<p>where $\u_i\in U_i$ for each $i$, is by taking $\u_1=\u_2=\cdots=\u_n=\0$. For any $\v\in V$, we may write</p>
<p>$$\v=\v_1+\v_2+\cdots+\v_n, \tag{1}$$</p>
<p>where $\v_i\in U_i$ for each $i$. Suppose that we can also write</p>
<p>$$\v=\w_1+\w_2+\cdots+\w_n, \tag{2}$$</p>
<p>where $\w_i\in U_i$ for each $i$ Subtracting $(2)$ from $(1)$ yields</p>
<p>$$\0=(\v_1-\w_1)+(\v_2-\w_2)+\cdots+(\v_n-\w_n).$$</p>
<p>For each $i$, we have that $\v_i-\w_i\in U_i$, since each $U_i$ is a subspace and thus closed under vector addition. Thus, since we have assumed $\0$ to have a unique expression of this form, it follows that $\v_i-\w_i=\0$ for all $i$. That is, $\v_i=\w_i$ for all $i$. We have shown that the expression for $\v$ is unique, and thus</p>
<p>$$V=\bigoplus_{i=1}^n U_i.$$</p>
</blockquote>
<p>This result tells us that all we need to do in order to check that a sum of subspaces is direct is to make sure that there is only one way to express the zero vector as a sum of vectors from each subspace. This is certainly simpler than checking that every vector has a unique representation of this form. However, in the case of two subspaces, we can do even better.</p>
<blockquote>
<p><strong>Corollary.</strong> Suppose $U$ and $W$ are subspaces of a vector space $V$ and that $V=U+W$. Then $V=U\oplus W$ if and only if $U\cap W=\set{\0}$.</p>
<p><strong>Proof.</strong> Suppose first that $V=U\oplus W$ and choose $\v\in U\cap W$. Then $\v\in U$ and $\v\in W$, so we also have that $-\v\in W$ because $W$ is a subspace of $V$ and so it is closed under scalar multiplication (in this case by $-1$). We can thus write</p>
<p>$$\0=\v+(-\v),$$</p>
<p>and this expression is unique by the above proposition. It follows that $\v=-\v-\0$, and thus $U\cap W =\set{\0}$.</p>
<p>Next, suppose that $U\cap W=\set{\0}$ and let</p>
<p>$$ \0=\u+\w,$$</p>
<p>where $\u\in U$ and $\w\in W$. Then $\u=-\w$ and so $-\w\in U$, which implies that $\w\in U$. Since $\w\in U$ and $w\in W$, it follows that $\w\in U\cap W$. By hypothesis, this means that $\w=\0$ and thus $\u=\0$. We have shown that the only way to decompose $\0$ in the form above is as the sum of zero vectors. Thus, it follows from the above proposition that $V=U\oplus W$, completing the proof.</p>
</blockquote>
<p>This result tells us that the sum of two subspaces is direct whenever their intersection is trivial. Unfortunately, this result does not generalize to sums of three or more subspaces. In fact, in the case of three or more subspaces, even pairwise intersections being trivial is not enough to guarantee that a sum is direct.</p>
<h3 id="spanandlinearcombinationsanamespanandlinearcombinations">Span and Linear Combinations<a name="span-and-linear-combinations"></a></h3>
<p>Given a vector space $V$ and a vector $\v\in V$, what is the smallest subspace of $V$ containing $\v$? It's not a trick question — the answer is actually somewhat obvious. It's the set $U=\set{a\v\in V\mid a\in\F}$ of all scalar multiples of $\v$. It is certainly clear that $U$ satisfies the criteria of a subspace. Furthermore, the only proper subspace of $U$ is $\set{\0}$, so it is certainly the smallest subspace of $V$ which contains $\v$. This is a very important idea, and it leads us to our next definition.</p>
<blockquote>
<p><strong>Definition.</strong> If $V$ is a vector space with $\v\in V$, then the <strong>span</strong> of $\v$ is the subspace</p>
<p>$$\span\v = \set{a\v\in V\mid a\in\F}.$$</p>
<p>Furthermore, if $(\v_1,\ldots,\v_n)$ is a list of vectors in $V$, then the span of $(v_1,\ldots,v_n)$ is the subspace</p>
<p>$$\begin{align}<br>
\span(\v_1,\ldots,\v_n) &amp;= \sum_{i=1}^n \span\v_i \\<br>
&amp;= \sum_{i=1}^n \set{a\v_i\in V\mid a\in\F} \\<br>
&amp;= \bset{\sum_{i=1}^n a\v_i\in V\mid a\in\F}.<br>
\end{align}$$</p>
<p>For consistency, we define the span of the empty list to be</p>
<p>$$\span()=\set{\0}.$$</p>
<p>If $V=\span(\v_1,\ldots,\v_n)$, we say that $V$ is <strong>spanned</strong> by the list $(\v_1,\ldots,\v_n)$, or that this list <strong>spans</strong> $V$.</p>
</blockquote>
<p>It is important to distinguish between <em>lists</em> and <em>sets</em>. In a list, the order of the entries matters and entries can be repeated more than once.</p>
<p>In the above definition, we first defined the span of one vector to be the smallest subspace containing that vector. We then defined the span of a list of vectors to be the sum of the spans of each vector in the list.</p>
<p>Equivalently, the span of a list of vectors is a subspace containing every possible sum of scalar multiples of those vectors. In order to avoid having to repeat the phrase &quot;sum of scalar multiples&quot; over and ove, we define a new term which means the same thing.</p>
<blockquote>
<p><strong>Definition.</strong> Let $V$ be a vector space with $\v_1,\v_2,\ldots,\v_n\in V$. A <strong>linear combination</strong> of the vectors $\v_1,\v_2,\ldots,\v_n$ is an expression of the form</p>
<p>$$\sum_{i=1}^n a_i\v_i = a_1\v_1+a_2\v_2+\cdots+a_n\v_n,$$</p>
<p>where $a_1,a_2,\ldots,a_n\in\F$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> Consider the vectors $(1,2)$, $(3,4)$ and $(0,-1)$ in the vector space $\R^2$. Some linear combinations of these vectors include</p>
<p>$$\begin{align}<br>
(9,9)&amp;=3(1,2)+2(3,4)+5(0,-1), \\<br>
(-1,0)&amp;=2(1,2)-\phantom{1}(3,4)+0(0,-1), \\<br>
(1,2)&amp;=\phantom{1}(1,2)+0(3,4)+0(0,-1).<br>
\end{align}$$</p>
</blockquote>
<p>With this definition, we can now think of the span of a list of vectors as the set of all linear combinations of those vectors. In addition, we can now see that a vector space $V$ is spanned by a list of vectors if every vector in $V$ can be expressed as a linear combination of vectors in that list.</p>
<p>Although we are not quite ready to define the <em>dimension</em> of a vector space, we now have the necessary machinery to distinguish between finite-dimensional and infinite-dimensional vector spaces.</p>
<blockquote>
<p><strong>Definition.</strong> A vector space $V$ is <strong>finite-dimensional</strong> if it is spanned by a finite list of vectors in $V$.</p>
<p>If no finite list of vectors spans $V$, then $V$ is <strong>infinite-dimensional</strong>.</p>
</blockquote>
<p>We will be primarily concerned with finite-dimensional vector spaces. These have much nicer properties and are considerably easier to work with and conceptualize. Infinite-dimensional vector spaces can be extremely poorly behaved and are dealt with in a branch of mathematics called functional analysis.</p>
<blockquote>
<p><strong>Example.</strong> For any positive integer $n$, the vector space $\R^n$ is finite-dimensional. This is because</p>
<p>$$\begin{align}<br>
\R &amp;= \span(1), \\<br>
\R^2 &amp;= \span\big((1,0), (0,1)\big), \\<br>
\R^3 &amp;= \span\big((1,0,0), (0,1,0), (0,0,1)\big), \\<br>
\R^4 &amp;= \span\big((1,0,0,0), (0,1,0,0), (0,0,1,0), (0,0,0,1)\big), \\<br>
&amp;\;\; \vdots<br>
\end{align}$$</p>
<p>Of course, there are other lists of vectors that span each $\R^n$, but to show that a vector space is finite-dimensional, we need only demonstrate that one such list exists.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> We have already been introduced to an infinite-dimensional vector space, namely $\P(\F)$. This is the set of polynomials with coefficients in some field $\F$.</p>
<p>To show that it is infinite-dimensional, consider any finite list of polynomials in $\P(\F)$ and let $n$ be the highest degree of the polynomials in the list. Then the polynomial $x^{n+1}$ is not in the span of this list, so it follows that no finite list of polynomials spans $\P(\F)$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> On the other hand, the vector space $\P_n(\F)$ of polynomials whose degree is at most $n$ is finite-dimensional.</p>
</blockquote>
<p>It is tempting to try to define the dimension of a finite-dimensional vector space using the machinery we have already developed. We could, perhaps, attempt to define the dimension of $V$ as the length of a list of vectors which spans $V$. Unfortunately, this is not well-defined because a vector space can have many spanning lists of different lengths. For example,</p>
<p>$$\begin{align}<br>
\R^2 &amp;= \span\big((0,1),(1,0)\big) \\<br>
&amp;= \span\big((0,1),(1,0),(1,0)\big) \\<br>
&amp;= \span\big((1,2),(2,3),(3,4),(4,5)\big),<br>
\end{align}$$</p>
<p>because any vector in $\R^2$ can be expressed as a linear combination of vectors in each list. In fact, we can always take a list which spans a vector space $V$ and add to it any vector in $V$, resulting in a new list which also spans $V$. This means that the length of a spanning list is certainly not unique, and so we cannot define dimension in this way.</p>
<h3 id="linearindependenceanamelinearindependence">Linear Independence<a name="linear-independence"></a></h3>
<p>Intuition tells us that we are on the correct track, though. It should be somewhat obvious that the vector space $\R^2$ cannot possibly be spanned by any list containing <em>less than</em> two vectors. If this is truly the case, it should make sense to define the dimension of $\R^2$ to be $two$. We will therefore work toward figuring out the minimum length of a spanning list of vectors. The important concept of linear independence, which we will now define, will help us with this.</p>
<blockquote>
<p><strong>Definition.</strong> A list $(\v_1,\v_2,\ldots,\v_n)$ of vectors in a vector space $V$ is <strong>linearly independent</strong> if the only choices of $a_i\in F$ which satisfy</p>
<p>$$\sum_{i=1}^n a_i\v_i = \0 \tag{3}$$</p>
<p>are the scalars $a_1=a_2=\cdots=a_n=0$.</p>
<p>If there exist $a_i\in\F$, not all zero, for which $(3)$ holds, then the list $(\v_1,\v_2,\ldots,\v_n)$ is <strong>linearly dependent</strong>.</p>
</blockquote>
<p>In every vector space, any list containing the zero vector must be linearly dependent. This is because we can take the coefficient of $\0$ to be any scalar.</p>
<p>Let's look at some more interesting examples.</p>
<blockquote>
<p><strong>Example.</strong> In $\R^2$, the list</p>
<p>$$\big((0,1),(1,0)\big)$$</p>
<p>is linearly independent because</p>
<p>$$a_1(0,1)+a_2(1,0)=(0,0)$$</p>
<p>only if $a_1=a_2=0$. No matter how hard we try, we cannot come up with any other linear combination of these vectors which yields the zero vector.</p>
<p>However, if we add $(1,0)$ to the list, the resulting list</p>
<p>$\big((0,1),(1,0),(1,0)\big)$$</p>
<p>is linearly dependent because</p>
<p>$$0(0,1)+(1,0)-(1,0)=(0,0).$$</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> As a less trivial example, the list</p>
<p>$$\big((1,2),(2,3),(3,4),(4,5)\big)$$</p>
<p>is also linearly dependent because</p>
<p>$$(1,1)+(2,3)-5(3,4)+3(4,5)=(0,0).$$</p>
</blockquote>
<p>Up until now, we have sort of been weaving a twisted web of interconnected ideas. Our next theorem relates them all and gives an alternate characterization of linear independence in terms of spans and direct sums.</p>
<blockquote>
<p><strong>Theorem.</strong> A list $(\v_1,\v_2,\ldots,\v_n)$ of nonzero vectors in a vector space $V$ is linearly independent if and only if</p>
<p>$$\span(\v_1,\v_2,\ldots,\v_n) = \bigoplus_{i=1}^n \span\v_i.$$</p>
<p><strong>Proof.</strong> First, suppose that the list $(\v_1,\v_2,\ldots,\v_n)$ is linearly independent. Then by the definition of span, we have that</p>
<p>$$\span(\v_1,\v_2,\ldots,\v_n) = \sum_{i=1}^n \span\v_i.$$</p>
<p>From our earlier proposition, we need to show that only choices of $\u_i\in\span\v_i$ for which</p>
<p>$$\0=\u_1+\u_2+\cdots+\u_n$$</p>
<p>are the vectors</p>
<p>$$\u_1=\u_2=\cdots=\u_n=\0.$$</p>
<p>Since $u_i\in\span\v_i$ for each $i$, we have that $\u_i=a_i\v_i$ for some $a_i\in\F$. Since $(\v_1,\v_2,\ldots,\v_n)$ is linearly independent, we have that</p>
<p>$$\begin{align}<br>
\0 &amp;= a_1\v_1+a_2\v_2+\cdots+a_n\v_n \\<br>
&amp;= \u_1+\u_2+\cdots+\u_n<br>
\end{align}$$</p>
<p>only if $a_1=a_2=\cdots=a_n=0$, which implies that $\u_i=\0$ for all $i$. If follows then that the desired sum is direct.</p>
<p>Suppose next that</p>
<p>$$\span(\v_1,\v_2,\ldots,\v_n) = \bigoplus_{i=1}^n \span\v_i.$$</p>
<p>Again from the above proposition, we know that the only choices of $\u_i\in\span\v_i$ which satisfy</p>
<p>$$\u_1=\u_2=\cdots=\u_n=\0.$$</p>
<p>Since each $\u_i\in\span\v_i$, it follows that for each $i$, there is some $a_i\in\F$ for which $\u_i=a_i\v_i$. Since each $\v_i\ne 0$, it must be that each $a_i=0$. Thus, the list $(\v_1,\v_2,\ldots,\v_n)$ is linearly independent.</p>
</blockquote>
<p>This theorem establishes a somewhat different way of thinking about linear independence, and since the two are equivalent we are free to use either as a definition of linear independence.</p>
<p>Linearly depdendent lists have an interesting, yet fairly undesirable, property which is explained in the next theorem. This result is commonly called the linear dependence lemma, and it will be crucial for many future arguments. It states than any linearly dependent list contains a vector which is in the span of the previous vectors in the list. Furthermore, it asserts that removing this vector from the list does not affect the list's span.</p>
<blockquote>
<p><strong>Linear Dependence Lemma/Steinitz Exchange Lemma.</strong> Suppose $(\v_1,\v_2,\ldots,\v_n)$ is a linearly dependent list of vectors in a vector space $V$ for which $\v_1\ne\0$. Then there exists a natural number $i\ge 2$ for which</p>
<p>$(a) \;\;$ $\v_i\in\span(\v_1,\v_2,\ldots,\v_n)$,<br>
$(b) \;\;$ removing $\v_i$ from $(\v_1,\v_2,\ldots,\v_n)$ results in a list with the same span.</p>
<p><strong>Proof.</strong> Since $(\v_1,\v_2,\ldots,\v_n)$ is linearly dependent, there exist $a_1,a_2,\ldots,a_n\in\F$, not all zero, for which</p>
<p>$$a_1\v_1+a_2\v_2+\cdots+a_n\v_n = 0.$$</p>
<p>We have assumed that $\v_1\ne\0$, so it is possible that $a_1=0$, but not all of $a_2,a_3,\ldots,a_n$ can equal zero. Let $i$ be the largest index for which $a_i\ne 0$. Then</p>
<p>$$a_1\v_1+a_2\v_2+\cdots+a_i\v_i = 0,$$</p>
<p>so we may write</p>
<p>$$\v_i = -\frac{a_1}{a_i}\v_1-\frac{a_2}{a_i}\v_2 - \cdots - \frac{a_{i-1}}{a_i}\v_{i-1}.$$</p>
<p>Since we have expressed $\v_i$ as a linear combination of $\v_1,\v_2,\ldots,\v_{i-1}$, it follows that $\v_i\in\span(\v_1,\v_2,\ldots,\v_n)$. This completes the proof of $(a)$.</p>
<p>Next, choose any $\u\in\span(\v_1,\v_2,\ldots,\v_n)$. By definition, there exist $a_1,a_2,\ldots,a_n\in\F$ for which</p>
<p>$$\u=a_1\v_1+a_2\v_2+\cdots+a_n\v_n.$$</p>
<p>Using the result of $(a)$, we can replace $v_i$ in the above equation with a linear combination of the vectors $\v_1,\v_2,\ldots,\v_{i-1}$. Thus, $\u$ is also in the span of the list which results from removing $\v_i$.</p>
</blockquote>
<p>Now we will establish that linearly independent lists possess the property I mentioned earlier. Namely, we will show that linearly independent lists are the shortest possible spanning lists.</p>
<blockquote>
<p><strong>Theorem.</strong> In any finite-dimensional vector space $V$, the length of a list of vectors which spans $V$ is never shorter than any linearly independent list.</p>
<p><strong>Proof.</strong> Suppose $(\u_1,\u_2,\ldots,\u_m)$ is a list of vectors which spans $V$, and that $(\v_1,\v_2,\ldots,\v_n)$ is a linearly independent list. This implies $\v_i\ne\0$ for every $i$. We will show that $m\ge n$ through the following algorithmic construction.</p>
<div style="margin-left:1.5em">
<p><strong>Step 1.</strong><br>
Because $(\u_1,\u_2,\ldots,\u_m)$ spans $V$, adding any vector to this list must result in a list which is linearly dependent. Namely, the list $L_{1,1}=(\v_1,\u_1,\u_2,\ldots,\u_m)$ is linearly dependent. It follows from the linear dependence lemma that there exists a positive integer $i\le m$ for which the removal of $\u_i$ from $L_{1,1}$ results in a new list $L_{1,2}$ which still spans $V$. Notice that $L_{1,2}$ is a list of length $m$. To simplify matters, we replace the index of every $\u_j$ for which $j&gt;i$ with $j-1$, so that we can write</p>
<p>$$L_{1,2}=(\v_1,\u_1,\u_2,\ldots,\u_{m-1}).$$</p>
<p><strong>Step k.</strong><br>
Because the list $L_{k-1,2}$ from the previous step spans $V$, adding any vector to this list must result in a list which is linearly dependent. Namely, the list $L_{k,1}$ formed by adding $\v_k$ to the beginning of $L_{k-1,1}$ is linearly dependent. It follows from the linear dependence lemma that we can remove some vector from $L_{k,1}$ without affecting its span. Since</p>
<p>$$L_{k,1}=(\v_k,\v_{k-1},\ldots,\v_1,\u_1,\u_2,\ldots,\u_{m-k+1})$$</p>
<p>and $(\v_k,\v_{k-1},\ldots,\v_1)$ is linearly independent, there must exist a positive integer $i\le m-k+1$ for which the removal of $\u_i$ from $L_{k,1}$ results in a new list $L_{k,2}$ which still spans $V$. Notice that $L_{k,2}$ is a list of length $m$. Next, we replace the index of every $\u_j$ for which $j&gt;i$ with $j-1$, so that we can write</p>
<p>$$L_{k,2}=(\v_k,\v_{k-1},\ldots,\v_1,\u_1,\u_2,\ldots,\u_{m-k}).$$</p>
</div>
<p>At the end of step $n$, we have used up every $\v_i$ and so the algorithm terminates. It cannot terminate before this because each list $K_{k,1}$ is linearly dependent, and so $m-n\ge 0$. We conclude that $m\ge n$, as desired.</p>
</blockquote>
<p>Algorithmic proofs like the above are common in linear algebra. Linear algebra has applications in almost every field, partially because it lends itself so nicely to computation.</p>
<p>I will end here for now, and next time I will talk about bases for vector spaces.</p>
]]></content:encoded></item><item><title><![CDATA[Vector Spaces (1) - Subspaces and Sums]]></title><description><![CDATA[A vector space is a special kind of set containing elements called vectors, which can be added together and scaled in all the ways one would generally expect.]]></description><link>https://algebrology.github.io/vector-spaces-1/</link><guid isPermaLink="false">5c8c6f0bfdce06003e4aeff0</guid><category><![CDATA[linear algebra]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Sat, 16 Mar 2019 04:04:40 GMT</pubDate><content:encoded><![CDATA[<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#vector-spaces">Vector Spaces</a></li>
<li><a href="#basic-properties">Basic Properties</a></li>
<li><a href="#examples">Examples</a></li>
<li><a href="#subspaces-and-sums">Subspaces and Sums</a></li>
</ol>
<hr>
<h3 id="introductionanameintroduction">Introduction<a name="introduction"></a></h3>
<p>It's almost ridiculous that I would expose you to free abelian groups before talking about vector spaces and linear algebra. Vector spaces and free abelian groups have a lot in common, but vector spaces are more familiar, more ubiquitous and easier to compute with.</p>
<p>At their core, vector spaces are very simple and their definition will closely mimic that of groups and topological spaces. Recall that a group is a set with a binary operation, an identity and inverses for all its elements. A topological space is a set and a collection of &quot;open sets&quot; which include the set itself, the empty set, finite intersections and arbitrary unions of open sets. Vector spaces are defined in a similar manner.</p>
<p>A vector space is a special kind of set containing elements called vectors, which can be added together and scaled in all the ways one would generally expect.</p>
<p>You have likely encountered the idea of a vector before as some sort of arrow, anchored to the origin in euclidean space with some well-defined magnitude and direction.</p>
<p><img src="https://algebrology.github.io/content/images/2019/03/vector-arrow.svg" alt="vector-arrow"></p>
<p>This is the sort of vector encountered in introductory physics classes. However, such arrows are not the only mathematical objects that can be added and scaled, so it would be silly to restrict our attention only to them. We will make a more abstract and inclusive definition of vector spaces, which are the main objects of study in linear algebra.</p>
<p>We would like our definition to include some way to scale vectors so that we can expand of shrink them in magnitude while preserving their direction. Since in general there is no concept of vector multiplication, we will need to bring in additional elements by which we are allowed to multiply our vectors to achieve a scaling effect. These <em>scalars</em> will be the elements of a field, which we have encountered before in my posts on constructing the rational numbers, but I will give the definition again because it is so important.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>field</strong> is a set $\F$ of elements called <strong>scalars</strong>, together with two binary operations:</p>
<div style="margin-left:1.5em">
<p><strong>Addition</strong><br>
which assigns to any pair of scalars $a,b\in\F$ the scalar $a+b\in\F$,</p>
<p><strong>Multiplication</strong><br>
which assigns to any pair of scalars $a,b\in\F$ the scalar $ab\in\F$.</p>
</div>
<p>Any field and its operations must satisfy the following properties:</p>
<div style="margin-left:1.5em"> 
<p><strong>Additive Identity</strong><br>
There exists $0\in\F$ such that $0+a=a$ for every $a\in\F$.</p>
<p><strong>Additive Inverses</strong><br>
For every $a\in\F$, there exists $b\in\F$ for which $a+b=0$.</p>
<p><strong>Commutative Property of Addition</strong><br>
For all $a,b\in\F$, we have that $a+b=b+a$.</p>
<p><strong>Associative Property of Addition</strong><br>
For all $a,b,c\in\F$, we have that $(a+b)+c=a+(b+c)$.</p>
<p><strong>Multiplicative Identity</strong><br>
There exists $1\in\F$, with $1\ne 0$, such that $1a=a$ for every $a\in\F$.</p>
<p><strong>Multiplicative Inverses</strong><br>
For every $a\in\F$ with $a\ne 0$, there exists $b\in\F$ for which $ab=1$.</p>
<p><strong>Commutative Property of Multiplication</strong><br>
For all $a,b\in\F$, we have that $ab=ba$.</p>
<p><strong>Associative Property of Multiplication</strong><br>
For all $a,b,c\in\F$, we have that $(ab)c=a(bc)$.</p>
<p><strong>Distributive Property</strong><br>
For all $a,b,c\in\F$, we have that $a(b+c)=ab+ac$.</p>
</div>
</blockquote>
<p>It is not too difficult to verify that the set $\R$ of real numbers is a field when considered with the usual addition and multiplication. Similarly, the sets $\C$ of complex numbers and $\Q$ of rational numbers form fields when equipped with their usual arithmetic.</p>
<p>There are other examples of fields besides these familiar ones. For example, the set $\Z_3$ of integers modulo $3$ is a field when considered with addition and multiplication modulo $3$. The tables below describe addition and multiplication in $\Z_3$, so you can check for yourself that the field axioms hold.</p>
<p><img src="https://algebrology.github.io/content/images/2019/03/Z_3-tables.svg" alt="Z_3-tables"></p>
<p>In fact, there are many more examples of finite fields. For any prime number $p$, the set $\Z_p$ of integers modulo $p$ forms a field. However, for our purposes we will usually only be interested in the fields $\R$ and $\C$.</p>
<h3 id="vectorspacesanamevectorspaces">Vector Spaces<a name="vector-spaces"></a></h3>
<p>With all of this in mind, we can move forward with defining the concept of a vector space over a field. This definition ensures that vectors interact with scalars and other vectors in a reasonable way.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>vector space over a field</strong> $\F$ is a set $V$ of elements called <strong>vectors</strong>, together with two operations:</p>
<div style="margin-left:1.5em">
<p><strong>Vector Addition</strong><br>
which assigns to any pair of vectors $\vec{u},\vec{v}\in V$ the vector $\vec{u}+\vec{v}\in V$,</p>
<p><strong>Scalar Multiplication</strong><br>
which assigns to any scalar $a\in\F$ and any vector $\vec{v}\in V$ the vector $a\vec{v}\in V$.</p>
</div>
<p>Any vector space and its operations must satisfy the following properties:</p>
<div style="margin-left:1.5em">
<p><strong>Zero Vector</strong><br>
There exists $\vec{0}\in V$ such that $\vec{0}+\vec{v}=\vec{v}$ for every $\vec{v}\in V$.</p>
<p><strong>Additive Inverses</strong><br>
For every $\vec{u}\in V$, there exists $\vec{v}\in V$ for which $\vec{u}+\vec{v}=\vec{0}$.</p>
<p><strong>Commutative Property of Addition</strong><br>
For all $\vec{u},\vec{v}\in V$, we have that $\vec{u}+\vec{v}=\vec{v}+\vec{u}$.</p>
<p><strong>Associative Property of Addition</strong><br>
For all $\vec{u},\vec{v}, \vec{w}\in V$, we have that $(\vec{u}+\vec{v})+\vec{w}=\vec{u}+(\vec{v}+\vec{w})$.</p>
<p><strong>Compatibility with Field Multiplication</strong><br>
For all $a,b\in\F$ and $\vec{v}\in V$, we have that $(ab)\vec{v}=a(b\vec{v})$.</p>
<p><strong>Scalar Multiplicative Identity</strong><br>
For every $\vec{v}\in V$, we have that $1\vec{v}=\vec{v}$.</p>
<p><strong>First Distributive Property</strong><br>
For all $a,b\in\F$ and $\vec{v}\in V$, we have that $(a+b)\vec{v}=a\vec{v}+b\vec{v}$.</p>
<p><strong>Second Distributive Property</strong><br>
For all $a\in\F$ and $\vec{u},\vec{v}\in V$, we have that $a(\vec{u}+\vec{v})=a\vec{u}+a\vec{v}$.</p>
</div>
</blockquote>
<p>Although the choice of field is important when defining a particular vector space, it is often ignored when talking generally about vector spaces. This is because many results hold true for all vector spaces, regardless of the field over which they are defined. Whenever this information is important, it will be specified. Otherwise, we will often refer simply to a vector space $V$, with the understanding that some field $\F$ is lurking in the background.</p>
<h3 id="basicpropertiesanamebasicproperties">Basic Properties<a name="basic-properties"></a></h3>
<p>Now we will verify five basic facts that are true of all vector spaces. They may seem obvious, and many of them closely mimic analogous results we've already seen in group theory. Nonetheless, without proof we could not in good faith use them in our later arguments.</p>
<p>The first important property is that a vector space only has one zero vector.</p>
<blockquote>
<p><strong>Theorem.</strong> In any vector space $V$, the zero vector is unique.</p>
<p><strong>Proof.</strong> Suppose there exist two zero vectors, $\vec{0}_1,\vec{0}_2\in V$. Then, using the definition of a zero vector and the commutative property, we have that</p>
<p>\begin{align}<br>
\vec{0}_1 &amp;= \vec{0}_1+\vec{v}, \\<br>
\vec{0}_2 &amp;= \vec{v}+\vec{0}_2,<br>
\end{align}</p>
<p>for every $\vec{v}\in V$. In light of these equations,</p>
<p>\begin{align}<br>
\vec{0}_1 &amp;= \vec{0}_1 + \vec{0}_2 \\<br>
&amp;= \vec{0}_2.<br>
\end{align}</p>
<p>It follows that the zero vector is unique.</p>
</blockquote>
<p>The next fact that we will prove is that each vector in a vector space has only one additive inverse. Its proof is extremely similar to the above.</p>
<blockquote>
<p><strong>Theorem.</strong> In any vector space $V$, every vector $\vec{u}\in V$ has a unique additive inverse.</p>
<p><strong>Proof.</strong> Suppose $\vec{u}$ has two additive inverses, $\vec{v}_1,\vec{v}_2\in V$. Then, using the definition of an additive inverse and the commutative property, we have that</p>
<p>\begin{align}<br>
\vec{v}_1+\vec{u} &amp;= \vec{0}, \\<br>
\vec{u}+\vec{v}_2 &amp;= \vec{0}.<br>
\end{align}</p>
<p>In light of these equations,</p>
<p>\begin{align}<br>
\vec{v}_1 &amp;= \vec{v}_1 + \vec{0} &amp; \scriptstyle\textit{zero vector}\\<br>
&amp;= \vec{v}_1 + (\vec{u}+\vec{v_2}) &amp; \scriptstyle\textit{additive inverses}\\<br>
&amp;= (\vec{v}_1 + \vec{u}) + \vec{v}_2 &amp; \scriptstyle\textit{associativity}\\<br>
&amp;= \vec{0} + \vec{v}_2 &amp; \scriptstyle\textit{additive inverses}\\<br>
&amp;= \vec{v}_2. &amp; \scriptstyle\textit{zero vector}<br>
\end{align}</p>
<p>It follows that $u$ has a unique additive inverse.</p>
</blockquote>
<p>Because of this theorem, no confusion arises if we write $-\vec{v}$ to denote the additive inverse of $V$. We will adopt this notation for the rest of time.</p>
<p>We will not take the time to do this, but it should be clear how to modify the above two proofs to show that in any field $\F$, additive and multiplicative identities are unique, as well as additive and multiplicative inverses.</p>
<p>Next, we show that the scalar product of a field's additive identity $0$ with any vector yields the zero vector.</p>
<blockquote>
<p><strong>Theorem.</strong> In any vector space $V$, we have that $0\vec{v}=\vec{0}$ for every $\vec{v}\in V$.</p>
<p><strong>Proof.</strong> We proceed via the following computation:</p>
<p>\begin{align}<br>
0\vec{v} + 0\vec{v} &amp;= (0+0)\vec{v} &amp; \scriptstyle\textit{first distributive property}\\<br>
&amp;= 0\vec{v} &amp; \scriptstyle\textit{additive identity}\\<br>
&amp;= 0\vec{v} + \vec{0}. &amp; \scriptstyle\textit{zero vector}<br>
\end{align}</p>
<p>Adding $-0\vec{v}$ to both sides yields $0\vec{v} = \vec{0}$, the desired equality.</p>
</blockquote>
<p>It is very important to realize that in the above proof, the scalar $0$ on the left is the additive identity in the underlying field $\F$, while the vector $\vec{0}$ on the right is the zero vector in the vector space $V$. Remember that we do not have any concept of multiplying vectors.</p>
<p>We will now prove a result of similar obviousness, whose proof is similar to the above.</p>
<blockquote>
<p><strong>Theorem.</strong> In any vector space $V$ over a field $\F$, we have that $a\vec{0}=\vec{0}$ for every $a\in\F$.</p>
<p><strong>Proof.</strong> We proceed via the following computation:</p>
<p>\begin{align}<br>
a\vec{0} + a\vec{0} &amp;= a(\vec{0}+\vec{0}) &amp; \scriptstyle\textit{second distributive property}\\<br>
&amp;= a\vec{0} &amp; \scriptstyle\textit{zero vector}\\<br>
&amp;= a\vec{0} + \vec{0}. &amp; \scriptstyle\textit{zero vector}<br>
\end{align}</p>
<p>Adding $-a\vec{0}$ to both sides yields $a\vec{0} = \vec{0}$, the desired equality.</p>
</blockquote>
<p>There is one obvious fact left to prove, namely that the scalar product of $-1$ with any vector yields the additive inverse of that vector.</p>
<blockquote>
<p><strong>Theorem.</strong> In any vector space $V$, we have that $-1\vec{v}=-\vec{v}$ for every $\vec{v}\in V$.</p>
<p><strong>Proof.</strong> We proceed via the following computation:</p>
<p>\begin{align}<br>
-1\vec{v}+\vec{v} &amp;= -1\vec{v}+1\vec{v} &amp; \scriptstyle\textit{multiplicative identity}\\<br>
&amp;= (-1+1)\vec{v} &amp; \scriptstyle\textit{first distributive property}\\<br>
&amp;= 0\vec{v} &amp; \scriptstyle\textit{additive inverses}\\<br>
&amp;= \vec{0}. &amp; \scriptstyle\textit{zero vector}<br>
\end{align}</p>
<p>Adding $-\vec{v}$ to both sides yields $-1\vec{v} = -\vec{v}$, the desired equality.</p>
</blockquote>
<h3 id="examplesanameexamples">Examples<a name="examples"></a></h3>
<p>We are now armed with a number of facts about abstract vector spaces and their interactions with scalars, but we have yet to exhibit a single actual example of a vector space. We will now examine some vector spaces that are important throughout the entire subject of linear algebra.</p>
<blockquote>
<p><strong>Example.</strong> It isn't too hard to see that the set $\{\0\}$, which contains only the zero vector, is a vector space over any field. We are forced to define vector addition and scalar multiplication in the only possible way, and $\0$ must act as both the zero vector and its own additive inverse. This is analogous to the trivial group in group theory, and is not a particularly interesting example.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> For any field $\F$, it happens that $\F$ is a vector space over itself, taking vector addition to be the same as field addition and scalar multiplication to be the same as field multiplication. This, again, is easy to check directly from the definitions. As specific examples, $\R$ and $\C$ are both vector spaces over themselves.</p>
</blockquote>
<p>For the next example, recall the definition of cartesian product. In particular, recall that $\F^n$ is the set of all ordered tuples of length $n$ with components in $\F$. For instance, $(i, 3+2i, 4)$ is an element of $\C^3$ and $(0, -\pi, 8, \sqrt{2})$ is an element of $\R^4$.</p>
<blockquote>
<p><strong>Example.</strong> For any field $\F$, the set $\F^n$ is a vector space over $\F$ if we define vector addition and scalar multiplication in the following (obvious) way.</p>
<p>Given $\x=(x_1,x_2,\ldots,x_n)$ and $\y=(y_1,y_2,\ldots,y_n)$ in $\F^n$, we define addition component-wise in terms of field addition as follows:</p>
<p>$$\begin{align}<br>
\x+\y &amp;= (x_1,x_2,\ldots,x_n)+(y_1,y_2,\ldots,y_n) \\<br>
&amp;= (x_1+y_1,x_2+y_2,\ldots,x_n+y_n).<br>
\end{align}$$</p>
<p>Similarly, given $a\in\F$ and $\x=(x_1,x_2,\ldots,x_n)\in\F^n$, we define scalar multiplication component-wise in terms of field multiplication as follows:</p>
<p>$$\begin{align}<br>
a\x &amp;= a(x_1,x_2,\ldots,x_n) \\<br>
&amp;= (ax_1,ax_2,\ldots,ax_n).<br>
\end{align}$$</p>
<p>It is not terribly difficult to show that $\F^n$ does in fact constitute a vector space over $\F$ with these operations. For instance, clearly $\0=(0,0,\ldots,0)$ acts as the zero vector in $\F^n$, and $-\x=(-x_1,-x_2,\ldots,-x_n)$ acts as the additive inverse of $\x=(x_1,x_2,\ldots,x_n)$.</p>
</blockquote>
<p>Our next example as a vector space may be slightly surprising at first, but it is both highly important and an excellent example of a vector space whose vectors are certainly not arrows of any kind. First, we will need the following definition.</p>
<blockquote>
<p><strong>Definition.</strong> Let $\F$ denote a field and let $n$ be a natural number. A <strong>polynomial</strong> with coefficients $a_0,a_1,\ldots,a_n$ in $\F$ is a function $p:\F\to\F$ of the form</p>
<p>$$\begin{align}<br>
p(x) &amp;= \sum_{i=0}^n a_ix^i \\<br>
&amp;= a_0 + a_1x+a_2x^2+\cdots+a_nx^n<br>
\end{align}$$</p>
<p>for all $x\in\F$. If $a_n\ne 0$, we say that $n$ is the <strong>degree</strong> of $p$. We write $\mathscr{P}(\F)$ to denote the set of all polynomials with coefficients in $\F$.</p>
</blockquote>
<p>As a concrete example, the function $p:\R\to\R$ defined by</p>
<p>$$p(x)=3+2x+8x^2$$</p>
<p>is a polynomial of degree two with coefficients in $\R$, namely $a_0=3$, $a_1=2$ and $a_2=8$. It is thus an element of $\mathscr{P}(\R)$.</p>
<p>If $p(x)=0$ for all $x\in\F$, we call $p$ the <strong>zero polynomial</strong>, whose degree is undefined. However, for the purposes of adding and multiplying polynomials, it is sometimes useful to formally treat the degree of the zero polyomial as $-\infty$.</p>
<blockquote>
<p><strong>Example.</strong> It is easy to see that for any field $\F$, the set $\mathscr{P}(\F)$ forms a vector space over $\F$ when equipped with the following definitions of vector addition and scalar multiplication.</p>
<p>For any two vectors $\p,\q\in\mathscr{P}(\F)$, we define their sum $\p+\q\in\mathscr{P}(\F)$ in terms of function addition. That is,</p>
<p>$$(\p+\q)(x)=\p(x)+\q(x)$$</p>
<p>for all $x\in\F$. Similarly, for any $a\in\F$ and $\p\in\mathscr{P}(\F)$, we define scalar multiplication as expected:</p>
<p>$$(a\p)(x)=a\p(x)$$</p>
<p>for all $x\in\F$. It is clear that the zero polynomial must act as the zero vector. Again, checking that $\mathscr{P}(\F)$ is actually a vector space with these definitions is easy but fairly time consuming, so I won't do it here.</p>
</blockquote>
<h3 id="subspacesandsumsanamesubspacesandsums">Subspaces and Sums<a name="subspaces-and-sums"></a></h3>
<p>It often happens that a vector space contains a subset which also acts as a vector space under the same operations of addition and scalar multiplication. For instance, the vector space $\{\0\}$ is a (fairly boring) subset of any vector space. This phenomenon is so important that we give it a name.</p>
<blockquote>
<p><strong>Definition.</strong> A subset $U$ of a vector space $V$ is a <strong>subspace</strong> of $V$ if it is itself a vector space under the same operations of vector addition and scalar multiplication.</p>
</blockquote>
<p>It should go without saying that $U$ and $V$ are defined over the same field.</p>
<p>Note the unfortunate naming conflict with subspaces of topological spaces. Normally they are discussed in different contexts and so this causes no confusion. In cases where confusion may arise, we will sometimes refer to them as <em>topological subspaces</em> and <em>linear subspaces</em>.</p>
<p>Using this definition, certainly $\{\0\}$ constitutes a subspace of every vector space. At the other extreme, every vector space is a subspace of itself (because every set is a subset of itself). However, the concept of a subspace wouldn't be very interesting if these were the only possibilities. Before demonstrating some nontrivial proper subspaces, we provide a more straightforward method for determining whether a subset of a vector space constitutes a subspace.</p>
<blockquote>
<p><strong>Proposition.</strong> A subset $U$ of a vector space $V$ is a subspace of $V$ if and only if the following three conditions hold:</p>
<ol>
<li>The zero vector is in $U$.</li>
<li>The set $U$ is closed under addition of vectors.</li>
<li>The set $U$ is closed under scalar multiplication.</li>
</ol>
</blockquote>
<p>Proving this would consist largely of statements like &quot;the associative property holds in $U$ because it holds in $V$.&quot; Therefore, I will omit the proof of this proposition.</p>
<blockquote>
<p><strong>Example.</strong> Consider the set $U=\{(x,0\in\F^2\mid x\in\F\}$, where $\F$ is any field. This is the set of all ordered pairs with entries in $\F$ whose second component is the additive identity.</p>
<p>The zero vector $(0,0)$ is certainly in $U$. From the definition of addition in $\F^2$, the sum of two vectors in $U$ must be of the form</p>
<p>$$(x,0) + (y,0) = (x+y,0)$$</p>
<p>which is certainly in $U$. Similarly, the scalar product of any $a\in\F$ with any vector in $U$ must be of the form</p>
<p>$$a(x,0)=(ax,0)$$</p>
<p>which is also in $U$. We have shown that all three conditions are met, so $U$ is a subspace of $\F^2$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> If $\F$ is a field and $n$ is a natural number, then the set of all polynomials whose degree is at most $n$ forms a subspace of $\P(\F)$. We denote this subspace $\P_n(\F)$.</p>
<p>To see that this is truly a subspace, we will verify the three conditions of the above proposition. We have already stated that the degree of the zero polynomial is $-\infty$, which is certainly less than $N$, so $\0\in\P_n(\F)$.</p>
<p>Next, suppose that $\p,\q\in\P_n(\F)$. Then there exist degrees $j,k\le n$ and coefficients $a_0,\ldots,a_j,b_0,\ldots,b_k\in\F$ for which</p>
<p>$$\begin{align}<br>
\p(x) &amp;= \sum_{i=0}^j a_ix^i, \\<br>
\q(x) &amp;= \sum_{i=0}^k b_ix^i<br>
\end{align}$$</p>
<p>for every $x\in\F$. Since $j$ is the degree of $\p$ and $k$ is the degree of $\q$, we know by definition that $a_j,b_k\ne 0$. Suppose without loss of generality that $j\le k$. It follows that</p>
<p>$$\begin{align}<br>
(\p+\q)(x) &amp;= \sum_{i=0}^j a_ix^i + \sum_{i=0}^k b_ix^i \\<br>
&amp;= \sum_{i=0}^j (a_i+b_i)x^i + \sum_{i=j+1}^k b_ix^i<br>
\end{align}$$</p>
<p>for all $x\in\F$. Note that we have combined like terms until the smaller polynomial ran out, then added terms from the higher-degree polynomial until it was also depleted.</p>
<p>For each $0\le i\le k$, define</p>
<p>$$c_i =<br>
\begin{cases}<br>
a_i + b_i &amp; \text{if } i\le j, \\<br>
b_i &amp; \text{if } i&gt;j.<br>
\end{cases}$$</p>
<p>Then $c_0,\ldots,c_k\in\F$ are the coefficients of the polynomial $\p+\q$. That is,</p>
<p>$$(\p+\q)(x)=\sum_{i=0}^k c_i x^i.$$</p>
<p>If $c_k\ne 0$ then $\p+\q$ is a polynomial of degree $k$. If $c_k = 0$ then $\p+\q$ is a polynomial of degree less than $k$. Since $k\le n$, it follows either way that $\p+\q\in\P(\F)$.</p>
<p>Next, suppose that $a\in\F$ and $p\in\P(\F)$. Then there exists a degree $j\le n$ and coefficients $b_0,\ldots,b_j\in\F$ for which</p>
<p>$$\p(x)=\sum_{i=0}^j b_i x^i$$</p>
<p>for all $x\in\F$. Thus,</p>
<p>$$\begin{align}<br>
a\p(x) &amp;= a\sum_{i=0}^j b_i x^i \\<br>
&amp;= \sum_{i=0}^j ab_i x^i<br>
\end{align}$$</p>
<p>for every $x\in\F$. If $a=0$ then clearly $a\p$ is the zero polynomial and thus $a\p=\0\in\P_n(\F)$. If $a\ne 0$ then for each $0\le i\le j$, define $c_i=ab_i$. Then $c_0,\ldots,c_j$ are the coefficients of $a\p$. That is,</p>
<p>$$a\p(x)=\sum_{i=0}^j c_i x^i.$$</p>
<p>Therefore, $a\p$ is a polynomial of degree $j\le n$, so $a\p\in\P(\F)$. We have established (rather painstakingly) that all three conditions hold, and so we conclude that $\P_n(\F)$ is in fact a subspace of $\P(\F)$.</p>
</blockquote>
<p>We will now work toward defining sums of subspaces. We will begin with the following theorem, which establishes that the intersection of subspaces is always a subspace.</p>
<blockquote>
<p><strong>Theorem.</strong> If $U$ and $W$ are subspaces of a vector space $V$, their intersection  $U\cap V$ is also a subspace of $V$.</p>
<p><strong>Proof.</strong> We will again show that the conditions of the proposition hold. Since $U$ and $W$ are subspaces, we know that they both contain the zero vector and are closed under vector addition and scalar multiplication.</p>
<p>Since $\0\in U$ and $\0\in W$, certainly $\0\in U\cap W$.</p>
<p>For any vectors $\u,\v\in U\cap W$, we have that $\u,\v\in U$ and $\u,\v\in W$. Since both subspaces are closed under vector addition, $u+v\in U$ and $u+v\in W$. Thus, $\u+\v\in U\cap W$.</p>
<p>Lastly, suppose $a\in\F$ and $\v\in U\cap W$. Then $\v\in U$ and $\v\in W$, so $a\v\in U$ and $a\v\in W$ because both subspaces are closed under scalar multiplication. It follows that $a\v\in U\cap W$, completing the proof.</p>
</blockquote>
<p>Using an inductive argument, it is easy to show that this result can be extended to any finite intersection of subspaces.</p>
<p>Naturally, we might now consider the question of whether the union of subspaces is a subspaces. A moment's thought will probably be enough to convince you that this is not true in general. The following example shows why.</p>
<blockquote>
<p><strong>Example.</strong> Consider the subspaces</p>
<p>$$\begin{align}<br>
U &amp;= \{(x,0)\in\R^2\mid x\in\R\}, \\<br>
W &amp;= \{(0,y)\in\R^2\mid y\in\R\}<br>
\end{align}$$</p>
<p>of $\R^2$. We can picture $U$ as the space of all vectors lying on the horizontal axis of the cartesian plane, and $W$ as the space of all vectors lying on the vertical axis. The union of these subspaces, $U\cup W$, is not closed under vector addition. To illustrate this, notice that $(1,0)\in U$ and $(0,1)\in W$, and that $(1,0)+(0,1)=(1,1)$. However, $(1,1)\notin U$ and $(1,1)\notin W$, so certainly $(1,1)\notin U\cup W$. Therefore, $U\cup W$ is not a subspace of $\R^2$.</p>
</blockquote>
<p>This is somewhat unfortunate behavior, because intersections and unions of sets are very natural and easy to work with. We would therefore like to define a way to combine subspaces in order to obtain a new subspace which is <em>as close as possible</em> to their union. We mean this in the sense that this new subspace should be the smallest subspace containing their union. To this end, we define the sum of subspaces in the following manner.</p>
<blockquote>
<p><strong>Definition.</strong> Let $U$ and $W$ be subspaces of a vector space $V$. The <strong>sum</strong> of $U$ and $W$ is the set</p>
<p>$$U+W=\{\u+\w\in V\mid\u\in U,\w\in W\}.$$</p>
<p>Similarly, the sum of subspaces $U_1,\ldots,U_n$ of $V$ is the set</p>
<p>$$\sum_{i=1}^n U_i = \{\u_1 + \cdots + u_n\in V\mid \u_1\in U_1,\ldots,\u_n\in U_n\}.$$</p>
</blockquote>
<p>This definition certainly fixes the issue we had with unions of subspaces, because it ensures by its very definition that it contains the sum of any vector in $U$ and any vector in $W$. We should make sure that the sum of subspaces is actually a subspace.</p>
<blockquote>
<p><strong>Theorem.</strong> If $U$ and $W$ be subspaces of a vector space $V$, their sum $U+W$ is also a subspace of $V$.</p>
<p><strong>Proof.</strong> Since $U$ and $W$ are subspaces, we know that they both contain the zero vector and are closed under vector addition and scalar multiplication.</p>
<p>Since $\0\in U$ and $\0\in W$, clearly $\0\in U+W$ because $\0=\0+\0$, i.e., it is the sum of a vector in $U$ and a vector in $W$.</p>
<p>Next, let $\v_1,\v_2\in U+W$. Then</p>
<p>$$\begin{align}<br>
\v_1 &amp;= \u_1+\w_1, \\<br>
\v_2 &amp;= \u_2+\w_2<br>
\end{align}$$</p>
<p>for some vectors $\u_1,\u_2\in U$ and $\w_1,\w_2\in W$. Since $U$ and $W$ are closed under vector addition, we know that $\u_1+\u_2\in U$ and $\w_1+\w_2\in W$. Therefore,</p>
<p>$$\begin{align}<br>
\v_1+\v_2 &amp;= (\u_1+\w_1) + (\u_2+\w_2) \\<br>
&amp;= (\u_1+\u_2) + (\w_1+\w_2) \\<br>
&amp;\in U+W<br>
\end{align}$$</p>
<p>since it is the sum of a vector in $U$ and a vector in $W$.</p>
<p>Finally, let $a\in\F$ and $\v\in U+W$, so that $\v=\u+\w$ for some $\u\in U$ and some $\w\in W$. Since $U$ and $W$ are closed under scalar multiplication, we know that $a\u\in U$ and $a\w\in W$. Thus,</p>
<p>$$\begin{align}<br>
a\v &amp;= a(\u+\w) \\<br>
&amp;= a\u + a\w \\<br>
&amp;\in U+W<br>
\end{align}$$</p>
<p>since it is the sum of a vector in $U$ and a vector in $W$.</p>
</blockquote>
<p>Now that we have established that the sum of two subspaces is a subspace, we will prove our assertion above — that the sum of two subspaces is the smallest subspace containing their union. We do this by showing that any subspace containing their union must also contain their sum.</p>
<blockquote>
<p><strong>Theorem.</strong> If $U_1$ and $U_2$ are subspaces of a vector space $V$, then every subspace containing $U_1\cup U_2$ also contains $U_1+U_2$.</p>
<p><strong>Proof.</strong> Suppose $W$ is a subspace of $V$ for which $U_1\cup U_2\subseteq W$, and let $\v\in U_1+U_2$. Clearly $\v=\u_1+\u_2$ for some $\u_1\in U_1$ and some $\u_2\in U_2$, and so $\v\in W$ since $U_1\cup U_2\subseteq W$ and $W$ must be closed under vector addition since it is a subspace of $V$. Thus, $U_1+U_2\subseteq W$, completing the proof.</p>
</blockquote>
<p>The above is equivalent to saying that $U_1+U_2$ is the intersection of all subspaces containing $U_1\cup U_2$. Again, this result extends to any finite number of subspaces via a simple inductive argument.</p>
<p>This verifies that the sum of subspaces acts as we originally intended, in that it is as close to their union as possible while remaining a subspace. Now that we have demonstrated this nice characterization of sums, I will provide an example before moving on.</p>
<blockquote>
<p><strong>Example.</strong> Consider the subspaces</p>
<p>$$\begin{align}<br>
U_1 &amp;= \{(x,0,0)\in\R^3\mid x\in\R\}, \\<br>
U_2 &amp;= \{(0,y,0)\in\R^3\mid y\in\R\}, \\<br>
U_3 &amp;= \{(0,0,z)\in\R^3\mid z\in\R\}<br>
\end{align}$$</p>
<p>of the vector space $\R^3$. These correspond to the subspaces of vectors lying along the $x$, $y$ and $z$ axes, respectively. The sum of these three subspaces is $\R^3$ itself, because for any vector $(x,y,z)\in\R^3$, we may write</p>
<p>$$(x,y,z)=(x,0,0)+(0,y,0)+(0,0,z),$$</p>
<p>which is a sum of vectors in $U_1$, $U_2$ and $U_3$, respectively. This sum has a particularly nice property — namely that every vector in $\R^3$ has a <em>unique</em> representation as a sum of vectors in $U_1$, $U_2$ and $U_3$. Put another way, this sum is special because we are forced to express $(x,y,z)$ as above — there is no other way to break it down as such a sum. This is really why we choose to express points in $\R^3$ using these coordinates. In fact, this property of sums is so important that it has a name. But I will discuss direct sums next time, since it's very late and I'm a sleepyhead.</p>
</blockquote>
]]></content:encoded></item><item><title><![CDATA[Free Abelian Groups]]></title><description><![CDATA[Essentially, free abelian groups give us a rigorous way of talking about formal linear combinartions of some set of generators.]]></description><link>https://algebrology.github.io/free-abelian-groups/</link><guid isPermaLink="false">5c8732fffdce06003e4aef59</guid><category><![CDATA[algebra]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Tue, 12 Mar 2019 04:54:30 GMT</pubDate><content:encoded><![CDATA[<ol>
<li><a href="#cyclic-groups">Cyclic Groups</a></li>
<li><a href="#direct-sums">Direct Sums</a></li>
<li><a href="#free-abelian-groups">Free Abelian Groups</a></li>
</ol>
<hr>
<p>This is gonna be a long one. And before we can even talk about free abelian groups we'll need a few definitions. Cyclic groups and direct sums are interesting in their own right. However, I will spend as little time on them as possible right now, using them only as a means to an end. So consider this a speed-run through the machinery needed to construct and effectively use free abelian groups.</p>
<h3 id="cyclicgroupsanamecyclicgroups">Cyclic Groups<a name="cyclic-groups"></a></h3>
<blockquote>
<p><strong>Definition.</strong> If $G$ is a group and $x\in G$, then the <strong>cyclic subgroup</strong> of $G$ <strong>generated by $x$</strong> is the set</p>
<p>$$\inner{x}=\{x^n\mid n\in\Z\}.$$</p>
</blockquote>
<p>So basically a cyclic subgroup consists of everything you can get by multiplying a single element repeatedly. Let's look at a few examples:</p>
<blockquote>
<p><strong>Example.</strong> Let's look at some cyclic subgroups of the infinite group $\Z$ of integers whose operation is the usual addition. Since this is an abelian group, we will be using additive notation in this example, so $x^n$ becomes $nx$ and $ab$ becomes $a+b$.</p>
<p>The cyclic subgroup of $\Z$ generated by $0$ is the set</p>
<p>$$\inner{0}=\{0n\mid n\in\Z\},$$</p>
<p>which is really just $\{0\}$, the trivial group.</p>
<p>The cyclic subgroup of $\Z$ generated by $1$ is the set</p>
<p>$$\inner{1}=\{1n\mid n\in\Z\},$$</p>
<p>which is quite clearly just $\Z$ itself.</p>
<p>The cyclic subgroup of $\Z$ generated by $2$ is the set</p>
<p>$$\inner{2}=\{2n\mid n\in\Z\},$$</p>
<p>which is the set $2\Z$ of multiples of two.</p>
<p>It shouldn't be too hard to see that in general,</p>
<p>$$\begin{align}<br>
\inner{k} &amp;= \{kn\mid n\in\Z\} \\<br>
&amp;= \abs{k}\Z,<br>
\end{align}$$</p>
<p>which is the set of integer multiples of $k$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> Now let's look at a finite group. We'll use $D_3$, the dihedral group of the triangle, which I introduced in my <a href="https://algebrology.github.io/groups-and-their-basic-properties/#examples">first post about groups</a>. Recall that</p>
<p>$$D_3 = \{1,r_1,r_2,f_1,f_2,f_3\},$$</p>
<p>where $1$ is the identity transformation, $r_1$ and $r_2$ are rotations and $f_1,f_2$ and $f_3$ are reflections. Recall also here that the group operation is composition, so that $f_1\circ r_2$ is the result of first rotating via $r_2$ and then reflecting via $f_1$.</p>
<p>To make the following discussion simpler, I have computed the entire group table below:</p>
<p><img src="https://algebrology.github.io/content/images/2019/03/D_3-multiplication-table-2.svg" alt="D_3-multiplication-table-2"></p>
<p>The section I've highlighted in red is the subgroup of rotations, $\{1,r_1,r_2\}$. Recall that the reflections do not form a subgroup, since the composition of two reflections is usually a rotation.</p>
<p>The cyclic subgroup of $D_3$ generated by $1$ is the set</p>
<p>$$\begin{align}<br>
\inner{1} &amp;= \{1^n\mid n\in\Z\} \\<br>
&amp;= \{1\},<br>
\end{align}$$</p>
<p>since composing the identity with itself always results in the identity no matter how many times we do it.</p>
<p>The cyclic subgroup of $D_3$ generated by $r_1$ is the set</p>
<p>$$\begin{align}<br>
\inner{r_1} &amp;= \{r_1^n\mid n\in\Z\} \\<br>
&amp;= \{\ldots, r_1^0, r_1^1, r_1^2, \ldots\} \\<br>
&amp;= \{1,r_1,r_2\},<br>
\end{align}$$</p>
<p>the subgroup of rotations. This cyclic subgroup has order three, because once we reach $r_2$, composing with $r_1$ again gets us back to the identity.</p>
<p>The cyclic subgroup of $D_3$ generated by $r_2$ is the same as the above, since $r_2^0=1$, $r_2^1=r_2$, $r_2^2=r_1$ and $r_2^3=1$, bringing us right back to where we started.</p>
<p>The cyclic subgroup of $D_3$ generated by $f_1$ is the set</p>
<p>$$\begin{align}<br>
\inner{f_1} &amp;= \{f_1^n\mid n\in\Z\} \\<br>
&amp;= \{\ldots, f_1^0, f_1^1, f_1^2, \ldots\} \\<br>
&amp;= \{1, f_1\}.<br>
\end{align}$$</p>
<p>That's because $f_1$ is its own inverse! The other reflections exhibit the same sort of behavior since they are also each their own inverse.</p>
<p>So to wrap things up, $D_3$ has five cyclic subgroups, one of order $1$, one of order $3$, and three of order $2$.</p>
</blockquote>
<p>I will not bother proving that cyclic subgroups are actually subgroups, or that they are always abelian, since the proofs of these facts are basically encompassed in the definition of cyclic subgroups.</p>
<p>Another important thing to notice is the following:</p>
<blockquote>
<p><strong>Note.</strong> All cyclic groups of the same order are isomorphic.</p>
</blockquote>
<p>In addition to finite groups, the above additionally implies that all infinite cyclic groups are isomorphic! This means that any infinite cyclic group is basically $\Z$.</p>
<h3 id="directsumsanamedirectsums">Direct Sums<a name="direct-sums"></a></h3>
<p>Given a collection of abelian groups, it is often convenient to form a new group which combines them in the following manner:</p>
<blockquote>
<p><strong>Definition.</strong> Given a collection of abelian groups $\{G_i\}_{i\in I}$ their <strong>direct sum</strong> is defined as follows:</p>
<ul>
<li>The underlying set is the cartesian product $\displaystyle\prod_{i\in I}G_i$.</li>
<li>The group operation $+$ is given componentwise:</li>
</ul>
<p>$$(g_1,g_2,g_3,\ldots) + (h_1,h_2,h_3,\ldots) = (g_1 + h_1, g_2 + h_2, g_3 +h_3,\ldots),$$</p>
<p>where addition in the $i$th slot represents the group operation in $G_i$.</p>
<p>The resulting group is denoted $\displaystyle\bigoplus_{i\in I}G_i$.</p>
</blockquote>
<p>Since I am speed-running this, I will not bother proving that the direct sum of abelian groups is an abelian group. The proof is very straightforward — it essentially inherits its group properties from the groups it is made from.</p>
<p>I will, however, give one example.</p>
<blockquote>
<p><strong>Example.</strong> Let's look at the direct sum of $\Z$ and $2\Z$. This group consists of all ordered pairs of the form $(n, 2m)$, where $n$ and $m$ are integers. That is, all ordered pairs where the first component is any integer and the second is any even integer.</p>
<p>Given two elements $(n_1, 2m_1)$ and $(n_2, 2m_2)$ of the direct sum $\Z\oplus 2\Z$, their sum is $$(n_1+n_2, 2(m_1+m_2)).$$</p>
</blockquote>
<h3 id="freeabeliangroupsanamefreeabeliangroups">Free Abelian Groups<a name="free-abelian-groups"></a></h3>
<p>Essentially, free abelian groups give us a rigorous way of talking about formal linear combinartions of some set of generators. I will explain what I mean by this in a bit more detail laters on.</p>
<blockquote>
<p><strong>Definition.</strong> Let $B$ denote a subset of an abelian group $F$. Then $F$ is a <strong>free abelian group</strong> with <strong>basis</strong> $B$ if $F=\displaystyle\bigoplus_{b\in B}\inner{b}$ and the cyclic subgroup $\inner{b}$ is infinite for every $b\in B$.</p>
</blockquote>
<p>It is easy to see that free abelian groups are always isomorphic to direct sums of copies of $\Z$, since all infinite cyclic groups are isomorphic to the integers. Furthermore, if $F$ is free abelian then any element $x\in F$ can be written uniquely as</p>
<p>$$x=\sum_{b\in B}m_b b,$$</p>
<p>where $m_b\in\Z$ and all but a finite number of the $m_b$ are zero.</p>
<blockquote>
<p><strong>Example.</strong> Let $B=\{\text{cat},\text{mouse},\text{sheep}\}$. Then any element in the free abelian group with basis $B$ can be written</p>
<p>$$a\cdot\text{cat} + b\cdot\text{mouse} + c\cdot\text{sheep},$$</p>
<p>where $a,b$ and $c$ are integers.</p>
</blockquote>
<p>The next theorem is incredibly important. If you are familiar with linear algebra, it shows us that homomorphisms between free abelian groups behave very similarly to linear maps between vector spaces, in that they are determined entirely by how they act on their bases.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $F$ be a free abelian group with basis $B$. For any abelian group $G$ and any function $f':B\to G$, there exists a unique homomorphism $f:F\to G$ for which $f\restriction{B}=f'$.</p>
<p><strong>Proof.</strong> Choose $x\in F$, so that $x=\displaystyle\sum_{b\in B}m_b b$ for some integers $\{m_b\}_{b\in B}$. Define</p>
<p>$$f(x)=\sum_{b\in B}m_b f'(b).$$</p>
<p>This is clearly a well-defined homomorphism because the expression for $x$ is unique (by definition). In addition, $f$ is unique because two homomorphisms that agree on generators must be equal.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> The above construction of $f$ from $f'$ is called <strong>extending by linearity</strong>.</p>
</blockquote>
<p>It is not difficult to show that any two bases for the same free abelian group must have the same cardinality. This cardinality is akin to the dimension of a vector space, but we use a different name for it.</p>
<blockquote>
<p><strong>Definition.</strong> If $F$ is a free abelian group with basis $B$, then the <strong>rank</strong> of $F$ is an invariant equal to the cardinality of $B$. That is, $\text{rank }F=\abs{B}$, and this does not depend on our choice of basis.</p>
</blockquote>
<p>I'm going to end the post here and save the good stuff for next time. In particular, I still need to talk about matrices for homomorphisms between free abelian groups, Smith Normal Form, and the Fundamental Theorem of Finitely Generated Abelian Groups.</p>
<p>Until next time :D</p>
]]></content:encoded></item><item><title><![CDATA[The First Isomorphism Theorem]]></title><description><![CDATA[It would be nice if there was some sort of relationship between cosets of the kernel and the image of a homomorphism. Oh wait... there is!]]></description><link>https://algebrology.github.io/the-first-isomorphism-theorem/</link><guid isPermaLink="false">5c84a734fdce06003e4aeefc</guid><category><![CDATA[algebra]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Sun, 10 Mar 2019 06:34:06 GMT</pubDate><content:encoded><![CDATA[<p>In the study of group theory, there are a few important theorems called the First, Second and Third Isomorphism Theorems. The second and third are really just special cases of the first, and they will not be immediately useful to us so I will put off their discussion until some other time.</p>
<p>Before proceeding any further, I would like to make a notational change that will make my life a little easier when writing these posts about algebra. Thus far, I have been using $e$ to denote the identity element of a group. From here on out, I will use the following convention:</p>
<blockquote>
<p><strong>Notation.</strong> Given an arbitrary group $G$, its identity element will be written $1_G$ (or simply $1$ when no confusion can arise).</p>
<p>If we know we are working with an additive abelian group, the identity will instead be written $0_G$ (or simply $0$).</p>
</blockquote>
<p>This notational convention may seem odd at first, but it will make things easier to understand in general.</p>
<p>We will need the result I proved in my <a href="https://algebrology.github.io/normal-subgroups-and-quotient-groups/">previous post</a> that the kernel of a group homomorphism is a normal subgroup of the domain. We will also need the following theorem, which basically states that for any homomorphism, elements in the same coset of the kernel all get mapped to the same element.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $f:G\to H$ be a group homomorphism with $x,y\in G$ and let $K=\ker f$ to keep things concise. Then $Kx=Ky$ if and only if $f(x)=f(y)$.</p>
<p><strong>Proof.</strong> Suppose first that $Kx=Ky$. Then $xy^{-1}\in K=\ker f$, so</p>
<p>$$\begin{align}<br>
f(xy^{-1}) &amp;= f(x)f(y^{-1}) \\<br>
&amp;= f(x)f(y)^{-1} \\<br>
&amp;= 1_H.<br>
\end{align}$$</p>
<p>Multiplication on the right by $f(y)$ yields $f(x)=f(y)$.</p>
<p>Suppose conversely that $f(x)=f(y)$. Then we can essentially just do the above in reverse. Multiplication on the right by $f(y)^{-1}$ yields</p>
<p>$$\begin{align}<br>
f(x)f(y)^{-1} &amp;= f(x)f(y^{-1})\\<br>
&amp;= f(xy^{-1}) \\<br>
&amp;= 1_H,<br>
\end{align}$$</p>
<p>and thus $xy^{-1}\in\ker f=K$ by definition, completing the proof.</p>
</blockquote>
<p>This theorem, while deceptively simple, tells us something extremely important. Namely, that each coset of the kernel corresponds to precisely one element of the codomain. It would be nice if there was some sort of relationship between cosets of the kernel and the image of a homomorphism. Oh wait... there is!</p>
<blockquote>
<p><strong>First Isomorphism Theorem.</strong> If $f:G\to H$ is a group homomorphism, then $G/\ker f$ is isomorphic to $\im f$.</p>
<p><strong>Proof.</strong> Our claim is that the function $\varphi:G/\ker f\to\im f$ defined by $$\varphi(Kx)=f(x)$$ for every $Kx\in G/\ker f$ is an isomorphism. Again for convenience, we let $K=\ker f$.</p>
<p>First, since $\varphi$ acts on cosets we must show that it is well defined, i.e., if $Kx=Ky$ then $\varphi(Kx)=\varphi(Ky)$. But from the above theorem, if $Kx=Ky$ then $f(x)=f(y)$, so it follows immediately that this is true.</p>
<p>Next we argue that $\varphi$ is a homomorphism. Again, choose cosets $Kx$ and $Ky$ in $G/K$. Then</p>
<p>$$\begin{align}<br>
\varphi(Kx)\varphi(Ky) &amp;= f(x)f(y) \\<br>
&amp;= f(xy) \\<br>
&amp;= \varphi(K(xy))<br>
\end{align}$$</p>
<p>because $f$ is a homomorphism, so $\varphi$ essentially inherits this property from $f$.</p>
<p>Finally, we need to show that $\varphi$ is a bijection. To see that it is injective, suppose that $\varphi(Kx)=\varphi(Ky)$ for some cosets $Kx$ and $Ky$ in $G/K$. Then $f(x)=f(y)$, and so $Kx=Ky$ by the above theorem. To see that $\varphi$ is surjective, choose any $b\in\im f$. By definition, there exists $a\in G$ for which $f(a)=b$. Thus $b=\varphi(Ka)$, and so it follows that $\varphi$ is bijective, completing the proof.</p>
</blockquote>
<p>I'm trying to keep this post short, but I should provide at least one example of how this theorem can be applied.</p>
<blockquote>
<p><strong>Example.</strong> Consider the abelian group $\R^*$ of nonzero real numbers under multiplication, the group $\R^+$ of positive real numbers under multiplication, and the subgroup $\{-1,1\}$ of $\R^*$. This subgroup is certainly normal since it comes from an abelian group.</p>
<p>We will show that the quotient group $\frac{\R^*}{\{-1,1\}}$ is isomorphic to $\R^+$. And we'll do it using the first isomorphism theorem!</p>
<p>We need to define a surjective homomorphism $f:\R^*\to \R^+$ for which $\ker f=\{-1,1\}$. This is actually really easy. We'll just let $f(x)=\abs{x}$. Clearly $f(x)=1$ precisely when either $x=1$ or $x=-1$, and so $\ker f=\{-1,1\}$ as desired. That $f$ is a homomorphism follows from the fact that $\abs{xy}=\abs{x}\abs{y}$ for all real numbers. It's just as easy to see that $f$ is surjective, because for any $x\in\R^+$ we know that at least $\abs{x}=x$.</p>
<p>We've demonstrated that there exists a surjective homomorphism from $\R^*$ to $\R^+$ whose kernel is $\{-1,1\}$. The first isomorphism theorem immediately yields the desired result — that $\frac{\R^*}{\{-1,1\}}$ is isomorphic to $\R^+$.</p>
<p>We don't even need to construct the isomorphism, although it's fairly straightforward to do so (the proof of the first isomorphism theorem tells us how).</p>
<p>And intuitively, these groups <em>should</em> be isomorphic, if we think of the quotient group $\frac{\R^*}{\{-1,1\}}$ in the correct way. Basically, $\R^*$ is a gluing together of two copies of $\R^+$ — one copy containing all the positive numbers, and the other a mirror image containing all the negative numbers. The quotient group essentially &quot;factors out&quot; the sign of each number, leaving us with only one copy. The result is something that looks, acts and feels exactly like $\R^+$.</p>
</blockquote>
]]></content:encoded></item><item><title><![CDATA[Normal Subgroups and Quotient Groups]]></title><description><![CDATA[Let's now revisit the quotient set $G/H$, where $H$ is a subgroup of $G$. What we'd really like to do is turn $G/H$ into a group in a meaningful way. What should the group operation be, though?]]></description><link>https://algebrology.github.io/normal-subgroups-and-quotient-groups/</link><guid isPermaLink="false">5c7f40876fe30a003ed53402</guid><category><![CDATA[algebra]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Wed, 06 Mar 2019 04:52:39 GMT</pubDate><content:encoded><![CDATA[<ol>
<li><a href="#normal-subgroups">Normal Subgroups</a></li>
<li><a href="#quotient-groups">Quotient Groups</a></li>
</ol>
<hr>
<h3 id="normalsubgroupsanamenormalsubgroups">Normal Subgroups<a name="normal-subgroups"></a></h3>
<p>Let's now revisit the quotient set $G/H$, where $H$ is a subgroup of $G$. What we'd really like to do is turn $G/H$ into a group in a meaningful way. What should the group operation be, though? Recall that the elements of $G/H$ are the cosets of $H$, so what we really need to define is a meaningful way to multiply cosets.</p>
<p>It would be great if we could define coset multiplication as follows:</p>
<blockquote>
<p><strong>Desired Definition.</strong> If $H$ is a subgroup of $G$ and $x,y\in G$, then<br>
$$(Hx)(Hy)=H(xy).$$</p>
</blockquote>
<p>Unfortunately, this is not always uniquely defined. That is, if $Hx=Ha$ and $Hy=Hb$ for some $a,b\in G$, it is possible that $H(xy)\ne H(ab)$, which means this isn't an acceptable way to multiply cosets. This is the same behavior we saw when I proposed an incorrect definition of addition while <a href="https://algebrology.github.io/constructing-the-rational-numbers-1/#construction">constructing the rational numbers</a>. The result cannot depend on our choice of representative for the equivalence class!</p>
<p>Interestingly, in this case the remedy is not to define coset multiplication in a different manner. Rather, we choose to restrict our attention only to cosets of a certain, particularly nice sort of subgroup.</p>
<blockquote>
<p><strong>Definition.</strong> A subgroup $N$ of a group $G$ is a <strong>normal subgroup</strong> if $xnx^{-1}\in N$ whenever $n\in N$ and $x\in G$. We refer to this defining property of normal subgroups by saying they are <strong>closed under conjugation</strong>.</p>
</blockquote>
<p>It goes without saying that every subgroup of an abelian group is normal, since in that case</p>
<p>$$\begin{align}<br>
xnx^{-1} &amp;= xx^{-1}n \\<br>
&amp;= n,<br>
\end{align}$$</p>
<p>which is in $N$ by definition. However, there are certainly non-abelian groups with normal subgroups. And normal subgroups are particularly special because their left and right cosets are the same, even if they are not subgroups of an abelian group!</p>
<blockquote>
<p><strong>Theorem.</strong> If $N$ is a normal subgroup of a group $G$, then $xN=Nx$ for every $x\in G$.</p>
<p><strong>Proof.</strong> For any $g\in xN$, we have that $g=xn$ for some $n\in N$. Since $N$ is closed under conjugation, $xnx^{-1}\in N$ and so $g=(xnx^{-1}x)\in Nx$. Thus, $xN\subseteq Nx$.</p>
<p>The proof that $Nx\subseteq xN$ is nearly identical, so I will not include it.</p>
</blockquote>
<p>In a recent post I showed that the kernel of a homomorphism is a always a subgroup of its domain. It turns out that they are actually <em>normal</em> subgroups, and this will be very important to us later on.</p>
<blockquote>
<p><strong>Theorem.</strong> The kernel of any group homomorphism is a normal subgroup of the domain.</p>
<p><strong>Proof.</strong> Let $G$ and $H$ be groups and let $f:G\to H$ denote a homomorphism between them. Suppose $x\in G$ and $k\in\ker f$. We need to show that $xkx^{-1}\in\ker f$.</p>
<p>Since $k\in\ker f$, we have by definition that $f(k)=e$. Furthermore, since $f$ is a homomorphism,</p>
<p>$$\begin{align}<br>
f(xkx^{-1}) &amp;= f(x)f(k)f(x^{-1}) \\<br>
&amp;= f(x)ef(x)^{-1} \\<br>
&amp;= f(x)f(x)^{-1} \\<br>
&amp;= e.<br>
\end{align}$$</p>
<p>Thus, $xkx^{-1}$ is in $\ker f$ by definition, so $\ker f$ is a normal subgroup of $G$.</p>
</blockquote>
<h3 id="quotientgroupsanamequotientgroups">Quotient Groups<a name="quotient-groups"></a></h3>
<p>We are now ready to define quotient <em>groups</em>!</p>
<blockquote>
<p><strong>Definition.</strong> Let $N$ denote a normal subgroup of a group $G$. The <strong>quotient group of $G$ modulo $N$</strong> is the set $G/N$ together with coset multiplication defined by $(Nx)(Ny)=N(xy)$ for all $x,y\in G$.</p>
</blockquote>
<p>That's all well and good, but we're getting ahead of ourselves again. We still haven't actually shown that this multiplication is well defined when restricted to cosets of normal subgroups. Let's do that right now. After that, we'll still need to prove that this thing we'ved defined is really a group.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $N$ denote a normal subgroup of a group $G$, with $Nx=Na$ and $Ny=Nb$ for some $a,b,x,y\in G$. Then $N(xy)=N(ab)$.</p>
<p><strong>Proof.</strong> Since $Nx=Na$, we know that $x\in Na$ and thus $x=n_1a$ for some $n_1\in N$. Similarly, since $Ny=Nb$, we know that $y\in Nb$ and thus $y=n_2b$ for some $n_2\in N$. Thus,</p>
<p>$$\begin{align}<br>
xy &amp;= n_1an_2b \\<br>
&amp;\in NaNb \\<br>
&amp;= N(ab),<br>
\end{align}$$</p>
<p>so $N(xy)=N(ab)$, as desired.</p>
</blockquote>
<p>Now we need to show that quotient groups are actually groups. The proof of this is fairly straightforward.</p>
<blockquote>
<p><strong>Theorem.</strong> The quotient group as defined above is in fact a group.</p>
<p><strong>Proof.</strong> We have already shown that coset multiplication is well defined. We will show first that it is associative. Consider $Nx, Ny, Nz\in G/N$. By definition,</p>
<p>$$\begin{align}<br>
Nx(NyNz) &amp;= NxN(yz) \\<br>
&amp;= N(xyz) \\<br>
&amp;= N(xy)Nz \\<br>
&amp;= (NxNy)Nz.<br>
\end{align}$$</p>
<p>Next, we will show that $N$ is the identity element of $G/N$. Because $N$ is a subgroup of $G$, the identity element $e\in G$ is in $N$. Thus, for any $Nx\in G/N$, we have that</p>
<p>$$\begin{align}<br>
NNx &amp;= NeNx \\<br>
&amp;= N(ex) \\<br>
&amp;= Nx \\<br>
&amp;= N(xe) \\<br>
&amp;= NxNe \\<br>
&amp;= NxN.<br>
\end{align}$$</p>
<p>Finally, we will show that for any $Nx\in G/N$, the coset $Nx^{-1}$ is its inverse. This is clear because</p>
<p>$$\begin{align}<br>
NxNx^{-1} &amp;= N(xx^{-1}) \\<br>
&amp;= Ne \\<br>
&amp;= N(x^{-1}x) \\<br>
&amp;= Nx^{-1}Nx.<br>
\end{align}$$</p>
<p>We have shown that $G/N$ has all the properties of a group, so the proof is complete.</p>
</blockquote>
<p>Before moving on, let's look at a concrete example of a quotient group which is hopefully already familiar to you.</p>
<blockquote>
<p><strong>Example.</strong> Consider again the group $\Z$ of integers under addition and its subgroup $2\Z$ of even integers. Certainly $2\Z$ is a normal subgroup because $\Z$ is abelian, and we may thus form the quotient group $\Z/2\Z$. Recall that this quotient group contains only two cosets, namely $2\Z$ and $2\Z+1$.</p>
<p>Coset &quot;multiplication&quot; here is really coset addition because we are working in an additive group. Since $2\Z$ is the identity, we have that</p>
<p>$$\begin{align}<br>
2\Z+2\Z &amp;= 2\Z, \\<br>
2\Z + (2\Z+1) &amp;= 2\Z+1,\\<br>
(2\Z+1) + 2\Z &amp;= 2\Z+1, \\<br>
(2\Z+1) + (2\Z+1) &amp;= 2\Z.<br>
\end{align}$$</p>
<p>Let's rename these cosets, just for kicks. We'll refer to $2\Z$ as $0$ and $2\Z+1$ as $1$. Then we get</p>
<p>$$\begin{align}<br>
0 + 0 &amp;= 0, \\<br>
0 + 1 &amp;= 1,\\<br>
1 + 0 &amp;= 1, \\<br>
1 + 1 &amp;= 0.<br>
\end{align}$$</p>
<p>This is precisely the group $\Z_2$ of integers modulo $2$, with the operation of addition modulo $2$. In fact, the formal way to define $\Z_n$, the group of integers modulo $n$, is as the quotient group $\Z/n\Z$.</p>
</blockquote>
<p>Now I'd like to give some motivation for why quotient groups are so important and useful. This is best done by example, because otherwise my explanation would likely turn into another incomprehensible rant.</p>
<blockquote>
<p><strong>Example.</strong> To start, we define a <strong>commutator</strong> of a group $G$ to be any element of the form $aba^{-1}b^{-1}$, where $a,b\in G$. Notice that $aba^{-1}b^{-1}$ reduces to the identity $e$ if and only if $ab=ba$. That is, the commutator $aba^{-1}b^{-1}$ collapses to the identity precisely when $a$ and $b$ commute. Clearly in any abelian group, every commutator is equal to the identity element.</p>
<p>Suppose a quotient group $G/N$ is abelian. That is, for any two cosets $Nx, Ny\in G/N$, their product commutes, i.e.,</p>
<p>$$\begin{align}<br>
NxNy &amp;= N(xy) \\<br>
&amp;= N(yx) \\<br>
&amp;= NyNx.<br>
\end{align}$$</p>
<p>But this is true if and only if</p>
<p>$$\begin{align}<br>
xy(yx)^{-1} &amp;= xyx^{-1}y^{-1} \\<br>
&amp;\in N.<br>
\end{align}$$</p>
<p>Thus, a quotient group is abelian precisely when all commutators are contained within the identity coset!</p>
<p>A slightly more liberal, though meaningful, way of phrasing this is by saying that if we <em>factor out</em> all the commutators of $G$, we are always left with an abelian group. This because all commutators are collapsed to the identity element in the quotient group when we quotient out the commutator subgroup. This is just one example of a more general phenomenon.</p>
</blockquote>
]]></content:encoded></item><item><title><![CDATA[Cosets and Lagrange's Theorem]]></title><description><![CDATA[It's a bit difficult to explain exactly why cosets are so important without working with them for a while first. But as you'll hopefully start to understand within my next few posts, cosets pop up everywhere and are a necessary tool to get anything done in the world of algebra.]]></description><link>https://algebrology.github.io/cosets-and-lagranges-theorem/</link><guid isPermaLink="false">5c7df11d6fe30a003ed533c6</guid><category><![CDATA[algebra]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Tue, 05 Mar 2019 03:53:03 GMT</pubDate><content:encoded><![CDATA[<ol>
<li><a href="#cosets">Cosets</a></li>
<li><a href="#lagrange's-theorem">Lagrange's Theorem</a></li>
</ol>
<hr>
<h3 id="cosetsanamecosets">Cosets<a name="cosets"></a></h3>
<p>It's a bit difficult to explain exactly why cosets are so important without working with them for a while first. But as you'll hopefully start to understand within my next few posts, cosets pop up everywhere and are a necessary tool to get anything done in the world of algebra. Let's dig in, shall we?</p>
<blockquote>
<p><strong>Definition.</strong> Let $H$ denote a subgoup of a group $G$. For any fixed element $x\in G$, the <strong>right coset</strong> of $H$ with respect to $x$ is the set $Hx=\{hx\in G\mid h\in H\}$.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> Let $H$ denote a subgoup of a group $G$. For any fixed element $x\in G$, the <strong>left coset</strong> of $H$ with respect to $x$ is the set $xH=\{xh\in G\mid h\in H\}$.</p>
</blockquote>
<blockquote>
<p><strong>Note.</strong> For abelian groups with additive notation, right and left cosets are instead denoted by $H+x$ and $x+H$, respectively.</p>
</blockquote>
<p>So basically a coset is a set obtained by taking the elements of a subgroup and adding a particular element to all of them. Note that a coset does need not be a subgroup! To get a feeling for what cosets really are and how they behave, let's look at an example.</p>
<blockquote>
<p><strong>Example.</strong> Take $G=\Z$, the additive group of integers, and $H=2\Z$, the set of even integers. Certainly $2\Z$ is a subgroup of $Z$ because it contains the identity $0$, the sum of two even integers is even, and every even integer has an even inverse — its negative.</p>
<p>Let's look at a few cosets of $2\Z$. How about the right cosets with respect to $0,1,2$ and $3$? Notice that because $\Z$ is an additive group, these cosets will be written $2\Z+0$, $2\Z+1$, $2\Z+2$ and $2\Z+3$.</p>
<p>Directly from the definition of a right coset, we see that</p>
<p>$$\begin{align}<br>
2\Z+0 &amp;= \{x+0\in\Z\mid x\in 2\Z\} \\<br>
&amp;= \{x\in\Z\mid x\in 2\Z\} \\<br>
&amp;= 2\Z.<br>
\end{align}$$</p>
<p>So in this case, the coset with respect to $0$ is just the subgroup $2\Z$. This actually always happens, as you can easily see. Adding the identity to all elements of a subgroup will of course just yield that subgroup again!</p>
<p>Next, let's look at $2\Z+1$. This is the set $\{x+1\in\Z\mid x\in 2\Z\}$, which consists of things like $\ldots,-3,-1,1,3,5,\ldots$. This is really just the set of odd integers! It's definitely not a subgroup of $\Z$ though, since it doesn't contain $0$.</p>
<p>The coset $2\Z+2$ is the set $\{x+2\in\Z\mid z\in 2\Z\}$, which contains elements like $\ldots,-4,-2,0,2,4,\ldots$. But this is the set of even integers again! That means $2\Z+2=2\Z$.</p>
<p>Lastly, let's look at $2\Z+2$. This is the set $\{x+3\in\Z\mid z\in 2\Z\}$, which contains things like $\ldots,-3,-1,1,3,5,\ldots$. We've already seen that somewhere before. It's just the set of odd integers again. That is, $2\Z+3=2\Z+1$.</p>
<p>It looks like there might actually only be two distinct cosets of $2\Z$. They are $2\Z$ itself and $2\Z+1$. Furthermore, every element of $\Z$ is in either one coset or the other, but never both (because an integer is either even or odd). So these cosets actually <em>partition</em> $\Z$, which is a very important point. But let's not get too far ahead of ourselves.</p>
</blockquote>
<p>The first thing I'd like to prove about cosets is fairly simple — if we are working in an abelian group, left and right cosets are the same!</p>
<blockquote>
<p><strong>Theorem.</strong> If $H$ is a subgroup of an abelian group $G$, then $H+x=x+H$ for every $x\in G$.</p>
<p><strong>Proof.</strong> We will proceed by demonstrating that each side is a subset of the other.</p>
<p>We show first that $H+x\subseteq x+H$. Choose $g\in H+x$, so that $g=h+x$ for some $h\in H$. Since $G$ is abelian, $h+x=x+h$ and thus $g=x+h\in x+H$. It follows that $H+x\subseteq x+H$.</p>
<p>The proof that $x+H\subseteq H+x$ is completely analogous to the above, so we won't bother with it. We can thus conclude that $H+x=x+H$.</p>
</blockquote>
<p>This implies, for instance, that we could instead write $1+2\Z$ to denote the odd integers, but to me this doesn't look right for some reason and so I usually don't. In fact, for most of our purposes we really only need to consider one variety of coset. I tend to favor right cosets.</p>
<p>Now it's time to prove my suspicion from the above example. This is important, so pay close attention. If you don't remember what a partition is, I advise you to read <a href="https://algebrology.github.io/equivalence-relations-and-quotient-sets/">my earlier post about them</a>.</p>
<blockquote>
<p><strong>Theorem.</strong> If $H$ is a subgroup of a group $G$, then the (left/right) cosets of $H$ partition $G$.</p>
<p><strong>Proof.</strong> We will prove the result for right cosets, since the proof for left cosets is practically identical.</p>
<p>It is clear that every coset $Hx$ is nonempty because the identity element $e$ is in $H$ by virtue of it being a subgroup, and thus $x=ex\in Hx$.</p>
<p>The next thing we need to show is that cosets cover all of $G$. But is clear that $$\bigcup_{x\in G}x=G\subseteq\bigcup_{x\in G}Hx,$$</p>
<p>and similarly that $$\bigcup_{x\in G}Hx\subseteq G=\bigcup_{x\in G}x,$$</p>
<p>because $Hx\subseteq G$ for every $x\in G$. Thus, $G=\bigcup_{x\in G}Hx$.</p>
<p>The last thing we need to show is that for any $x,y\in G$, if $Hx\ne Hy$ then $Hx\cap Hy=\varnothing$. That is, either cosets are the same or they are disjoint. We will argue the contrapositve, supposing that $Hx\cap Hy$ is nonempty. Then there exists some element $a\in Hx\cap Hy$. That is, $a\in Hx$ and $a\in Hy$, so $a=h_1x$ and $a=h_2y$ for some elements $h_1,h_2\in H$. It follows that $h_1x=h_2y$, and multiplication on the left by $h_1^{-1}$ show that $x=h_1^{-1}h_2y\in Hy$ because $h_1^{-1}h_2\in H$. Thus $Hx=Hy$, completing the proof.</p>
</blockquote>
<p>This is pretty exciting! It means that we can form an equivalence relation $\sim$ on any group $G$, where elements are equivalent if they are in the same coset of $H$. Moreover, we can form the quotient set $G\quotient{\sim}$, whose elements are precisely the cosets of $H$. For simplicity, we generally write $G/H$ instead of to denote this quotient set, since it is so very important that we are talking about the set of cosets of $H$. We'll revisit this special type of quotient set in my next post.</p>
<h3 id="lagrangestheoremanamelagrangestheorem">Lagrange's Theorem<a name="lagrange's-theorem"></a></h3>
<p>We're actually very close to proving this famous theorem already. We just need one lemma first:</p>
<blockquote>
<p><strong>Lemma.</strong> If $H$ is a subgroup of a group $G$ and $x\in G$, there exists a bijection $f:G\to Hx$.</p>
<p><strong>Proof.</strong> We define a function $f:H\to Hx$ by $f(h)=hx$ for every $h\in H$. That is, the function that maps every element of $H$ to its product with $x$. Clearly $f$ is injective because if $f(h_1)=f(h_2)$ then $h_1x=h2_x$ and the right cancellation law yields $h_1=h_2$. Furthermore, $f$ is surjective because for any $y\in Hx$ we have by definition that $y=hx$ for some $h\in H$, and thus $y=f(h)$. It follows that $f$ is bijective, as desired.</p>
</blockquote>
<p>Recall that a bijection exists between finite sets only if those sets contain the same number of elements. This fact, along with the lemma above, tells us that all cosets of a subgroup are the same size! We are now ready to prove Lagrange's Theorem, which is actually very easy at this point.</p>
<blockquote>
<p><strong>Lagrange's Theorem.</strong> If $H$ is a subgroup of a finite group $G$, then $\abs{G}$ is a multiple of $\abs{H}$.</p>
<p><strong>Proof.</strong> We have already established that every coset of $H$ contains the same number of elements, $\abs{H}$. Since $G$ is finite, there are a finite number of distinct cosets of $H$, say $n$ of them. Because these cosets partition $G$, it follows that $\abs{G}=n\abs{H}$, completing the proof.</p>
</blockquote>
<p>This is probably somewhat surprising unless you're already familiar with groups. But it's just part of the astonishing usefulness of cosets. Lagrange's Theorem is actually incredibly useful because it tells us instantly that certain things <em>cannot</em> be subgroups of other things. For instance, a group of order $12$ cannot ever have subgroups of order $5,7,8,9,10$ or $11$. Without this theorem, you might have guessed that this was the case, but it would have been pretty tricky to prove it conclusively.</p>
]]></content:encoded></item><item><title><![CDATA[Group Homomorphisms]]></title><description><![CDATA[A recurring theme in mathematics is that examining the maps between objects is indispensable to understanding those objects themselves. Of course, that depends on choosing the "correct" type of maps.]]></description><link>https://algebrology.github.io/group-homomorphisms/</link><guid isPermaLink="false">5c7b67366fe30a003ed53390</guid><category><![CDATA[algebra]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Sun, 03 Mar 2019 05:37:37 GMT</pubDate><content:encoded><![CDATA[<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#homomorphisms">Homomorphisms</a></li>
</ol>
<hr>
<h3 id="introductionanameintroduction">Introduction<a name="introduction"></a></h3>
<p>A long time ago I wrote <a href="https://algebrology.github.io/groups-and-their-basic-properties/">my first post about group theory</a>, in which I defined groups and subgroups, gave several examples of groups and proved a few of their basic but important properties. Hopefully at the very least you remember that a group is a set with an associative binary operation, an identity element and inverses for all its elements. The identity, as well as each element's invers, is unique and we can cancel like terms from both sides of equations like we generally do without thinking anyway.</p>
<p>Before I dive in, there are just a few quick things we will need before we can talk about homomorphisms. The first is an easy way to identify whether a subset of a group is actually a subgroup.</p>
<blockquote>
<p><strong>Lemma.</strong> A subset $H$ of a group $G$ is a subgroup of $G$ if it is nonempty and if $ab^{-1}\in H$ whenever $a,b\in H$.</p>
<p><strong>Proof.</strong> Since the operation on $H$ is inherited from $G$, it is clearly associative. Next, since $H$ is nonempty there exists $x\in H$, and thus by hypothesis $xx^{-1}=e\in H$, where $e$ is the identity element. Furthermore, since $e,x\in H$, we have that $ex^{-1}=x^{-1}\in H$, so $H$ contains inverses for all its elements. Finally, for any $a,b\in H$ we have established that $b^{-1}\in H$. So again by hypothesis, $a(b^{-1})^{-1}=ab\in H$, and so $H$ is closed under products. We have shown that $H$ is a group, and thus it is a subgroup of $G$.</p>
</blockquote>
<p>Notice that when I say something is &quot;closed under products,&quot; what this really means is that the group operation is a well defined binary operation. For multiplicative groups, the word &quot;product&quot; fits nicely. For abelian groups with additive notation, it doesn't make quite as much sense but I will probably slip up and use it anyway.</p>
<p>For the most part, this lemma will simply make it a little bit faster to verify that certain things are subgroups of other groups. This will help keep future proofs nice and short.</p>
<p>The next thing I should mention is the concept of a group's order. Because I have not talked about cardinality anywhere on my blog yet, I will only define order for finite groups. Luckily, this is all we'll need for a while.</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>order</strong> of a finite group is the number of elements that group contains. We denote the order of a group $G$ by $\abs{G}$.</p>
</blockquote>
<p>That's a pretty simple definition. The trivial group contains only the identity, so it is a group of order one. The dihedral group of the regular $n$-gon, written $D_n$ and which I talked about in my first post on groups, has $2n$ elements and thus $\abs{D_n}=2n$. There are an abundance of examples of interesting finite groups, and the order of a finite group is really just its size.</p>
<h3 id="homomorphismsanamehomomorphisms">Homomorphisms<a name="homomorphisms"></a></h3>
<p>A recurring theme in mathematics is that examining the maps between objects is indispensable to understanding those objects themselves. Of course, that depends on choosing the &quot;correct&quot; type of maps. For topological spaces, it is the continuous maps that help us to understand their structure. For groups, we have the following analogous idea.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>group homomorphism</strong> between groups $G$ and $H$ is a function $f:G\to H$ such that $f(x)f(y)=f(xy)$ for all $x,y\in G$.</p>
</blockquote>
<p>When it is understood that I am talking about groups, I will often refer to these things simply as homomorphisms, rather than group homomorphisms.</p>
<p>The first thing I want to stress is that <em>homomorphism</em> and <em>homeomorphism</em> are different words and different concepts. Homomorphisms are functions that preserve group multiplication, whereas homeomorphisms preserve open sets in topological spaces.</p>
<p>The seconds thing I need to mention is that in the above definition, the multiplication of $f(x)$ and $f(y)$ on the left side of the equation takes place in the group $H$, whereas the multiplication of $x$ and $y$ on the right takes place in $G$. That is to say, homomorphisms are precisely the maps for which is does not matter whether we multiply elements before or after applying the map.</p>
<p>To put it another way, we can multiply elements in $G$ and take the image of their product, or we can take their images first and then multiply them in $H$. Either way, we always get the same answer. This means that the group operation is preserved by homomorphisms. And that means that much of the group's structure is also preserved. There is, in fact, a special type of homomorphism which perfectly preserves a group's structure.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>group isomorphism</strong> between groups $G$ and $H$ is a bijective homomorphism $f:G\to H$.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> Two groups are <strong>isomorphic</strong> if there exists a group isomorphism between them.</p>
</blockquote>
<p>Isomorphisms are the nicest sort of map between groups. Since they are bijective, they map each group element to precisely one element in the other group. Since they are homomorphisms, they preserve the group operation. This means that an isomorphism is essentially just a way of renaming the elements of a group. They do not alter the way that elements interaction with each other. Perhaps an example will help get my point across.</p>
<blockquote>
<p><strong>Example.</strong> The group $(\R, +)$ of real numbers under addition and the group $(\R^+, \cdot)$ of positive real numbers under multiplication are isomorphic. We can establish this by exhibiting an isomorphism between them.</p>
<p>Let's define the function $f:\R^+\to\R$ by $f(x)=\log x$. We will argue that $f$ is an isomorphism. First, it is clearly a homomorphism because from the basic properties of the logarithm,</p>
<p>$$\begin{align}<br>
f(x) + f(y) &amp;= \log x +\log y \\<br>
&amp;= \log(x \cdot y) \\<br>
&amp;= f(x \cdot y).<br>
\end{align}$$</p>
<p>In addition, we know that $f$ is bijective because it has an inverse function, the exponential map $f^{-1}:\R\to\R^+$ given explicitly by $f^{-1}(x)=e^x$.</p>
<p>What does it really mean when we say that these groups are isomorphic? It isn't just some meaningless abstract statement that these groups are &quot;essentially&quot; the same. It actually means that we can do algebra in one and then easily transfer it over to the other group! That is, we can do multiplication in $\R^+$ simply by doing the corresponding addition in $\R$ and then applying the logarithm. This is the principle that slide rules are based on. Addition is easier than multiplication, so we add our numbers by hand and then use the magic slide rule isomorphism machine to take us between groups and into the world of multiplication.</p>
</blockquote>
<p>Isomorphisms may be useful in certain cases, but they are actually a bit boring precisely <em>because</em> they perfectly preserve structure. After all, how interesting is a flat renaming of group elements? We might as well just call two isomorphic groups the same. General homomorphisms are actually much more interesting.</p>
<p>It may be enlightening to see an example of a homomorphism that is not an isomorphism, but is still surprisingly structure-preserving.</p>
<blockquote>
<p><strong>Example.</strong> Consider the group $\Z$ of integers under addition and the group $\{-1,1\}$ under multiplication. In $\{-1,1\}$ we have that</p>
<p>$$\begin{align}<br>
1\cdot 1 &amp;= 1, \\<br>
-1\cdot 1 &amp;= -1. \\<br>
1\cdot -1 &amp;= -1, \\<br>
-1\cdot -1 &amp;= 1,<br>
\end{align}$$</p>
<p>So clearly the identity element in $\{-1,1\}$ is $1$ and every element is its own inverse. Let's construct a function $f:\Z\to\{-1,1\}$ defined by</p>
<p>$$f(x) =<br>
\begin{cases}<br>
1 &amp; \text{if } x\in 2\Z, \\<br>
-1 &amp; \text{if } x\in 2\Z+1.<br>
\end{cases}$$</p>
<p>(Here $2\Z$ is the set of even integers and $2\Z+1$ is the set of odd integers. My reason for denoting them this way will become apparent once we talk about cosets in a later post.)</p>
<p>Let's show that this function is a homomorphism. Choose $m,n\in\Z$. There are three possible cases:</p>
<ol>
<li>Both $m$ and $n$ are even. That is, $m,n\in 2\Z$.</li>
<li>Both $m$ and $n$ are odd. That is, $m,n\in 2\Z+1$.</li>
<li>Either $m$ is even and $n$ is odd or vice versa. Because both groups we're considering are abelian, it suffices to consider only one of these cases, $m\in 2\Z$ and $n\in 2\Z+1$.</li>
</ol>
<p>We examine case 1 first. Suppose $m,n\in 2\Z$, so that $f(m)=f(n)=1$. Then $m=2k_1$ and $n=2k_2$ for some $k_1,k_2\in\Z$. This means that $$m+n=2k_1+2k_2=2(k_1+k_2)\in 2\Z.$$ Thus, $$f(m)f(n)=1\cdot 1=1=f(m+n).$$</p>
<p>Next, we look at case 2. Suppose that $m,n\in 2\Z+1$, so that $f(m)=f(n)=-1$. Then $m=2k_1+1$ and $n=2k_2+1$ for some $k_1,k_2\in\Z$. This means that $$m+n=(2k_1+1)+(2k_2+1)=2k_1+2k_2+2=2(k_1+k_2+1)\in 2\Z.$$ Thus, $$f(m)f(n)=-1\cdot -1=1=f(m+n).$$</p>
<p>Lastly, we examine case 3. Suppose that $m\in 2\Z$ but $n\in 2\Z+1$, so that $f(m)=1$ and $f(n)=-1$. Then $m=2k_1$ and $n=2k_2+1$ for some $k_1,k_2\in\Z$. This means that $$m+n=2k_1+2k_2+1=2(k_1+k_2)+1\in 2\Z+1.$$ Thus, $$f(m)f(n)=1\cdot -1=-1=f(m+n).$$</p>
<p>Since we have now checked that $f(m)f(n)=f(m+n)$ for all $m,n\in\Z$, it follows that $f$ is a homomorphism.</p>
<p>That was really us showing what we already know — that the sum of two even numbers is even, the sum of two odd numbers is also even, and the sum of an even and an odd number is odd. The property of being even or odd is sometimes called <strong>parity</strong>, and this homomorphism preseves the parity of integers, as well as what happens to the parity of integers after you add them together. Actually, parity is basically all that is preserved by this homomorphism, but it's still a nice example of how homomorphisms can preserve important aspects of a group's structure.</p>
</blockquote>
<p>Now for some basic properties of homomorphisms. I promise these next two proofs are easy and short. The first property is that homomorphisms always map the identity of one group to the identity of the other. If we were being very careful, we would represent the identity of $G$ by $e_G$ and the identity in $H$ by $e_H$. However, it is common to be a bit sloppy and use $e$ to represent the identity element in both groups. If proper care is taken, this should never result in any confusion.</p>
<blockquote>
<p><strong>Theorem.</strong> If $f:G\to H$ is a group homomorphism then $f(e)=e$.</p>
<p><strong>Proof.</strong> Since $f$ is a homomorphism, we have that</p>
<p>$$\begin{align}<br>
f(e)f(e) &amp;= f(ee) \\<br>
&amp;= f(e) \\<br>
&amp;= f(e)e.<br>
\end{align}$$</p>
<p>Thus, cancellation on the left yields the equality $f(e)=e$.</p>
</blockquote>
<p>The next property is just as straightforward — homomorphisms always map inverses to inverses.</p>
<blockquote>
<p><strong>Theorem.</strong> If $f:G\to H$ is a group homomorphism then $f(x^{-1})=f(x)^{-1}$ for every $x\in G$.</p>
<p><strong>Proof.</strong> Since $f$ is a homomorphism, we have that</p>
<p>$$\begin{align}<br>
f(x)f(x^{-1}) &amp;= f(xx^{-1}) \\<br>
&amp;= f(e) \\<br>
&amp;= e.<br>
\end{align}$$</p>
<p>Multiplication on the left by $f(x)^{-1}$ on both sides yields $f(x^{-1})=f(x)^{-1}$, as desired.</p>
</blockquote>
<p>Both of these properties only serve to reinforce my claim that homomorphisms are structure-preserving maps. I'll shut my smug face now and define two of the most important things ever.</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>kernel</strong> of a group homomorphism $f:G\to H$ is the set $$\ker f=\{x\in G\mid f(x)=e\}.$$ That is, it is the set of elements in $G$ that get mapped to the identity in $H$.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> The <strong>image</strong> of a group homomorphism $f:G\to H$ is the set $$\im f=f[G]=\{f(x)\in H\mid x\in G\}.$$ That is, it is the set of elements in $H$ that get mapped to by some element in $G$.</p>
</blockquote>
<p>If $f:G\to H$ is a group homomorphism and $f$ is surjective, it should be clear that $H=\im f$. Similarly, if $f$ is injective then $\ker f = \{e\}$. These facts, along with the next two theorems, are crucial to everything we will do with groups in the future.</p>
<blockquote>
<p><strong>Theorem.</strong> If $f:G\to H$ is a group homomorphism then its kernel is a subgroup of $G$.</p>
<p><strong>Proof.</strong> From the lemma at the beginning of this post, it suffices to show that $\ker f$ is nonempty and that $ab^{-1}\in \ker f$ whenever $a,b\in\ker f$.</p>
<p>The fact that $\ker f$ is nonempty follows immediately from the property that $f(e)=e$, so certainly $e\in\ker f$.</p>
<p>Next, suppose that $a,b\in\ker f$. That is, $f(a)=f(b)=e$. Then</p>
<p>$$\begin{align}<br>
f(ab^{-1}) &amp;= f(a)f(b^{-1}) \\<br>
&amp;= f(a)f(b)^{-1}) \\<br>
&amp;= ee^{-1} \\<br>
&amp;= e,<br>
\end{align}$$</p>
<p>and thus $ab^{-1}\in\ker f$ by definition. It follows that $\ker f$ is a subgroup of $G$.</p>
</blockquote>
<p>Along similar lines:</p>
<blockquote>
<p><strong>Theorem.</strong> If $f:G\to H$ is a group homomorphism then its image is a subgroup of $H$.</p>
<p><strong>Proof.</strong> We will employ the same technique as in the previous proof.</p>
<p>Certainly $\im f$ is nonempty because $e\in G$ and thus $f(e)=e\in\im g$.</p>
<p>Next, suppose that $a,b\in\in f$. That is, $a=f(x)$ and $b=f(y)$ for some $x,y\in G$. Certainly $y^{-1}\in G$ and thus $xy^{-1}\in G$ as well, so<br>
$$\begin{align}<br>
ab^{-1} &amp;= f(x)f(y)^{-1} \\<br>
&amp;= f(xy^{-1}) \\<br>
&amp;\in \im f,<br>
\end{align}$$</p>
<p>as desired.</p>
</blockquote>
<p>In my next few posts, you will begin to understand why it's such a big deal that the kernal and image of a homomorphism are subgroups.</p>
]]></content:encoded></item><item><title><![CDATA[Path Connectedness]]></title><description><![CDATA[Now we can think about a different type of "connectedness." Intuitively, if a space is "connected" you should be able to draw a path between any two points in the space. Otherwise, if there are points in the space that cannot be connected by a path, it is "disconnected."]]></description><link>https://algebrology.github.io/path-connectedness/</link><guid isPermaLink="false">5c706452e91ada004c1f7d50</guid><category><![CDATA[topology]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Wed, 27 Feb 2019 02:47:43 GMT</pubDate><content:encoded><![CDATA[<ol>
<li><a href="#some-useful-tools">Some Useful Tools</a></li>
<li><a href="#path-connected-spaces">Path Connected Spaces</a></li>
<li><a href="#a-pathological-example">A Path-ological Example</a></li>
</ol>
<hr>
<h3 id="someusefultoolsanamesomeusefultools">Some Useful Tools<a name="some-useful-tools"></a></h3>
<p>I defined connectedness in an earlier post, but there's actually a completely different route we could have taken to define it. Recall that a space is connected if it cannot be separated by disjoint neighborhoods. In this post we'll explore an alternative approach to defining connectedness of topological space, called <em>path connectedness</em>.</p>
<p>But before we do so, we actually need another fact about connectedness. (Recall that $\overline B$ denotes the closure of the set $B$.)</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ denote a topological space with a connected subsset $A\subseteq X$. If the subset $B\subseteq X$ satisfies $A\subseteq B\subseteq \overline{A}$, then $B$ is connected.</p>
<p><strong>Proof.</strong> We give a proof by contradiction. Suppose $B$ is not connected. Then there exist open sets $U,V\subseteq B$ which separate $B$. Since $A$ is a connected subset of $B$, this means that either $A\subseteq U$ or $A\subseteq V$. Assume without loss of generality that $A\subseteq U$. Then $A\cap V=\varnothing$ because $U\cap V = \varnothing$. Note, however, that $B\cap V\ne\varnothing$. Thus, we may choose $x\in B\cap V$. Certainly $x\in B$ and so it follows that $x\in\overline{A}$ because $B\subseteq\overline{A}$. On the other hand, we have $x\in V$, which is open in $X$ and disjoint from $B$, so $x\notin\overline{A}$. This is a contradiction, since it cannot be that both $x\in\overline{A}$ and $x\notin\overline{A}$.</p>
</blockquote>
<p>What the above theorem really says is that adding limit points to a connected set always results in another connected set. This result will be invaluable to us throughout this post and in the future. Here is just one example of its usefulness:</p>
<blockquote>
<p><strong>Corollary.</strong> All types of intervals in $\mathbb{R}$ (open, half-open and closed) are connected.</p>
<p><strong>Proof.</strong> We've shown previously that open intervals are connected. Therefore, the rest are as well since they can be obtained from open intervals by adding limit points.</p>
</blockquote>
<p>Of particular interest to us is the connectedness of the closed unit interval $[0, 1]$.</p>
<p>The following invaluable lemma allows us to determine when a piecewise function constructed from continuous maps is continuous.</p>
<blockquote>
<p><strong>Gluing Lemma.</strong> Let $X$ and $Y$ denote topological spaces with closed subsets $U,V\subseteq X$ for which $X=U\cup V$. If $f_1:U\to Y$ and $f_2:V\to Y$ are continuous functions which agree on their intersection, i.e. $f_1\restriction{U\cap V}=f_2\restriction{U\cap V}$, then there exists a unique continuous function $f:X\to Y$ with $f\restriction{U}=f_1$ and $f\restriction{V}=f_2$.</p>
<p><strong>Proof.</strong> Showing that such a function is unique is fairly straightforward. We simply define $f:X\to Y$ by</p>
<p>$$f(x) =<br>
\begin{cases}<br>
f_1(x) &amp; \text{if } x\in U, \\<br>
f_2(x) &amp; \text{if } x\in V.<br>
\end{cases}$$</p>
<p>This function is clearly well defined because $f_1$ and $f_2$ agree on their intersection.</p>
<p>We will argue next that $f$ is continuous by demonstrating that the preimage of any open set in $Y$ is open in $X$. To this end, suppose $W\subseteq Y$ is open. Then</p>
<p>$$\begin{align}<br>
f^{-1}[W] &amp;= X \cap f^{-1}[W] \\<br>
&amp;= (U \cup V) \cap f^{-1}[W] \\<br>
&amp;= (U \cap f^{-1}[W]) \cup (V \cap f^{-1}[W]) \\<br>
&amp;= (U \cap f_1^{-1}[W]) \cup (V \cap f_2^{-1}[W]) \\<br>
&amp;= f_1^{-1}[W] \cup f_2^{-1}[W].<br>
\end{align}$$</p>
<p>Since $f_1$ and $f_2$ are continuous, certainly $f_1^{-1}[W]$ and $f_2^{-1}[W]$ are open in $U$ and $V$, respectively. Thus $f^{-1}[W]$ is open in $U\cup V = X$ since it is the union of open sets. It follows that $f$ is continuous, as desired.</p>
</blockquote>
<h3 id="pathconnectedspacesanamepathconnectedspaces">Path Connected Spaces<a name="path-connected-spaces"></a></h3>
<p>We can now work toward defining path connectedness. But first we need to define what a path is! The definition may look a bit ominous at first, but it's really just a formal way of saying exactly what you'd expect.</p>
<blockquote>
<p><strong>Definition.</strong> Let $X$ denote a topological space with points $a,b\in X$. A <strong>path</strong> in $X$ from $a$ to $b$ is a continuous map $p:[0,1]\to X$ for which $p(0)=a$ and $p(1)=b$.</p>
</blockquote>
<p>It is implicitly understood in such cases that $[0,1]$ is to be viewed as a subspace of $\R$ in the standard topology.</p>
<p>Basically a path is just a continuous map from the unit interval into the space of interest. Some authors use the word to describe both a path $p$ and its image $p([0,1])\subseteq X$, but I will keep the concepts separate. That is to say, a path is <em>not</em> the set of points that its image traces out in the codomain. Its image is a useful way to visualize it, however. And of course, since $[0,1]$ is a connected set, so is its image because paths are continuous! This means that the image of a path looks like a connected curve.</p>
<p><img src="https://algebrology.github.io/content/images/2019/03/path-1.svg" alt="path-1"></p>
<p>Now we can think about a different type of &quot;connectedness.&quot; Intuitively, if a space is &quot;connected&quot; you should be able to draw a path between any two points in the space. Otherwise, if there are points in the space that cannot be connected by a path, it is &quot;disconnected.&quot;</p>
<p>But we've already used these words for a different concept, so we need to make up new ones.</p>
<blockquote>
<p><strong>Definition.</strong> A topological space $X$ is <strong>path connected</strong> if for any pair of points $a,b\in X$ there is a path $p:[0,1]\to X$ from $a$ to $b$.</p>
</blockquote>
<p>You may be wondering whether path connectedness and connectedness are actually one and the same. After all, it's hard to visualize a space that is connected in one sense but not the other. It turns out that one direction is easy.</p>
<blockquote>
<p><strong>Theorem.</strong> Every path connected topological space is connected.</p>
<p><strong>Proof.</strong> We give a proof by contradiction. Suppose a space $X$ is path connected but not connected. Then there exists a separation of $X$ by nonempty disjoint open sets $U,V\subseteq X$. Choose points $u\in U$ and $v\in V$. Since $X$ is path connected, there exists a path $p:[0,1]\to X$ with $p(0)=u$ and $p(1)=v$. Define $U'=U\cap p([0,1])$ and $V'=V\cap p([0,1])$. We will show that $U'$ and $V'$ form a separation of $p([0,1])$.</p>
<p>First, we note that $u\in U'$ because $u\in U$ and $p(0)=u$ so $u\in p([0,1])$. Similarly, $v\in V'$ because $v\in V$ and $p(1)=v$ so $v\in p([0,1])$. Thus, $U'$ and $V'$ are nonempty. Next, clearly</p>
<p>$$\begin{align}<br>
U' \cup V' &amp;= \big(U \cap p([0, 1])\big) \cup \big(V \cap p([0, 1])\big) \\<br>
&amp;= (U \cup V) \cap p([0,1]) \\<br>
&amp;= X \cap p([0,1]) \\<br>
&amp;= p([0,1]).<br>
\end{align}$$</p>
<p>A similar computation yields</p>
<p>$$\begin{align}<br>
U' \cap V' &amp;= \big(U \cap p([0, 1])\big) \cap \big(V \cap p([0, 1])\big) \\<br>
&amp;= (U \cap V) \cap p([0,1]) \\<br>
&amp;= \varnothing \cap p([0,1]) \\<br>
&amp;= \varnothing.<br>
\end{align}$$</p>
<p>Finally, $U'$ and $V'$ are both open in the subspace topology on $p([0,1])$ because they are intersections of $p([0,1])$ with open sets in $X$ ($U$ and $V$, respectively). It follows that $U'$ and $V'$ separate $p([0,1])$ and so it is disconnected. However, it is the continuous image of the connected set $[0,1]$, so this is a contradiction.</p>
</blockquote>
<p>So path connectedness implies connectedness. But as we shall see later on, the converse does not necessarily hold. So the two notions are actually different. It takes more to be a path connected space than a connected one!</p>
<p>As with any topological concept, we want to show that path connectedness is preserved by continuous maps.</p>
<blockquote>
<p><strong>Theorem.</strong> If $f:X\to Y$ is a continuous function and $X$ is path connected, then $f[X]\subseteq Y$ is also path connected.</p>
<p><strong>Proof.</strong> Choose points $a,b\in f[X]$ and pick $a'\in f^{-1}[\{a\}]$ and $b'\in f^{-1}[\{b\}]$. Since $X$ is path connected, there exists some path $p:[0,1]\to X$ from $a'$ to $b'$. We argue that $f\circ p:[0,1]\to Y$ is a path in $Y$ from $a$ to $b$.</p>
<p>Certainly $f\circ p$ is continuous since it is the composition of continuous maps. Furthermore,</p>
<p>$$\begin{align}<br>
(f\circ p)(0) &amp;= f(p(0)) \\<br>
&amp;= f(a') \\<br>
&amp;= a,<br>
\end{align}$$</p>
<p>and</p>
<p>$$\begin{align}<br>
(f\circ p)(1) &amp;= f(p(1)) \\<br>
&amp;= f(b') \\<br>
&amp;= b.<br>
\end{align}$$</p>
<p>Thus $f\circ p$ is a path from $a$ to $b$, completing the proof.</p>
</blockquote>
<p>We saw earlier that disconnected spaces could be decomposed into maximal connected subsets. There is also a way of doing this for path disconnected spaces as well.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ denote a topological space and define a relation $\sim$ on $X$ as follows: $a\sim b$ if and only if there exists a path in $X$ from $a$ to $b$. Then $\sim$ is an equivalence relation.</p>
<p><strong>Proof.</strong> We first show reflexivity. For any point $a\in X$, the constant map $p:[0,1]\to X$ given by $p(t)=a$ is certainly a path from $a$ to itself, so $a\sim a$.</p>
<p>Next we show symmetry. If $a\sim b$ then there exists a path $p:[0,1]\to X$ with $p(0)=a$ and $p(1)=b$. Define $p^{-}:[0,1]\to X$ by $p^{-}(t)=p(1-t)$. We may think of $p^{-}$ as tracing out $p$ in reverse. That $p^{-}$ is well defined follows because $1-t\in [0,1]$ whenever $t\in [0,1]$, so the images of $p$ and $p^{-}$ are actually equal. Next, $p^{-}$ is continuous since it is the composition of the continuous functions $p$ and $1-t$. Finally, $p^{-}(0)=p(1)=b$ and $p^{-}(1)=p(0)=a$, so $p^{-}$ is a path from $b$ to $a$. Thus, $b\sim a$.</p>
<p>Lastly, we show transitivity. If $a\sim b$ and $b\sim c$ then there exist paths $p_1:[0,1]\to X$ with $p_1(0)=a$ and $p_1(1)=b$ and $p_2:[0,1]\to X$ with $p_2(0)=b$ and $p_2(1)=c$. Define $p:[0,1]\to X$ by</p>
<p>$$p(t) =<br>
\begin{cases}<br>
p_1(2t) &amp; \text{if } t\in [0,\frac{1}{2}], \\<br>
p_2(2t-1) &amp; \text{if } t\in [\frac{1}{2},1].<br>
\end{cases}$$</p>
<p>We may think of $p$ as a path which traces out $p_1$ and then $p_2$ at twice the original speed. That $p$ is well defined follows from the fact that $p_1(1)=p_2(0)=b$. By the gluing lemma, this also implies that $p$ is continuous. Furthermore, $p(0)=p_1(0)=a$ and $p(1)=p_2(1)=c$, so $p$ is a path from $a$ to $c$. Thus, $a\sim c$, completing the proof.</p>
</blockquote>
<p>The above result gives us our maximal path-connected subsets:</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>path components</strong> of a topological space are the equivalence classes of the relation $\sim$ defined above.</p>
</blockquote>
<h3 id="apathologicalexampleanameapathologicalexample">A Path-ological Example<a name="a-pathological-example"></a></h3>
<p>(I wonder how many topology professors have made that pun with regard to the example I give below? Probably hundreds.)</p>
<p>I mentioned above that connected spaces are not always path connected. In this section I will give an example of such a space, although the proof that this is the case is not exactly trivial. First, we will need the following result about sequences and limit points.</p>
<blockquote>
<p><strong>Definition.</strong> A sequence $\{x_n\}_{n\in\N}$ in a topological space $X$ is <strong>eventually constant</strong> if there exists $x \in X$ and $N \in \N$ for which $x_n=x$ whenever $n&gt;N$.</p>
</blockquote>
<p>Note that any eventually constant sequence trivially converges to the point $x$ in the above definition.</p>
<blockquote>
<p><strong>Lemma.</strong> Let $X$ denote a topological space and let $A$ be a subset of $X$. If there exists a sequence $\{p_n\}_{n\in\N}$ of points in $A$ that converges to $p\in X$ and which is not eventually constant, then $p$ is a limit point of $A$.</p>
<p><strong>Proof.</strong> Choose any neighborhood $U$ of $p$. Since $\{p_n\}_{n\in\N}$ converges to $p$, there exists $N_1\in\N$ for which $p_n\in U$ whenever $n&gt;N$. Since $\{p_n\}_{n\in\N}$ is not eventually constant, there exists $N_2\in N$ for which $p_n\neq p$ whenever $n&gt;N_2$. Choose $N=\max\{N_1,N_2\}$. For every $n&gt;N$, we have that $p_n\in U$ and $p_n\ne p$. By definition, it follows that $p$ is a limit point of $A$.</p>
</blockquote>
<p>We are now ready to give an example of a space that is connected but not path connected. Warning: it's pretty gross.</p>
<blockquote>
<p><strong>Example (The Topologist's Sine Curve).</strong> Define</p>
<p>$$\begin{align}<br>
A &amp;= \Big\{\Big(x,\sin\big(\tfrac{1}{x}\big)\Big)\in\R^2\mid x\in (0,1)\Big\}, \\<br>
B &amp;= \Big\{(0,y)\in\R^2\mid y\in [-1,1]\Big\}.<br>
\end{align}$$</p>
<p>$A$ is a sort of sine curve which oscillates more and more rapidly as you approach the origin from the right, and which does not include its endpoints. $B$ is just a vertical line. The <strong>topologist's sine curve</strong> is their union, $X=A\cup B$.</p>
<p><img src="https://algebrology.github.io/content/images/2019/03/topologists-sine-curve-1.svg" alt="topologists-sine-curve-1"></p>
<p>The blue curve in the above graph is the set $A$ and the red curve (which may be too narrow on your display to see clearly) is the $B$. The line $B$ is sort of the natural completion of $A$, in the sense that $A$ never touches is but really looks like it ought to. More formally, every point of $B$ is a limit point of $A$.</p>
<p>To show this, choose a point $(0,y)\in B$. Certainly $y=\sin\theta$ for some $\theta\in [0,2\pi)$, and thus $y=\sin(\theta+2\pi n)$ for every $n\in \N$. If $x_n=\frac{1}{\theta+2\pi n}$ then $y=\sin\big(\tfrac{1}{x_n}\big)$ for every $n\in \N$. Furthermore, it is clear that $\lim_{n\to\infty}x_n = 0$. Thus,</p>
<p>$$\lim_{x_n\to 0}\Big(x_n,\sin\big(\tfrac{1}{x_n}\big)\Big) = (0,y).$$</p>
<p>Since the sequence $\{x_n\}_{n\in\N}$ is not eventually constant, every point $y\in B$ is a limit point of $A$. Since $A$ is connected (it is the continuous image of $(0,1)$), it follows from the lemma at the beginning of this post that the topologist's sine curve $X$ is connected, since it can be obtained from a connected set plus by adding limit points.</p>
<p>Showing that $X$ is not path connected is a bit trickier. We will proceed by contradiction, supposing that there exists a path $p:[0,1]\to X$ with $p(0)=(0,0)\in B$ and $p(1)\in A$. Since $B$ is closed in $\R^2$, it follows from the continuity of $p$ that $p^{-1}[B]$ is closed in $[0,1]$. Thus, $p^{-1}[B]$ contains a largest element, which we shall denote by $b$. This implies that $p\restriction{[b,1]}:[b,1]\to X$ is continuous with $p\restriction{[b,1]}(b)\in B$. and $p\restriction{[b,1]}(t)\in X-B = A$ for every $t&gt;b$.</p>
<p>We define $p':[0,1]\to X$ by $p'(t)=p\big((1-b)t+b\big)$. Clearly $p'(0)=p(b)$ and $p'(1)=p(1)$. Also, $p'$ is continuous since it is the composition of continuous functions. Thus, $p'$ is a path in $X$ from $p(b)$ to $p(1)$ with $p'([0,1])=p([b,1])$. Furthermore, $p'(0)\in B$ while $p'(t)\in A$ for every $t\in (0,1]$.</p>
<p>Now, for any $t\in [0,1]$ we can write $p'(t)=\big(x(t),y(t)\big)$ in terms of its components $x:[0,1]\to [0,1]$ and $y:[0,1]\to [-1,1]$. Clearly both $x$ and $y$ are continuous. Observe that $x(0)=0$ while $x(t)\in (0,1]$ whenever $t\in (0,1]$. In addition, $y(0)=0$ while $y(t)=\sin\big(\frac{1}{x(t)}\big)$ for any $t\in (0,1]$. Choose $n\in\N$ and $u\in\Big(0,x\big(\frac{1}{n}\big)\Big)$ for which $\sin\big(\frac{1}{u}\big)=(-1)^n$. From the intermediate value theorem, there exists $t_n\in\big(0,\frac{1}{n}\big)$ for which $x(t_n)=u$. Since $\lim_{n\to\infty}\frac{1}{n}=0$ and $0&lt;t_n&lt;\frac{1}{n}$ for every $n\in\N$, clearly $\lim_{n\to\infty}t_n=0$. However, $y(t_n)=\sin\big(\frac{1}{x(t_n)}\big)=\sin\big(\frac{1}{u_n}\big)=(-1)^n$, which does not converge. This contradicts the continuity of $y$.</p>
</blockquote>
<p>There are a few other well known examples of spaces like the above that are connected but not path connected. The most prominent is the <strong>topologist's whirlpool</strong>, which is essentially just the polar form of the topologist's sine curve.</p>
<p>One might wonder if there is a sufficient additional criterion for a connected space to be path connected? The answer is yes. If a space is connected and <em>locally path connected</em> then it is path connected. However, I will not define local path connectedness or prove this result, since I do not really care about it and it is not useful to us.</p>
]]></content:encoded></item><item><title><![CDATA[Constructing the Rational Numbers (2)]]></title><description><![CDATA[This is a continuation of Constructing the Rational Numbers (1). Before moving forward with the rest of the construction, I'd like to formally change my notation for rational numbers from that of equivalence classes of ordered pairs of integers to that of fractions.]]></description><link>https://algebrology.github.io/constructing-the-rational-numbers-2/</link><guid isPermaLink="false">5c6f446575042202e3d9fce2</guid><category><![CDATA[foundations]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Fri, 22 Feb 2019 00:38:35 GMT</pubDate><content:encoded><![CDATA[<h3 id="contents">Contents</h3>
<ol>
<li><a href="#canonical-form">Canonical Form</a></li>
<li><a href="#where-is-z">Where is $\Z$?</a></li>
<li><a href="#ordering-the-rationals">Ordering the Rationals</a></li>
<li><a href="#filling-the-gaps">Filling the Gaps</a></li>
<li><a href="#field-axioms">Field Axioms</a></li>
</ol>
<hr>
<h3 id="canonicalformanamecanonicalform">Canonical Form<a name="canonical-form"></a></h3>
<p>This is a continuation of <a href="https://algebrology.github.io/constructing-the-rational-numbers-1/">Constructing the Rational Numbers (1)</a>. Before moving forward with the rest of the construction, I'd like to formally change my notation for rational numbers from that of equivalence classes of ordered pairs of integers to that of fractions.</p>
<p>That is, from here on out the rational number $[(a,b)]$ will simply be written as the <strong>fraction</strong> $\frac{a}{b}$. And now rational numbers look exactly how you would expect. Yay! Note that the bar doesn't really mean anything yet, this notation is currently just a formalism.</p>
<p>Remember that there are many choices of representative for each rational number. For instance, $\frac{1}{2}=\frac{2}{4}$ in the same way that $[(1,2)]=[(2,4)]$ because, in the language of my last post, $(1,2)\sim_\Q (2,4)$. Last time we defined this equivalence relation, $\sim_\Q$, which determines whether two fractions are really the same rational number. Let's rephrase this in terms of fractions and without the formality of the equivalence relation, because it's very important:</p>
<blockquote>
<p><strong>Rule (Cross Multiplication).</strong> Two fractions $\frac{a}{b}$ and $\frac{c}{d}$ represent the same rational number if and only if $ad=bc$.</p>
</blockquote>
<p>We know in our hearts that although there are many fractional representations of each rational, there is always one preferred representation. That is, although $\frac{1}{2}$ and $\frac{2}{4}$ are both valid representations of the same number, we definitely prefer to call it $\frac{1}{2}$ because it's simpler somehow.</p>
<p>We can easily make this decision rigorous, but first we need to recall the following definition:</p>
<blockquote>
<p><strong>Definition.</strong> Given two integers $a$ and $b$, their <strong>greatest common divisor</strong>, written $\gcd(a,b)$, is the largest positive integer which is a factor of both $a$ and $b$.</p>
</blockquote>
<p>Here are some examples, although you have likely seen this concept before.</p>
<blockquote>
<p><strong>Example.</strong> Consider $4$ and $6$. We can write $4=2\cdot 2$ and $6=2\cdot 3$. They both have one factor in common: $2$. Thus, $\gcd(4,6)=2$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> Consider $15$ and $30$. We can write $15=3\cdot 5$ and $30=2\cdot 3\cdot 5$. They have two prime factors in common, so their greatest common divisor is the product of these factors: $\gcd(15, 30)=15$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> Consider $0$ and $6$. We can trivially write $0=0\cdot 6$, and so $\gcd(0, 6) = 6$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> Consider $3$ and $7$. We cannot simplify either number further since they are both prime. Clearly they have no factors in common other than $1$, so $\gcd(3, 7)=1$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> Consider $4$ and $9$. Neither is prime, so we can write $4=2\cdot 2$ and $9=3\cdot 3$. There are still no common factors other than $1$, so $\gcd(4,9)=1$.</p>
</blockquote>
<p>That last example is particularly interesting, since we had two integers which were not prime, and whose greatest common divisor was still 1. We have a special name for this:</p>
<blockquote>
<p><strong>Definition.</strong> Two integers $a$ and $b$ are <strong>coprime</strong> (or <strong>relatively prime</strong>) if $\gcd(a,b)=1$.</p>
</blockquote>
<p>I will not prove it, since we are taking properties of the integers for granted, but the greatest common divisor of two integers always exists as long as they are not both zero.</p>
<p>We now have the tools to choose a preferred representation for each rational number.</p>
<blockquote>
<p><strong>Definition.</strong> A fractional representation $\frac{a}{b}$ for a rational number is in <strong>canonical form</strong> if $a$ and $b$ are coprime and $b&gt;0$.</p>
</blockquote>
<p>This definition should hopefully make sense to you. It means, for instance, that $\frac{1}{2}$ is the canonical form for the rational number which is also represented by $\frac{2}{4}$, $\frac{-1}{-2}$, etc.</p>
<p>However, whenever we make a definition like this it is important to determine two things. Does it always exist? And if it exists, is it unique? In this case, the answer to both questions is yet. There is always exactly one canonical form for each rational number, and so we may refer to it as <em>the</em> canonical form. Let's prove this!</p>
<blockquote>
<p><strong>Proposition.</strong> Every rational number has a unique canonical form.</p>
<p><strong>Proof.</strong> Choose a rational number $q$. We argue first that a canonical form for $q$ exists. Let $\frac{a}{b}$ be some fraction which represents $q$. Note that $\gcd(a,b)$ is guaranteed to exist because $b\neq 0$. Thus there are integers $m$ and $n$ for which</p>
<p>$$\frac{a}{b} = \frac{m\cdot\gcd(a,b)}{n\cdot\gcd(a,b)}.$$</p>
<p>We argue that $\frac{a}{b}=\frac{m}{n}$. This is easily done by remembering the above rule for determining whether two fractions represent the same rational number. From the first equality above, we have that</p>
<p>$$a \cdot n \cdot \gcd(a,b) = b \cdot m \cdot \gcd(a,b).$$</p>
<p>Using the left cancellation property of the integers, we may cancel $\gcd(a,b)$ from both sides to obtain</p>
<p>$$an=bm.$$</p>
<p>Again using the cross multiplication rule for equivalent fractions, we see that $\frac{a}{b}=\frac{m}{n}$. If $n$ is positive, this is certainly a canonical form for $q$. Otherwise, we may easily obtain a canonical form by negating both slots of the fraction: $\frac{-m}{-n}$. It is easy to show from here that $\frac{a}{b}=\frac{-m}{-n}$, so I will not bother with it.</p>
<p>We have demonstrated that a canonical form for $q$ exists. It remains to show that it is unique. That is, we aim to show that if $\frac{m_1}{n_1}$ and $\frac{m_2}{n_2}$ are both canonical forms for $q$, then $m_1=m_2$ and $n_1=n_2$.</p>
<p>This is not too difficult. Since both fractions are already in canonical form, we know that $\gcd(m_1,n_1)=1$ and $\gcd(m_2,n_2)=1$. Furthermore, since they both represent $q$, we also know that $\frac{m_1}{n_1} = \frac{m_2}{n_2}$ and thus $m_1n_2=n_1m_2$.</p>
<p>Certainly this indicates that $m_1$ is a factor of $n_1m_2$. But $m_1$ and $n_1$ are coprime by hypothesis, so it must be the case that $m_1$ is a factor of $m_2$. Reversing the argument, we see that $m_2$ is a factor of $m_1$. This can only be true if $m_1=m_2$. The argument that $n_1=n_2$ is exactly analogous. It follows then that the canonical form is unique, as desired.</p>
</blockquote>
<p>So now we know that every rational number can be represented in a way that is in this sense &quot;most desirable.&quot; It certainly aligns with the simplification of fractions that we all saw in elementary school.</p>
<p>Now, remember that we already defined addition and multiplication of rational numbers in my previous post. Let's review those definitions here.</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>sum</strong> of two rational numbers $\frac{a}{b}$ and $\frac{c}{d}$ is the rational number $\frac{ad+bc}{bd}$.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> The <strong>product</strong> of two rational numbers $\frac{a}{b}$ and $\frac{c}{d}$ is the rational number $\frac{ac}{bd}$.</p>
</blockquote>
<p>It's worth mentioning that taking the sum or product of two fractions in canonical form does not always result in a canonical form fraction.</p>
<blockquote>
<p><strong>Example.</strong> The fraction $\frac{1}{2}$ is certainly in canonical form. However,</p>
<p>$$\begin{align}<br>
\frac{1}{2}+\frac{1}{2} &amp;= \frac{1\cdot 2 + 2\cdot 1}{2\cdot 2} \\<br>
&amp;= \frac{4}{4}.<br>
\end{align}$$</p>
<p>This is certainly not in canonical form, since $\gcd(4,4)=4$.</p>
<p>However, this is not really an issue since we can just rewrite it in canonical form post-computation. In this case, the canonical form for the result is $\frac{1}{1}$.</p>
</blockquote>
<h3 id="whereiszanamewhereisz">Where is $\Z$?<a name="where-is-z"></a></h3>
<p>Ordinarily, when we are not being as ridiculously pedantic as we are in this post, we would consider the integers to be a subset of the rational numbers. But technically this is not the case of our construction, since our rationals are equivalence classes of pairs of integers and thus not integers themselves. It's our goal, therefore, to identify a subset of rationals that looks and behaves like the integers.</p>
<p>The above example may have already given this away. Technically the rational number whose canonical form is $\frac{1}{1}$ is not the same thing as the integer $1$. But I mean, come on... they're pretty darn similar. This leads us to the following identification.</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>rational integers</strong> are the set of rational numbers whose canonical form is $\frac{n}{1}$, where $n$ is an integer. We refer to $\frac{n}{1}$ as the <strong>rational integer corresponding to $n$</strong>.</p>
</blockquote>
<p>I don't actually think &quot;rational integer&quot; is a phrase that anyone ever uses, but it will serve our purposes for this post to distinguish between an integer and its corresponding rational number.</p>
<p>Rational integers behave just as we would expect under addition and multiplication. That is, their sums and products are also rational integers. Moreover, they are the <em>correct</em> rational integers. Let's make this a bit more formal:</p>
<blockquote>
<p><strong>Proposition.</strong> If $m$ and $n$ are integers, then the sum of their corresponding rational integers is the rational integer corresponding to their sum. That is,</p>
<p>$$\frac{m}{1}+\frac{n}{1}=\frac{m+n}{1}.$$</p>
<p><strong>Proof.</strong> The result actually follows immediately from our definition of rational addition, since</p>
<p>$$\begin{align}<br>
\frac{m}{1}+\frac{n}{1} &amp;= \frac{m\cdot 1 + 1\cdot n}{1\cdot 1} \\<br>
&amp;= \frac{m+n}{1}.<br>
\end{align}$$</p>
</blockquote>
<p>We can do exactly the same sort of thing for multiplication:</p>
<blockquote>
<p><strong>Proposition.</strong> If $m$ and $n$ are integers, then the product of their corresponding rational integers is the rational integer corresponding to their product. That is,</p>
<p>$$\frac{m}{1}\cdot\frac{n}{1}=\frac{mn}{1}.$$</p>
<p><strong>Proof.</strong> Again, the result follows directly from our definition of rational multiplication. Note that</p>
<p>$$\begin{align}<br>
\frac{m}{1}\cdot\frac{n}{1} &amp;= \frac{m\cdot n}{1\cdot 1} \\<br>
&amp;= \frac{mn}{1}.<br>
\end{align}$$</p>
</blockquote>
<p>So these rational integers really do correspond to the integers in a one-to-one manner. Since there's no functional difference between rational integers and integers, we might as well just say they're the same thing.</p>
<h3 id="orderingtherationalsanameorderingtherationals">Ordering the Rationals<a name="ordering-the-rationals"></a></h3>
<p>One of the requirements we imposed on our construction last post was that the rationals should be ordered in a way that's compatible with the integers. That means that if $n$ and $m$ are integers with $n &lt; m$, then we definitely want it to be the case that $\frac{n}{1} &lt; \frac{m}{1}$. That is, integers and rational integers should have the same order.</p>
<p>But in addition to rational integers, all the other rational numbers should be comparable as well. This is done easily enough.</p>
<blockquote>
<p><strong>Definition.</strong> Given two rational numbers $\frac{a}{b}$ and $\frac{c}{d}$ in canonical form, their <strong>order</strong> is determined by saying that $\frac{a}{b} &lt; \frac{c}{d}$ if and only if $ad&lt;bc$.</p>
</blockquote>
<p>Just as a quick check that this definition of order makes any sense, let's look at a couple simple examples.</p>
<blockquote>
<p><strong>Example.</strong> Consider the fractions $\frac{1}{2}$ and $\frac{2}{3}$. These are already in canonical form, and we would certainly expect $\frac{1}{2}$ to be less than $\frac{2}{3}$. Let's check that this is indeed the case.</p>
<p>According to the definition, we just need to make sure that $1 \cdot 3 &lt; 2\cdot 2$. This is obviously true ($3&lt;4$) and so we're done.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> Consider the fractions $\frac{0}{1}$ and $\frac{-3}{2}$. Of course we hope it should be the case that $\frac{-3}{2}$ is less than $\frac{0}{1}$.</p>
<p>Again, plugging these straight into the definition, we see that</p>
<p>$$\begin{align}<br>
-3\cdot 1 &amp;= -3 \\<br>
&amp;&lt; 0 \\<br>
&amp;= 2\cdot 0.<br>
\end{align}$$</p>
</blockquote>
<p>These sanity checks indicate that we're on the right track with our definition of order. But we need to guarantee that our order extends that of the integers. Let's do that now.</p>
<blockquote>
<p><strong>Proposition.</strong> If $m$ and $n$ are integers with $m&lt;n$ then $\frac{m}{1} &lt; \frac{n}{1}$.</p>
<p><strong>Proof.</strong> There's really not much to prove. Since $m&lt;n$ we certainly have that $m\cdot 1 = 1\cdot n$. By the definition of order, this means that $\frac{m}{1} &lt; \frac{n}{1}$.</p>
</blockquote>
<p>So this order is compatible with the order on the integers. Yay! Now that our rational numbers are ordered, we're allowed to put them on the number line if we so choose.</p>
<h3 id="fillingthegapsanamefillingthegaps">Filling the Gaps<a name="filling-the-gaps"></a></h3>
<p>Our motivation for inventing rational numbers was to fill the two types of gaps we identified in the previous post as being missing from the integers. Namely, we required that our rational numbers satisfy the following properties:</p>
<blockquote>
<ul>
<li>If $a$ and $b$ are integers with $a\ne 0$, there exists a rational number $x$ for which $ax=b$.</li>
<li>If $p$ and $q$ are rational numbers with $p&lt;q$, there exists a rational number $x$ for which $p&lt;x&lt;q$.</li>
</ul>
</blockquote>
<p>It's actually pretty straightforward to show that our construction guarantees these properties. Let's get straight to work!</p>
<blockquote>
<p><strong>Proposition.</strong> If $a$ and $b$ are integers with $a\ne 0$, there exists a rational number $x$ for which $ax=b$.</p>
<p><strong>Proof.</strong> Technically we should consider the rational integers $\frac{a}{1}$ and $\frac{b}{1}$. We need to show that there is a rational number $\frac{p}{q}$ for which $\frac{a}{1}\cdot\frac{p}{q}=\frac{b}{1}$.</p>
<p>We argue that taking $p=b$ and $q=a$ will yield the desired result. That is, we only need to verify that $\frac{a}{1}\cdot\frac{b}{a}=\frac{b}{1}$. Notice that, from the definition of rational multiplication,</p>
<p>$$\begin{align}<br>
\frac{a}{1}\cdot\frac{b}{a} &amp;= \frac{ab}{1a} \\<br>
&amp;= \frac{ab}{a}.<br>
\end{align}$$</p>
<p>Since $\gcd(ab, a)=a$, the canonical form for the result is $\frac{b}{1}$, as desired.</p>
</blockquote>
<p>Neat! We plugged one type of gap and now have solutions to lots of equations. All that's left is to check that we've filled the other type of gap.</p>
<blockquote>
<p><strong>Proposition.</strong> If $p$ and $q$ are rational numbers with $p&lt;q$, there exists a rational number $x$ for which $p&lt;x&lt;q$.</p>
<p><strong>Proof.</strong> Suppose $p=\frac{p_1}{p_2}$ and $q=\frac{q_1}{q_2}$ are in canonical form. We argue that taking $x=\frac{p_1q_2 + p_2q_1}{2p_2q_2}$ will suffice. To see that we did not pull this out of thin air, notice that $x$ is really just the &quot;average&quot; of $p$ and $q$ put into fractional form, and thus we should expect it to sit directly between them on the number line.</p>
<p>We need to verify that $p&lt;x$ and that $x&lt;q$. To do so, we use the definition of order.</p>
<p>For the first inequality, note first that since $p&lt;q$, we have that $p_1q_2&lt;p_2q_1$. Thus,</p>
<p>$$\begin{align}<br>
(p_1)(2p_2q_2) &amp;= (p_2)(2p_1q_2) \\<br>
&amp;&lt; (p_2)(p_1q_2 + p_2q_1),<br>
\end{align}$$</p>
<p>It follows then that $\frac{p_1}{p_2} &lt; \frac{p_1q_2 + p_2q_1}{2p_2q_2}$. That is, $p&lt;x$.</p>
<p>The proof that $x&lt;q$ is completely analagous to the above.</p>
</blockquote>
<p>And there we have it. Our construction of the rational numbers satisfies all the properties we wanted it to!</p>
<p>So why did we do all this again? The short answer is because we can, and because it's kind of cool. The real answer is that we construct things in mathematics from the ground up so that</p>
<ol>
<li>We know that the objects we are working with actually exist.</li>
<li>We know exactly what properties our objects satisfy.</li>
</ol>
<p>I don't feel comfortable working with anything I can't get a feel for in this way.</p>
<h3 id="fieldaxiomsanamefieldaxioms">Field Axioms<a name="field-axioms"></a></h3>
<p>Our rational numbers actually have a few more properties than I've mentioned. Technically, they form what is called an ordered field. I won't prove all of the field axioms here, but from what I've done already and the properties of the integers, you should be able to fill in the gaps pretty easily now. I'll just give you the axioms and then call it quits.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>field</strong> is a set $\mathbb{F}$ equipped with two operations, addition $+$ and multiplication $\cdot$, which satisfy the following properties:</p>
<ol>
<li><strong>Associativity of Addition</strong>: For all $a,b,c\in\mathbb{F}$, $a+(b+c)=(a+b)+c$.</li>
<li><strong>Commutativity of Addition</strong>: For all $a,b\in\mathbb{F}$, $a+b=b+a$.</li>
<li><strong>Additive Identity</strong>: There exists $0\in\mathbb{F}$ for which $0+a=a$ for any $a\in\mathbb{F}$.</li>
<li><strong>Additive Inverses</strong>: For all $a\in\mathbb{F}$, there exists $-a\in\mathbb{F}$ for which $a+(-a)=0$.</li>
<li><strong>Associativity of Multiplication</strong>: For all $a,b,c\in\mathbb{F}$, $a\cdot (b\cdot c)=(a\cdot b)\cdot c$.</li>
<li><strong>Commutativity of Multiplication</strong>: For all $a,b\in\mathbb{F}$, $a\cdot b=b\cdot a$.</li>
<li><strong>Multiplicative Identity</strong>: There exists $1\in\mathbb{F}$ with $1\ne 0$ for which $1\cdot a=a$ for any $a\in\mathbb{F}$.</li>
<li><strong>Multiplicative Inverses</strong>: For all $a\in\mathbb{F}$ with $a\ne 0$, there exists $a^{-1}\in\mathbb{F}$ for which $a\cdot a^{-1}=1$.</li>
<li><strong>Distributivity of Multiplication over Addition</strong>: For all $a,b,c\in\mathbb{F}$, $a\cdot (b+c) = a\cdot b+a\cdot c$.</li>
</ol>
<p>Furthermore, $\mathbb{F}$ is an <strong>ordered field</strong> if there is an order $&lt;$ on $\mathbb{F}$ which is compatible with the field structure in the following sense: If $a,b\in\mathbb{F}$ and $a,b\ge 0$, then $a+b\ge 0$ and $a\cdot b\ge 0$.</p>
</blockquote>
<p>That's pretty much it. It's not too hard to show that the rationals $\Q$ form an ordered field under the definitions we gave for addition and multiplication. The additive identity is $\frac{0}{1}$, the multiplicative identity is $\frac{1}{1}$, the additive inverse of $\frac{a}{b}$ is $\frac{-a}{b}$, and the multiplicative inverse of $\frac{a}{b}$ is $\frac{b}{a}$ (provided $a\ne 0$).</p>
<p>The integers $\Z$ do <em>not</em> form an ordered field because they are lacking multiplicative inverses. This is precisely the first &quot;gap&quot; that we filled.</p>
<p>So... yeah.</p>
]]></content:encoded></item></channel></rss>