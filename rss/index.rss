<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>Algebrology</title><description>A gentle introduction to insanity.</description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>Algebrology</title><link>http://localhost:2368/</link></image><generator>Ghost 2.14</generator><lastBuildDate>Sun, 10 Mar 2019 05:38:04 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>Normal Subgroups and Quotient Groups</title><description>Let's now revisit the quotient set $G/H$, where $H$ is a subgroup of $G$. What we'd really like to do is turn $G/H$ into a group in a meaningful way. What should the group operation be, though?</description><link>http://localhost:2368/normal-subgroups-and-quotient-groups/</link><guid isPermaLink="false">5c7f40876fe30a003ed53402</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Wed, 06 Mar 2019 04:52:39 GMT</pubDate><content:encoded>&lt;ol&gt;
&lt;li&gt;&lt;a href="#normal-subgroups"&gt;Normal Subgroups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#quotient-groups"&gt;Quotient Groups&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="normalsubgroupsanamenormalsubgroups"&gt;Normal Subgroups&lt;a name="normal-subgroups"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Let's now revisit the quotient set $G/H$, where $H$ is a subgroup of $G$. What we'd really like to do is turn $G/H$ into a group in a meaningful way. What should the group operation be, though? Recall that the elements of $G/H$ are the cosets of $H$, so what we really need to define is a meaningful way to multiply cosets.&lt;/p&gt;
&lt;p&gt;It would be great if we could define coset multiplication as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Desired Definition.&lt;/strong&gt; If $H$ is a subgroup of $G$ and $x,y\in G$, then&lt;br&gt;
$$(Hx)(Hy)=H(xy).$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Unfortunately, this is not always uniquely defined. That is, if $Hx=Ha$ and $Hy=Hb$ for some $a,b\in G$, it is possible that $H(xy)\ne H(ab)$, which means this isn't an acceptable way to multiply cosets. This is the same behavior we saw when I proposed an incorrect definition of addition while &lt;a href="http://localhost:2368/constructing-the-rational-numbers-1/#construction"&gt;constructing the rational numbers&lt;/a&gt;. The result cannot depend on our choice of representative for the equivalence class!&lt;/p&gt;
&lt;p&gt;Interestingly, in this case the remedy is not to define coset multiplication in a different manner. Rather, we choose to restrict our attention only to cosets of a certain, particularly nice sort of subgroup.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A subgroup $N$ of a group $G$ is a &lt;strong&gt;normal subgroup&lt;/strong&gt; if $xnx^{-1}\in N$ whenever $n\in N$ and $x\in G$. We refer to this defining property of normal subgroups by saying they are &lt;strong&gt;closed under conjugation&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It goes without saying that every subgroup of an abelian group is normal, since in that case&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
xnx^{-1} &amp;amp;= xx^{-1}n \\&lt;br&gt;
&amp;amp;= n,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;which is in $N$ by definition. However, there are certainly non-abelian groups with normal subgroups. And normal subgroups are particularly special because their left and right cosets are the same, even if they are not subgroups of an abelian group!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $N$ is a normal subgroup of a group $G$, then $xN=Nx$ for every $x\in G$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; For any $g\in xN$, we have that $g=xn$ for some $n\in N$. Since $N$ is closed under conjugation, $xnx^{-1}\in N$ and so $g=(xnx^{-1}x)\in Nx$. Thus, $xN\subseteq Nx$.&lt;/p&gt;
&lt;p&gt;The proof that $Nx\subseteq xN$ is nearly identical, so I will not include it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In a recent post I showed that the kernel of a homomorphism is a always a subgroup of its domain. It turns out that they are actually &lt;em&gt;normal&lt;/em&gt; subgroups, and this will be very important to us later on.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; The kernel of any group homomorphism is a normal subgroup of the domain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Let $G$ and $H$ be groups and let $f:G\to H$ denote a homomorphism between them. Suppose $x\in G$ and $k\in\ker f$. We need to show that $xkx^{-1}\in\ker f$.&lt;/p&gt;
&lt;p&gt;Since $k\in\ker f$, we have by definition that $f(k)=e$. Furthermore, since $f$ is a homomorphism,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f(xkx^{-1}) &amp;amp;= f(x)f(k)f(x^{-1}) \\&lt;br&gt;
&amp;amp;= f(x)ef(x)^{-1} \\&lt;br&gt;
&amp;amp;= f(x)f(x)^{-1} \\&lt;br&gt;
&amp;amp;= e.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Thus, $xkx^{-1}$ is in $\ker f$ by definition, so $\ker f$ is a normal subgroup of $G$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="quotientgroupsanamequotientgroups"&gt;Quotient Groups&lt;a name="quotient-groups"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We are now ready to define quotient &lt;em&gt;groups&lt;/em&gt;!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $N$ denote a normal subgroup of a group $G$. The &lt;strong&gt;quotient group of $G$ modulo $N$&lt;/strong&gt; is the set $G/N$ together with coset multiplication defined by $(Nx)(Ny)=N(xy)$ for all $x,y\in G$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's all well and good, but we're getting ahead of ourselves again. We still haven't actually shown that this multiplication is well defined when restricted to cosets of normal subgroups. Let's do that right now. After that, we'll still need to prove that this thing we'ved defined is really a group.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $N$ denote a normal subgroup of a group $G$, with $Nx=Na$ and $Ny=Nb$ for some $a,b,x,y\in G$. Then $N(xy)=N(ab)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $Nx=Na$, we know that $x\in Na$ and thus $x=n_1a$ for some $n_1\in N$. Similarly, since $Ny=Nb$, we know that $y\in Nb$ and thus $y=n_2b$ for some $n_2\in N$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
xy &amp;amp;= n_1an_2b \\&lt;br&gt;
&amp;amp;\in NaNb \\&lt;br&gt;
&amp;amp;= N(ab),&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;so $N(xy)=N(ab)$, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now we need to show that quotient groups are actually groups. The proof of this is fairly straightforward.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; The quotient group as defined above is in fact a group.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We have already shown that coset multiplication is well defined. We will show first that it is associative. Consider $Nx, Ny, Nz\in G/N$. By definition,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
Nx(NyNz) &amp;amp;= NxN(yz) \\&lt;br&gt;
&amp;amp;= N(xyz) \\&lt;br&gt;
&amp;amp;= N(xy)Nz \\&lt;br&gt;
&amp;amp;= (NxNy)Nz.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Next, we will show that $N$ is the identity element of $G/N$. Because $N$ is a subgroup of $G$, the identity element $e\in G$ is in $N$. Thus, for any $Nx\in G/N$, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
NNx &amp;amp;= NeNx \\&lt;br&gt;
&amp;amp;= N(ex) \\&lt;br&gt;
&amp;amp;= Nx \\&lt;br&gt;
&amp;amp;= N(xe) \\&lt;br&gt;
&amp;amp;= NxNe \\&lt;br&gt;
&amp;amp;= NxN.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Finally, we will show that for any $Nx\in G/N$, the coset $Nx^{-1}$ is its inverse. This is clear because&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
NxNx^{-1} &amp;amp;= N(xx^{-1}) \\&lt;br&gt;
&amp;amp;= Ne \\&lt;br&gt;
&amp;amp;= N(x^{-1}x) \\&lt;br&gt;
&amp;amp;= Nx^{-1}Nx.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;We have shown that $G/N$ has all the properties of a group, so the proof is complete.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Before moving on, let's look at a concrete example of a quotient group which is hopefully already familiar to you.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider again the group $\Z$ of integers under addition and its subgroup $2\Z$ of even integers. Certainly $2\Z$ is a normal subgroup because $\Z$ is abelian, and we may thus form the quotient group $\Z/2\Z$. Recall that this quotient group contains only two cosets, namely $2\Z$ and $2\Z+1$.&lt;/p&gt;
&lt;p&gt;Coset &amp;quot;multiplication&amp;quot; here is really coset addition because we are working in an additive group. Since $2\Z$ is the identity, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
2\Z+2\Z &amp;amp;= 2\Z, \\&lt;br&gt;
2\Z + (2\Z+1) &amp;amp;= 2\Z+1,\\&lt;br&gt;
(2\Z+1) + 2\Z &amp;amp;= 2\Z+1, \\&lt;br&gt;
(2\Z+1) + (2\Z+1) &amp;amp;= 2\Z.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Let's rename these cosets, just for kicks. We'll refer to $2\Z$ as $0$ and $2\Z+1$ as $1$. Then we get&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
0 + 0 &amp;amp;= 0, \\&lt;br&gt;
0 + 1 &amp;amp;= 1,\\&lt;br&gt;
1 + 0 &amp;amp;= 1, \\&lt;br&gt;
1 + 1 &amp;amp;= 0.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;This is precisely the group $\Z_2$ of integers modulo $2$, with the operation of addition modulo $2$. In fact, the formal way to define $\Z_n$, the group of integers modulo $n$, is as the quotient group $\Z/n\Z$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now I'd like to give some motivation for why quotient groups are so important and useful. This is best done by example, because otherwise my explanation would likely turn into another incomprehensible rant.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; To start, we define a &lt;strong&gt;commutator&lt;/strong&gt; of a group $G$ to be any element of the form $aba^{-1}b^{-1}$, where $a,b\in G$. Notice that $aba^{-1}b^{-1}$ reduces to the identity $e$ if and only if $ab=ba$. That is, the commutator $aba^{-1}b^{-1}$ collapses to the identity precisely when $a$ and $b$ commute. Clearly in any abelian group, every commutator is equal to the identity element.&lt;/p&gt;
&lt;p&gt;Suppose a quotient group $G/N$ is abelian. That is, for any two cosets $Nx, Ny\in G/N$, their product commutes, i.e.,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
NxNy &amp;amp;= N(xy) \\&lt;br&gt;
&amp;amp;= N(yx) \\&lt;br&gt;
&amp;amp;= NyNx.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;But this is true if and only if&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
xy(yx)^{-1} &amp;amp;= xyx^{-1}y^{-1} \\&lt;br&gt;
&amp;amp;\in N.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Thus, a quotient group is abelian precisely when all commutators are contained within the identity coset!&lt;/p&gt;
&lt;p&gt;A slightly more liberal, though meaningful, way of phrasing this is by saying that if we &lt;em&gt;factor out&lt;/em&gt; all the commutators of $G$, we are always left with an abelian group. This because all commutators are collapsed to the identity element in the quotient group when we quotient out the commutator subgroup. This is just one example of a more general phenomenon.&lt;/p&gt;
&lt;/blockquote&gt;
</content:encoded></item><item><title>Cosets and Lagrange's Theorem</title><description>It's a bit difficult to explain exactly why cosets are so important without working with them for a while first. But as you'll hopefully start to understand within my next few posts, cosets pop up everywhere and are a necessary tool to get anything done in the world of algebra.</description><link>http://localhost:2368/cosets-and-lagranges-theorem/</link><guid isPermaLink="false">5c7df11d6fe30a003ed533c6</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Tue, 05 Mar 2019 03:53:03 GMT</pubDate><content:encoded>&lt;ol&gt;
&lt;li&gt;&lt;a href="#cosets"&gt;Cosets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lagrange's-theorem"&gt;Lagrange's Theorem&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="cosetsanamecosets"&gt;Cosets&lt;a name="cosets"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;It's a bit difficult to explain exactly why cosets are so important without working with them for a while first. But as you'll hopefully start to understand within my next few posts, cosets pop up everywhere and are a necessary tool to get anything done in the world of algebra. Let's dig in, shall we?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $H$ denote a subgoup of a group $G$. For any fixed element $x\in G$, the &lt;strong&gt;right coset&lt;/strong&gt; of $H$ with respect to $x$ is the set $Hx=\{hx\in G\mid h\in H\}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $H$ denote a subgoup of a group $G$. For any fixed element $x\in G$, the &lt;strong&gt;left coset&lt;/strong&gt; of $H$ with respect to $x$ is the set $xH=\{xh\in G\mid h\in H\}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note.&lt;/strong&gt; For abelian groups with additive notation, right and left cosets are instead denoted by $H+x$ and $x+H$, respectively.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So basically a coset is a set obtained by taking the elements of a subgroup and adding a particular element to all of them. Note that a coset does need not be a subgroup! To get a feeling for what cosets really are and how they behave, let's look at an example.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Take $G=\Z$, the additive group of integers, and $H=2\Z$, the set of even integers. Certainly $2\Z$ is a subgroup of $Z$ because it contains the identity $0$, the sum of two even integers is even, and every even integer has an even inverse — its negative.&lt;/p&gt;
&lt;p&gt;Let's look at a few cosets of $2\Z$. How about the right cosets with respect to $0,1,2$ and $3$? Notice that because $\Z$ is an additive group, these cosets will be written $2\Z+0$, $2\Z+1$, $2\Z+2$ and $2\Z+3$.&lt;/p&gt;
&lt;p&gt;Directly from the definition of a right coset, we see that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
2\Z+0 &amp;amp;= \{x+0\in\Z\mid x\in 2\Z\} \\&lt;br&gt;
&amp;amp;= \{x\in\Z\mid x\in 2\Z\} \\&lt;br&gt;
&amp;amp;= 2\Z.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;So in this case, the coset with respect to $0$ is just the subgroup $2\Z$. This actually always happens, as you can easily see. Adding the identity to all elements of a subgroup will of course just yield that subgroup again!&lt;/p&gt;
&lt;p&gt;Next, let's look at $2\Z+1$. This is the set $\{x+1\in\Z\mid x\in 2\Z\}$, which consists of things like $\ldots,-3,-1,1,3,5,\ldots$. This is really just the set of odd integers! It's definitely not a subgroup of $\Z$ though, since it doesn't contain $0$.&lt;/p&gt;
&lt;p&gt;The coset $2\Z+2$ is the set $\{x+2\in\Z\mid z\in 2\Z\}$, which contains elements like $\ldots,-4,-2,0,2,4,\ldots$. But this is the set of even integers again! That means $2\Z+2=2\Z$.&lt;/p&gt;
&lt;p&gt;Lastly, let's look at $2\Z+2$. This is the set $\{x+3\in\Z\mid z\in 2\Z\}$, which contains things like $\ldots,-3,-1,1,3,5,\ldots$. We've already seen that somewhere before. It's just the set of odd integers again. That is, $2\Z+3=2\Z+1$.&lt;/p&gt;
&lt;p&gt;It looks like there might actually only be two distinct cosets of $2\Z$. They are $2\Z$ itself and $2\Z+1$. Furthermore, every element of $\Z$ is in either one coset or the other, but never both (because an integer is either even or odd). So these cosets actually &lt;em&gt;partition&lt;/em&gt; $\Z$, which is a very important point. But let's not get too far ahead of ourselves.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The first thing I'd like to prove about cosets is fairly simple — if we are working in an abelian group, left and right cosets are the same!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $H$ is a subgroup of an abelian group $G$, then $H+x=x+H$ for every $x\in G$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We will proceed by demonstrating that each side is a subset of the other.&lt;/p&gt;
&lt;p&gt;We show first that $H+x\subseteq x+H$. Choose $g\in H+x$, so that $g=h+x$ for some $h\in H$. Since $G$ is abelian, $h+x=x+h$ and thus $g=x+h\in x+H$. It follows that $H+x\subseteq x+H$.&lt;/p&gt;
&lt;p&gt;The proof that $x+H\subseteq H+x$ is completely analogous to the above, so we won't bother with it. We can thus conclude that $H+x=x+H$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This implies, for instance, that we could instead write $1+2\Z$ to denote the odd integers, but to me this doesn't look right for some reason and so I usually don't. In fact, for most of our purposes we really only need to consider one variety of coset. I tend to favor right cosets.&lt;/p&gt;
&lt;p&gt;Now it's time to prove my suspicion from the above example. This is important, so pay close attention. If you don't remember what a partition is, I advise you to read &lt;a href="http://localhost:2368/equivalence-relations-and-quotient-sets/"&gt;my earlier post about them&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $H$ is a subgroup of a group $G$, then the (left/right) cosets of $H$ partition $G$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We will prove the result for right cosets, since the proof for left cosets is practically identical.&lt;/p&gt;
&lt;p&gt;It is clear that every coset $Hx$ is nonempty because the identity element $e$ is in $H$ by virtue of it being a subgroup, and thus $x=ex\in Hx$.&lt;/p&gt;
&lt;p&gt;The next thing we need to show is that cosets cover all of $G$. But is clear that $$\bigcup_{x\in G}x=G\subseteq\bigcup_{x\in G}Hx,$$&lt;/p&gt;
&lt;p&gt;and similarly that $$\bigcup_{x\in G}Hx\subseteq G=\bigcup_{x\in G}x,$$&lt;/p&gt;
&lt;p&gt;because $Hx\subseteq G$ for every $x\in G$. Thus, $G=\bigcup_{x\in G}Hx$.&lt;/p&gt;
&lt;p&gt;The last thing we need to show is that for any $x,y\in G$, if $Hx\ne Hy$ then $Hx\cap Hy=\varnothing$. That is, either cosets are the same or they are disjoint. We will argue the contrapositve, supposing that $Hx\cap Hy$ is nonempty. Then there exists some element $a\in Hx\cap Hy$. That is, $a\in Hx$ and $a\in Hy$, so $a=h_1x$ and $a=h_2y$ for some elements $h_1,h_2\in H$. It follows that $h_1x=h_2y$, and multiplication on the left by $h_1^{-1}$ show that $x=h_1^{-1}h_2y\in Hy$ because $h_1^{-1}h_2\in H$. Thus $Hx=Hy$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is pretty exciting! It means that we can form an equivalence relation $\sim$ on any group $G$, where elements are equivalent if they are in the same coset of $H$. Moreover, we can form the quotient set $G\quotient{\sim}$, whose elements are precisely the cosets of $H$. For simplicity, we generally write $G/H$ instead of to denote this quotient set, since it is so very important that we are talking about the set of cosets of $H$. We'll revisit this special type of quotient set in my next post.&lt;/p&gt;
&lt;h3 id="lagrangestheoremanamelagrangestheorem"&gt;Lagrange's Theorem&lt;a name="lagrange's-theorem"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We're actually very close to proving this famous theorem already. We just need one lemma first:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma.&lt;/strong&gt; If $H$ is a subgroup of a group $G$ and $x\in G$, there exists a bijection $f:G\to Hx$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We define a function $f:H\to Hx$ by $f(h)=hx$ for every $h\in H$. That is, the function that maps every element of $H$ to its product with $x$. Clearly $f$ is injective because if $f(h_1)=f(h_2)$ then $h_1x=h2_x$ and the right cancellation law yields $h_1=h_2$. Furthermore, $f$ is surjective because for any $y\in Hx$ we have by definition that $y=hx$ for some $h\in H$, and thus $y=f(h)$. It follows that $f$ is bijective, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Recall that a bijection exists between finite sets only if those sets contain the same number of elements. This fact, along with the lemma above, tells us that all cosets of a subgroup are the same size! We are now ready to prove Lagrange's Theorem, which is actually very easy at this point.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lagrange's Theorem.&lt;/strong&gt; If $H$ is a subgroup of a finite group $G$, then $\abs{G}$ is a multiple of $\abs{H}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We have already established that every coset of $H$ contains the same number of elements, $\abs{H}$. Since $G$ is finite, there are a finite number of distinct cosets of $H$, say $n$ of them. Because these cosets partition $G$, it follows that $\abs{G}=n\abs{H}$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is probably somewhat surprising unless you're already familiar with groups. But it's just part of the astonishing usefulness of cosets. Lagrange's Theorem is actually incredibly useful because it tells us instantly that certain things &lt;em&gt;cannot&lt;/em&gt; be subgroups of other things. For instance, a group of order $12$ cannot ever have subgroups of order $5,7,8,9,10$ or $11$. Without this theorem, you might have guessed that this was the case, but it would have been pretty tricky to prove it conclusively.&lt;/p&gt;
</content:encoded></item><item><title>Group Homomorphisms</title><description>A recurring theme in mathematics is that examining the maps between objects is indispensable to understanding those objects themselves. Of course, that depends on choosing the "correct" type of maps.</description><link>http://localhost:2368/group-homomorphisms/</link><guid isPermaLink="false">5c7b67366fe30a003ed53390</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Sun, 03 Mar 2019 05:37:37 GMT</pubDate><content:encoded>&lt;ol&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#homomorphisms"&gt;Homomorphisms&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="introductionanameintroduction"&gt;Introduction&lt;a name="introduction"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A long time ago I wrote &lt;a href="http://localhost:2368/groups-and-their-basic-properties/"&gt;my first post about group theory&lt;/a&gt;, in which I defined groups and subgroups, gave several examples of groups and proved a few of their basic but important properties. Hopefully at the very least you remember that a group is a set with an associative binary operation, an identity element and inverses for all its elements. The identity, as well as each element's invers, is unique and we can cancel like terms from both sides of equations like we generally do without thinking anyway.&lt;/p&gt;
&lt;p&gt;Before I dive in, there are just a few quick things we will need before we can talk about homomorphisms. The first is an easy way to identify whether a subset of a group is actually a subgroup.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma.&lt;/strong&gt; A subset $H$ of a group $G$ is a subgroup of $G$ if it is nonempty and if $ab^{-1}\in H$ whenever $a,b\in H$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since the operation on $H$ is inherited from $G$, it is clearly associative. Next, since $H$ is nonempty there exists $x\in H$, and thus by hypothesis $xx^{-1}=e\in H$, where $e$ is the identity element. Furthermore, since $e,x\in H$, we have that $ex^{-1}=x^{-1}\in H$, so $H$ contains inverses for all its elements. Finally, for any $a,b\in H$ we have established that $b^{-1}\in H$. So again by hypothesis, $a(b^{-1})^{-1}=ab\in H$, and so $H$ is closed under products. We have shown that $H$ is a group, and thus it is a subgroup of $G$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Notice that when I say something is &amp;quot;closed under products,&amp;quot; what this really means is that the group operation is a well defined binary operation. For multiplicative groups, the word &amp;quot;product&amp;quot; fits nicely. For abelian groups with additive notation, it doesn't make quite as much sense but I will probably slip up and use it anyway.&lt;/p&gt;
&lt;p&gt;For the most part, this lemma will simply make it a little bit faster to verify that certain things are subgroups of other groups. This will help keep future proofs nice and short.&lt;/p&gt;
&lt;p&gt;The next thing I should mention is the concept of a group's order. Because I have not talked about cardinality anywhere on my blog yet, I will only define order for finite groups. Luckily, this is all we'll need for a while.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;order&lt;/strong&gt; of a finite group is the number of elements that group contains. We denote the order of a group $G$ by $\abs{G}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's a pretty simple definition. The trivial group contains only the identity, so it is a group of order one. The dihedral group of the regular $n$-gon, written $D_n$ and which I talked about in my first post on groups, has $2n$ elements and thus $\abs{D_n}=2n$. There are an abundance of examples of interesting finite groups, and the order of a finite group is really just its size.&lt;/p&gt;
&lt;h3 id="homomorphismsanamehomomorphisms"&gt;Homomorphisms&lt;a name="homomorphisms"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A recurring theme in mathematics is that examining the maps between objects is indispensable to understanding those objects themselves. Of course, that depends on choosing the &amp;quot;correct&amp;quot; type of maps. For topological spaces, it is the continuous maps that help us to understand their structure. For groups, we have the following analogous idea.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;group homomorphism&lt;/strong&gt; between groups $G$ and $H$ is a function $f:G\to H$ such that $f(x)f(y)=f(xy)$ for all $x,y\in G$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When it is understood that I am talking about groups, I will often refer to these things simply as homomorphisms, rather than group homomorphisms.&lt;/p&gt;
&lt;p&gt;The first thing I want to stress is that &lt;em&gt;homomorphism&lt;/em&gt; and &lt;em&gt;homeomorphism&lt;/em&gt; are different words and different concepts. Homomorphisms are functions that preserve group multiplication, whereas homeomorphisms preserve open sets in topological spaces.&lt;/p&gt;
&lt;p&gt;The seconds thing I need to mention is that in the above definition, the multiplication of $f(x)$ and $f(y)$ on the left side of the equation takes place in the group $H$, whereas the multiplication of $x$ and $y$ on the right takes place in $G$. That is to say, homomorphisms are precisely the maps for which is does not matter whether we multiply elements before or after applying the map.&lt;/p&gt;
&lt;p&gt;To put it another way, we can multiply elements in $G$ and take the image of their product, or we can take their images first and then multiply them in $H$. Either way, we always get the same answer. This means that the group operation is preserved by homomorphisms. And that means that much of the group's structure is also preserved. There is, in fact, a special type of homomorphism which perfectly preserves a group's structure.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;group isomorphism&lt;/strong&gt; between groups $G$ and $H$ is a bijective homomorphism $f:G\to H$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Two groups are &lt;strong&gt;isomorphic&lt;/strong&gt; if there exists a group isomorphism between them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Isomorphisms are the nicest sort of map between groups. Since they are bijective, they map each group element to precisely one element in the other group. Since they are homomorphisms, they preserve the group operation. This means that an isomorphism is essentially just a way of renaming the elements of a group. They do not alter the way that elements interaction with each other. Perhaps an example will help get my point across.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The group $(\R, +)$ of real numbers under addition and the group $(\R^+, \cdot)$ of positive real numbers under multiplication are isomorphic. We can establish this by exhibiting an isomorphism between them.&lt;/p&gt;
&lt;p&gt;Let's define the function $f:\R^+\to\R$ by $f(x)=\log x$. We will argue that $f$ is an isomorphism. First, it is clearly a homomorphism because from the basic properties of the logarithm,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f(x) + f(y) &amp;amp;= \log x +\log y \\&lt;br&gt;
&amp;amp;= \log(x \cdot y) \\&lt;br&gt;
&amp;amp;= f(x \cdot y).&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;In addition, we know that $f$ is bijective because it has an inverse function, the exponential map $f^{-1}:\R\to\R^+$ given explicitly by $f^{-1}(x)=e^x$.&lt;/p&gt;
&lt;p&gt;What does it really mean when we say that these groups are isomorphic? It isn't just some meaningless abstract statement that these groups are &amp;quot;essentially&amp;quot; the same. It actually means that we can do algebra in one and then easily transfer it over to the other group! That is, we can do multiplication in $\R^+$ simply by doing the corresponding addition in $\R$ and then applying the logarithm. This is the principle that slide rules are based on. Addition is easier than multiplication, so we add our numbers by hand and then use the magic slide rule isomorphism machine to take us between groups and into the world of multiplication.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Isomorphisms may be useful in certain cases, but they are actually a bit boring precisely &lt;em&gt;because&lt;/em&gt; they perfectly preserve structure. After all, how interesting is a flat renaming of group elements? We might as well just call two isomorphic groups the same. General homomorphisms are actually much more interesting.&lt;/p&gt;
&lt;p&gt;It may be enlightening to see an example of a homomorphism that is not an isomorphism, but is still surprisingly structure-preserving.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the group $\Z$ of integers under addition and the group $\{-1,1\}$ under multiplication. In $\{-1,1\}$ we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
1\cdot 1 &amp;amp;= 1, \\&lt;br&gt;
-1\cdot 1 &amp;amp;= -1. \\&lt;br&gt;
1\cdot -1 &amp;amp;= -1, \\&lt;br&gt;
-1\cdot -1 &amp;amp;= 1,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;So clearly the identity element in $\{-1,1\}$ is $1$ and every element is its own inverse. Let's construct a function $f:\Z\to\{-1,1\}$ defined by&lt;/p&gt;
&lt;p&gt;$$f(x) =&lt;br&gt;
\begin{cases}&lt;br&gt;
1 &amp;amp; \text{if } x\in 2\Z, \\&lt;br&gt;
-1 &amp;amp; \text{if } x\in 2\Z+1.&lt;br&gt;
\end{cases}$$&lt;/p&gt;
&lt;p&gt;(Here $2\Z$ is the set of even integers and $2\Z+1$ is the set of odd integers. My reason for denoting them this way will become apparent once we talk about cosets in a later post.)&lt;/p&gt;
&lt;p&gt;Let's show that this function is a homomorphism. Choose $m,n\in\Z$. There are three possible cases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Both $m$ and $n$ are even. That is, $m,n\in 2\Z$.&lt;/li&gt;
&lt;li&gt;Both $m$ and $n$ are odd. That is, $m,n\in 2\Z+1$.&lt;/li&gt;
&lt;li&gt;Either $m$ is even and $n$ is odd or vice versa. Because both groups we're considering are abelian, it suffices to consider only one of these cases, $m\in 2\Z$ and $n\in 2\Z+1$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We examine case 1 first. Suppose $m,n\in 2\Z$, so that $f(m)=f(n)=1$. Then $m=2k_1$ and $n=2k_2$ for some $k_1,k_2\in\Z$. This means that $$m+n=2k_1+2k_2=2(k_1+k_2)\in 2\Z.$$ Thus, $$f(m)f(n)=1\cdot 1=1=f(m+n).$$&lt;/p&gt;
&lt;p&gt;Next, we look at case 2. Suppose that $m,n\in 2\Z+1$, so that $f(m)=f(n)=-1$. Then $m=2k_1+1$ and $n=2k_2+1$ for some $k_1,k_2\in\Z$. This means that $$m+n=(2k_1+1)+(2k_2+1)=2k_1+2k_2+2=2(k_1+k_2+1)\in 2\Z.$$ Thus, $$f(m)f(n)=-1\cdot -1=1=f(m+n).$$&lt;/p&gt;
&lt;p&gt;Lastly, we examine case 3. Suppose that $m\in 2\Z$ but $n\in 2\Z+1$, so that $f(m)=1$ and $f(n)=-1$. Then $m=2k_1$ and $n=2k_2+1$ for some $k_1,k_2\in\Z$. This means that $$m+n=2k_1+2k_2+1=2(k_1+k_2)+1\in 2\Z+1.$$ Thus, $$f(m)f(n)=1\cdot -1=-1=f(m+n).$$&lt;/p&gt;
&lt;p&gt;Since we have now checked that $f(m)f(n)=f(m+n)$ for all $m,n\in\Z$, it follows that $f$ is a homomorphism.&lt;/p&gt;
&lt;p&gt;That was really us showing what we already know — that the sum of two even numbers is even, the sum of two odd numbers is also even, and the sum of an even and an odd number is odd. The property of being even or odd is sometimes called &lt;strong&gt;parity&lt;/strong&gt;, and this homomorphism preseves the parity of integers, as well as what happens to the parity of integers after you add them together. Actually, parity is basically all that is preserved by this homomorphism, but it's still a nice example of how homomorphisms can preserve important aspects of a group's structure.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now for some basic properties of homomorphisms. I promise these next two proofs are easy and short. The first property is that homomorphisms always map the identity of one group to the identity of the other. If we were being very careful, we would represent the identity of $G$ by $e_G$ and the identity in $H$ by $e_H$. However, it is common to be a bit sloppy and use $e$ to represent the identity element in both groups. If proper care is taken, this should never result in any confusion.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $f:G\to H$ is a group homomorphism then $f(e)=e$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $f$ is a homomorphism, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f(e)f(e) &amp;amp;= f(ee) \\&lt;br&gt;
&amp;amp;= f(e) \\&lt;br&gt;
&amp;amp;= f(e)e.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Thus, cancellation on the left yields the equality $f(e)=e$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The next property is just as straightforward — homomorphisms always map inverses to inverses.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $f:G\to H$ is a group homomorphism then $f(x^{-1})=f(x)^{-1}$ for every $x\in G$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $f$ is a homomorphism, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f(x)f(x^{-1}) &amp;amp;= f(xx^{-1}) \\&lt;br&gt;
&amp;amp;= f(e) \\&lt;br&gt;
&amp;amp;= e.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Multiplication on the left by $f(x)^{-1}$ on both sides yields $f(x^{-1})=f(x)^{-1}$, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Both of these properties only serve to reinforce my claim that homomorphisms are structure-preserving maps. I'll shut my smug face now and define two of the most important things ever.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;kernel&lt;/strong&gt; of a group homomorphism $f:G\to H$ is the set $$\ker f=\{x\in G\mid f(x)=e\}.$$ That is, it is the set of elements in $G$ that get mapped to the identity in $H$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;image&lt;/strong&gt; of a group homomorphism $f:G\to H$ is the set $$\im f=f[G]=\{f(x)\in H\mid x\in G\}.$$ That is, it is the set of elements in $H$ that get mapped to by some element in $G$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If $f:G\to H$ is a group homomorphism and $f$ is surjective, it should be clear that $H=\im f$. Similarly, if $f$ is injective then $\ker f = \{e\}$. These facts, along with the next two theorems, are crucial to everything we will do with groups in the future.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $f:G\to H$ is a group homomorphism then its kernel is a subgroup of $G$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; From the lemma at the beginning of this post, it suffices to show that $\ker f$ is nonempty and that $ab^{-1}\in \ker f$ whenever $a,b\in\ker f$.&lt;/p&gt;
&lt;p&gt;The fact that $\ker f$ is nonempty follows immediately from the property that $f(e)=e$, so certainly $e\in\ker f$.&lt;/p&gt;
&lt;p&gt;Next, suppose that $a,b\in\ker f$. That is, $f(a)=f(b)=e$. Then&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f(ab^{-1}) &amp;amp;= f(a)f(b^{-1}) \\&lt;br&gt;
&amp;amp;= f(a)f(b)^{-1}) \\&lt;br&gt;
&amp;amp;= ee^{-1} \\&lt;br&gt;
&amp;amp;= e,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;and thus $ab^{-1}\in\ker f$ by definition. It follows that $\ker f$ is a subgroup of $G$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Along similar lines:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $f:G\to H$ is a group homomorphism then its image is a subgroup of $H$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We will employ the same technique as in the previous proof.&lt;/p&gt;
&lt;p&gt;Certainly $\im f$ is nonempty because $e\in G$ and thus $f(e)=e\in\im g$.&lt;/p&gt;
&lt;p&gt;Next, suppose that $a,b\in\in f$. That is, $a=f(x)$ and $b=f(y)$ for some $x,y\in G$. Certainly $y^{-1}\in G$ and thus $xy^{-1}\in G$ as well, so&lt;br&gt;
$$\begin{align}&lt;br&gt;
ab^{-1} &amp;amp;= f(x)f(y)^{-1} \\&lt;br&gt;
&amp;amp;= f(xy^{-1}) \\&lt;br&gt;
&amp;amp;\in \im f,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In my next few posts, you will begin to understand why it's such a big deal that the kernal and image of a homomorphism are subgroups.&lt;/p&gt;
</content:encoded></item><item><title>Path Connectedness</title><description>Now we can think about a different type of "connectedness." Intuitively, if a space is "connected" you should be able to draw a path between any two points in the space. Otherwise, if there are points in the space that cannot be connected by a path, it is "disconnected."</description><link>http://localhost:2368/path-connectedness/</link><guid isPermaLink="false">5c706452e91ada004c1f7d50</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Wed, 27 Feb 2019 02:47:43 GMT</pubDate><content:encoded>&lt;ol&gt;
&lt;li&gt;&lt;a href="#some-useful-tools"&gt;Some Useful Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#path-connected-spaces"&gt;Path Connected Spaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#a-pathological-example"&gt;A Path-ological Example&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="someusefultoolsanamesomeusefultools"&gt;Some Useful Tools&lt;a name="some-useful-tools"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I defined connectedness in an earlier post, but there's actually a completely different route we could have taken to define it. Recall that a space is connected if it cannot be separated by disjoint neighborhoods. In this post we'll explore an alternative approach to defining connectedness of topological space, called &lt;em&gt;path connectedness&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;But before we do so, we actually need another fact about connectedness. (Recall that $\overline B$ denotes the closure of the set $B$.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ denote a topological space with a connected subsset $A\subseteq X$. If the subset $B\subseteq X$ satisfies $A\subseteq B\subseteq \overline{A}$, then $B$ is connected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We give a proof by contradiction. Suppose $B$ is not connected. Then there exist open sets $U,V\subseteq B$ which separate $B$. Since $A$ is a connected subset of $B$, this means that either $A\subseteq U$ or $A\subseteq V$. Assume without loss of generality that $A\subseteq U$. Then $A\cap V=\varnothing$ because $U\cap V = \varnothing$. Note, however, that $B\cap V\ne\varnothing$. Thus, we may choose $x\in B\cap V$. Certainly $x\in B$ and so it follows that $x\in\overline{A}$ because $B\subseteq\overline{A}$. On the other hand, we have $x\in V$, which is open in $X$ and disjoint from $B$, so $x\notin\overline{A}$. This is a contradiction, since it cannot be that both $x\in\overline{A}$ and $x\notin\overline{A}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What the above theorem really says is that adding limit points to a connected set always results in another connected set. This result will be invaluable to us throughout this post and in the future. Here is just one example of its usefulness:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Corollary.&lt;/strong&gt; All types of intervals in $\mathbb{R}$ (open, half-open and closed) are connected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We've shown previously that open intervals are connected. Therefore, the rest are as well since they can be obtained from open intervals by adding limit points.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Of particular interest to us is the connectedness of the closed unit interval $[0, 1]$.&lt;/p&gt;
&lt;p&gt;The following invaluable lemma allows us to determine when a piecewise function constructed from continuous maps is continuous.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Gluing Lemma.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces with closed subsets $U,V\subseteq X$ for which $X=U\cup V$. If $f_1:U\to Y$ and $f_2:V\to Y$ are continuous functions which agree on their intersection, i.e. $f_1\restriction{U\cap V}=f_2\restriction{U\cap V}$, then there exists a unique continuous function $f:X\to Y$ with $f\restriction{U}=f_1$ and $f\restriction{V}=f_2$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Showing that such a function is unique is fairly straightforward. We simply define $f:X\to Y$ by&lt;/p&gt;
&lt;p&gt;$$f(x) =&lt;br&gt;
\begin{cases}&lt;br&gt;
f_1(x) &amp;amp; \text{if } x\in U, \\&lt;br&gt;
f_2(x) &amp;amp; \text{if } x\in V.&lt;br&gt;
\end{cases}$$&lt;/p&gt;
&lt;p&gt;This function is clearly well defined because $f_1$ and $f_2$ agree on their intersection.&lt;/p&gt;
&lt;p&gt;We will argue next that $f$ is continuous by demonstrating that the preimage of any open set in $Y$ is open in $X$. To this end, suppose $W\subseteq Y$ is open. Then&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f^{-1}[W] &amp;amp;= X \cap f^{-1}[W] \\&lt;br&gt;
&amp;amp;= (U \cup V) \cap f^{-1}[W] \\&lt;br&gt;
&amp;amp;= (U \cap f^{-1}[W]) \cup (V \cap f^{-1}[W]) \\&lt;br&gt;
&amp;amp;= (U \cap f_1^{-1}[W]) \cup (V \cap f_2^{-1}[W]) \\&lt;br&gt;
&amp;amp;= f_1^{-1}[W] \cup f_2^{-1}[W].&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Since $f_1$ and $f_2$ are continuous, certainly $f_1^{-1}[W]$ and $f_2^{-1}[W]$ are open in $U$ and $V$, respectively. Thus $f^{-1}[W]$ is open in $U\cup V = X$ since it is the union of open sets. It follows that $f$ is continuous, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="pathconnectedspacesanamepathconnectedspaces"&gt;Path Connected Spaces&lt;a name="path-connected-spaces"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We can now work toward defining path connectedness. But first we need to define what a path is! The definition may look a bit ominous at first, but it's really just a formal way of saying exactly what you'd expect.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $X$ denote a topological space with points $a,b\in X$. A &lt;strong&gt;path&lt;/strong&gt; in $X$ from $a$ to $b$ is a continuous map $p:[0,1]\to X$ for which $p(0)=a$ and $p(1)=b$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is implicitly understood in such cases that $[0,1]$ is to be viewed as a subspace of $\R$ in the standard topology.&lt;/p&gt;
&lt;p&gt;Basically a path is just a continuous map from the unit interval into the space of interest. Some authors use the word to describe both a path $p$ and its image $p([0,1])\subseteq X$, but I will keep the concepts separate. That is to say, a path is &lt;em&gt;not&lt;/em&gt; the set of points that its image traces out in the codomain. Its image is a useful way to visualize it, however. And of course, since $[0,1]$ is a connected set, so is its image because paths are continuous! This means that the image of a path looks like a connected curve.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/03/path-1.svg" alt="path-1"&gt;&lt;/p&gt;
&lt;p&gt;Now we can think about a different type of &amp;quot;connectedness.&amp;quot; Intuitively, if a space is &amp;quot;connected&amp;quot; you should be able to draw a path between any two points in the space. Otherwise, if there are points in the space that cannot be connected by a path, it is &amp;quot;disconnected.&amp;quot;&lt;/p&gt;
&lt;p&gt;But we've already used these words for a different concept, so we need to make up new ones.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A topological space $X$ is &lt;strong&gt;path connected&lt;/strong&gt; if for any pair of points $a,b\in X$ there is a path $p:[0,1]\to X$ from $a$ to $b$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You may be wondering whether path connectedness and connectedness are actually one and the same. After all, it's hard to visualize a space that is connected in one sense but not the other. It turns out that one direction is easy.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Every path connected topological space is connected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We give a proof by contradiction. Suppose a space $X$ is path connected but not connected. Then there exists a separation of $X$ by nonempty disjoint open sets $U,V\subseteq X$. Choose points $u\in U$ and $v\in V$. Since $X$ is path connected, there exists a path $p:[0,1]\to X$ with $p(0)=u$ and $p(1)=v$. Define $U'=U\cap p([0,1])$ and $V'=V\cap p([0,1])$. We will show that $U'$ and $V'$ form a separation of $p([0,1])$.&lt;/p&gt;
&lt;p&gt;First, we note that $u\in U'$ because $u\in U$ and $p(0)=u$ so $u\in p([0,1])$. Similarly, $v\in V'$ because $v\in V$ and $p(1)=v$ so $v\in p([0,1])$. Thus, $U'$ and $V'$ are nonempty. Next, clearly&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U' \cup V' &amp;amp;= \big(U \cap p([0, 1])\big) \cup \big(V \cap p([0, 1])\big) \\&lt;br&gt;
&amp;amp;= (U \cup V) \cap p([0,1]) \\&lt;br&gt;
&amp;amp;= X \cap p([0,1]) \\&lt;br&gt;
&amp;amp;= p([0,1]).&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;A similar computation yields&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U' \cap V' &amp;amp;= \big(U \cap p([0, 1])\big) \cap \big(V \cap p([0, 1])\big) \\&lt;br&gt;
&amp;amp;= (U \cap V) \cap p([0,1]) \\&lt;br&gt;
&amp;amp;= \varnothing \cap p([0,1]) \\&lt;br&gt;
&amp;amp;= \varnothing.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Finally, $U'$ and $V'$ are both open in the subspace topology on $p([0,1])$ because they are intersections of $p([0,1])$ with open sets in $X$ ($U$ and $V$, respectively). It follows that $U'$ and $V'$ separate $p([0,1])$ and so it is disconnected. However, it is the continuous image of the connected set $[0,1]$, so this is a contradiction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So path connectedness implies connectedness. But as we shall see later on, the converse does not necessarily hold. So the two notions are actually different. It takes more to be a path connected space than a connected one!&lt;/p&gt;
&lt;p&gt;As with any topological concept, we want to show that path connectedness is preserved by continuous maps.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $f:X\to Y$ is a continuous function and $X$ is path connected, then $f[X]\subseteq Y$ is also path connected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose points $a,b\in f[X]$ and pick $a'\in f^{-1}[\{a\}]$ and $b'\in f^{-1}[\{b\}]$. Since $X$ is path connected, there exists some path $p:[0,1]\to X$ from $a'$ to $b'$. We argue that $f\circ p:[0,1]\to Y$ is a path in $Y$ from $a$ to $b$.&lt;/p&gt;
&lt;p&gt;Certainly $f\circ p$ is continuous since it is the composition of continuous maps. Furthermore,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(f\circ p)(0) &amp;amp;= f(p(0)) \\&lt;br&gt;
&amp;amp;= f(a') \\&lt;br&gt;
&amp;amp;= a,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(f\circ p)(1) &amp;amp;= f(p(1)) \\&lt;br&gt;
&amp;amp;= f(b') \\&lt;br&gt;
&amp;amp;= b.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Thus $f\circ p$ is a path from $a$ to $b$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We saw earlier that disconnected spaces could be decomposed into maximal connected subsets. There is also a way of doing this for path disconnected spaces as well.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ denote a topological space and define a relation $\sim$ on $X$ as follows: $a\sim b$ if and only if there exists a path in $X$ from $a$ to $b$. Then $\sim$ is an equivalence relation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We first show reflexivity. For any point $a\in X$, the constant map $p:[0,1]\to X$ given by $p(t)=a$ is certainly a path from $a$ to itself, so $a\sim a$.&lt;/p&gt;
&lt;p&gt;Next we show symmetry. If $a\sim b$ then there exists a path $p:[0,1]\to X$ with $p(0)=a$ and $p(1)=b$. Define $p^{-}:[0,1]\to X$ by $p^{-}(t)=p(1-t)$. We may think of $p^{-}$ as tracing out $p$ in reverse. That $p^{-}$ is well defined follows because $1-t\in [0,1]$ whenever $t\in [0,1]$, so the images of $p$ and $p^{-}$ are actually equal. Next, $p^{-}$ is continuous since it is the composition of the continuous functions $p$ and $1-t$. Finally, $p^{-}(0)=p(1)=b$ and $p^{-}(1)=p(0)=a$, so $p^{-}$ is a path from $b$ to $a$. Thus, $b\sim a$.&lt;/p&gt;
&lt;p&gt;Lastly, we show transitivity. If $a\sim b$ and $b\sim c$ then there exist paths $p_1:[0,1]\to X$ with $p_1(0)=a$ and $p_1(1)=b$ and $p_2:[0,1]\to X$ with $p_2(0)=b$ and $p_2(1)=c$. Define $p:[0,1]\to X$ by&lt;/p&gt;
&lt;p&gt;$$p(t) =&lt;br&gt;
\begin{cases}&lt;br&gt;
p_1(2t) &amp;amp; \text{if } t\in [0,\frac{1}{2}], \\&lt;br&gt;
p_2(2t-1) &amp;amp; \text{if } t\in [\frac{1}{2},1].&lt;br&gt;
\end{cases}$$&lt;/p&gt;
&lt;p&gt;We may think of $p$ as a path which traces out $p_1$ and then $p_2$ at twice the original speed. That $p$ is well defined follows from the fact that $p_1(1)=p_2(0)=b$. By the gluing lemma, this also implies that $p$ is continuous. Furthermore, $p(0)=p_1(0)=a$ and $p(1)=p_2(1)=c$, so $p$ is a path from $a$ to $c$. Thus, $a\sim c$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above result gives us our maximal path-connected subsets:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;path components&lt;/strong&gt; of a topological space are the equivalence classes of the relation $\sim$ defined above.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="apathologicalexampleanameapathologicalexample"&gt;A Path-ological Example&lt;a name="a-pathological-example"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;(I wonder how many topology professors have made that pun with regard to the example I give below? Probably hundreds.)&lt;/p&gt;
&lt;p&gt;I mentioned above that connected spaces are not always path connected. In this section I will give an example of such a space, although the proof that this is the case is not exactly trivial. First, we will need the following result about sequences and limit points.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A sequence $\{x_n\}_{n\in\N}$ in a topological space $X$ is &lt;strong&gt;eventually constant&lt;/strong&gt; if there exists $x \in X$ and $N \in \N$ for which $x_n=x$ whenever $n&amp;gt;N$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that any eventually constant sequence trivially converges to the point $x$ in the above definition.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma.&lt;/strong&gt; Let $X$ denote a topological space and let $A$ be a subset of $X$. If there exists a sequence $\{p_n\}_{n\in\N}$ of points in $A$ that converges to $p\in X$ and which is not eventually constant, then $p$ is a limit point of $A$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose any neighborhood $U$ of $p$. Since $\{p_n\}_{n\in\N}$ converges to $p$, there exists $N_1\in\N$ for which $p_n\in U$ whenever $n&amp;gt;N$. Since $\{p_n\}_{n\in\N}$ is not eventually constant, there exists $N_2\in N$ for which $p_n\neq p$ whenever $n&amp;gt;N_2$. Choose $N=\max\{N_1,N_2\}$. For every $n&amp;gt;N$, we have that $p_n\in U$ and $p_n\ne p$. By definition, it follows that $p$ is a limit point of $A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We are now ready to give an example of a space that is connected but not path connected. Warning: it's pretty gross.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example (The Topologist's Sine Curve).&lt;/strong&gt; Define&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
A &amp;amp;= \Big\{\Big(x,\sin\big(\tfrac{1}{x}\big)\Big)\in\R^2\mid x\in (0,1)\Big\}, \\&lt;br&gt;
B &amp;amp;= \Big\{(0,y)\in\R^2\mid y\in [-1,1]\Big\}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;$A$ is a sort of sine curve which oscillates more and more rapidly as you approach the origin from the right, and which does not include its endpoints. $B$ is just a vertical line. The &lt;strong&gt;topologist's sine curve&lt;/strong&gt; is their union, $X=A\cup B$.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/03/topologists-sine-curve-1.svg" alt="topologists-sine-curve-1"&gt;&lt;/p&gt;
&lt;p&gt;The blue curve in the above graph is the set $A$ and the red curve (which may be too narrow on your display to see clearly) is the $B$. The line $B$ is sort of the natural completion of $A$, in the sense that $A$ never touches is but really looks like it ought to. More formally, every point of $B$ is a limit point of $A$.&lt;/p&gt;
&lt;p&gt;To show this, choose a point $(0,y)\in B$. Certainly $y=\sin\theta$ for some $\theta\in [0,2\pi)$, and thus $y=\sin(\theta+2\pi n)$ for every $n\in \N$. If $x_n=\frac{1}{\theta+2\pi n}$ then $y=\sin\big(\tfrac{1}{x_n}\big)$ for every $n\in \N$. Furthermore, it is clear that $\lim_{n\to\infty}x_n = 0$. Thus,&lt;/p&gt;
&lt;p&gt;$$\lim_{x_n\to 0}\Big(x_n,\sin\big(\tfrac{1}{x_n}\big)\Big) = (0,y).$$&lt;/p&gt;
&lt;p&gt;Since the sequence $\{x_n\}_{n\in\N}$ is not eventually constant, every point $y\in B$ is a limit point of $A$. Since $A$ is connected (it is the continuous image of $(0,1)$), it follows from the lemma at the beginning of this post that the topologist's sine curve $X$ is connected, since it can be obtained from a connected set plus by adding limit points.&lt;/p&gt;
&lt;p&gt;Showing that $X$ is not path connected is a bit trickier. We will proceed by contradiction, supposing that there exists a path $p:[0,1]\to X$ with $p(0)=(0,0)\in B$ and $p(1)\in A$. Since $B$ is closed in $\R^2$, it follows from the continuity of $p$ that $p^{-1}[B]$ is closed in $[0,1]$. Thus, $p^{-1}[B]$ contains a largest element, which we shall denote by $b$. This implies that $p\restriction{[b,1]}:[b,1]\to X$ is continuous with $p\restriction{[b,1]}(b)\in B$. and $p\restriction{[b,1]}(t)\in X-B = A$ for every $t&amp;gt;b$.&lt;/p&gt;
&lt;p&gt;We define $p':[0,1]\to X$ by $p'(t)=p\big((1-b)t+b\big)$. Clearly $p'(0)=p(b)$ and $p'(1)=p(1)$. Also, $p'$ is continuous since it is the composition of continuous functions. Thus, $p'$ is a path in $X$ from $p(b)$ to $p(1)$ with $p'([0,1])=p([b,1])$. Furthermore, $p'(0)\in B$ while $p'(t)\in A$ for every $t\in (0,1]$.&lt;/p&gt;
&lt;p&gt;Now, for any $t\in [0,1]$ we can write $p'(t)=\big(x(t),y(t)\big)$ in terms of its components $x:[0,1]\to [0,1]$ and $y:[0,1]\to [-1,1]$. Clearly both $x$ and $y$ are continuous. Observe that $x(0)=0$ while $x(t)\in (0,1]$ whenever $t\in (0,1]$. In addition, $y(0)=0$ while $y(t)=\sin\big(\frac{1}{x(t)}\big)$ for any $t\in (0,1]$. Choose $n\in\N$ and $u\in\Big(0,x\big(\frac{1}{n}\big)\Big)$ for which $\sin\big(\frac{1}{u}\big)=(-1)^n$. From the intermediate value theorem, there exists $t_n\in\big(0,\frac{1}{n}\big)$ for which $x(t_n)=u$. Since $\lim_{n\to\infty}\frac{1}{n}=0$ and $0&amp;lt;t_n&amp;lt;\frac{1}{n}$ for every $n\in\N$, clearly $\lim_{n\to\infty}t_n=0$. However, $y(t_n)=\sin\big(\frac{1}{x(t_n)}\big)=\sin\big(\frac{1}{u_n}\big)=(-1)^n$, which does not converge. This contradicts the continuity of $y$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are a few other well known examples of spaces like the above that are connected but not path connected. The most prominent is the &lt;strong&gt;topologist's whirlpool&lt;/strong&gt;, which is essentially just the polar form of the topologist's sine curve.&lt;/p&gt;
&lt;p&gt;One might wonder if there is a sufficient additional criterion for a connected space to be path connected? The answer is yes. If a space is connected and &lt;em&gt;locally path connected&lt;/em&gt; then it is path connected. However, I will not define local path connectedness or prove this result, since I do not really care about it and it is not useful to us.&lt;/p&gt;
</content:encoded></item><item><title>Constructing the Rational Numbers (2)</title><description>This is a continuation of Constructing the Rational Numbers (1). Before moving forward with the rest of the construction, I'd like to formally change my notation for rational numbers from that of equivalence classes of ordered pairs of integers to that of fractions.</description><link>http://localhost:2368/constructing-the-rational-numbers-2/</link><guid isPermaLink="false">5c6f446575042202e3d9fce2</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Fri, 22 Feb 2019 00:38:35 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#canonical-form"&gt;Canonical Form&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#where-is-z"&gt;Where is $\Z$?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ordering-the-rationals"&gt;Ordering the Rationals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#filling-the-gaps"&gt;Filling the Gaps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#field-axioms"&gt;Field Axioms&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="canonicalformanamecanonicalform"&gt;Canonical Form&lt;a name="canonical-form"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This is a continuation of &lt;a href="http://localhost:2368/constructing-the-rational-numbers-1/"&gt;Constructing the Rational Numbers (1)&lt;/a&gt;. Before moving forward with the rest of the construction, I'd like to formally change my notation for rational numbers from that of equivalence classes of ordered pairs of integers to that of fractions.&lt;/p&gt;
&lt;p&gt;That is, from here on out the rational number $[(a,b)]$ will simply be written as the &lt;strong&gt;fraction&lt;/strong&gt; $\frac{a}{b}$. And now rational numbers look exactly how you would expect. Yay! Note that the bar doesn't really mean anything yet, this notation is currently just a formalism.&lt;/p&gt;
&lt;p&gt;Remember that there are many choices of representative for each rational number. For instance, $\frac{1}{2}=\frac{2}{4}$ in the same way that $[(1,2)]=[(2,4)]$ because, in the language of my last post, $(1,2)\sim_\Q (2,4)$. Last time we defined this equivalence relation, $\sim_\Q$, which determines whether two fractions are really the same rational number. Let's rephrase this in terms of fractions and without the formality of the equivalence relation, because it's very important:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Rule (Cross Multiplication).&lt;/strong&gt; Two fractions $\frac{a}{b}$ and $\frac{c}{d}$ represent the same rational number if and only if $ad=bc$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We know in our hearts that although there are many fractional representations of each rational, there is always one preferred representation. That is, although $\frac{1}{2}$ and $\frac{2}{4}$ are both valid representations of the same number, we definitely prefer to call it $\frac{1}{2}$ because it's simpler somehow.&lt;/p&gt;
&lt;p&gt;We can easily make this decision rigorous, but first we need to recall the following definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given two integers $a$ and $b$, their &lt;strong&gt;greatest common divisor&lt;/strong&gt;, written $\gcd(a,b)$, is the largest positive integer which is a factor of both $a$ and $b$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here are some examples, although you have likely seen this concept before.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider $4$ and $6$. We can write $4=2\cdot 2$ and $6=2\cdot 3$. They both have one factor in common: $2$. Thus, $\gcd(4,6)=2$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider $15$ and $30$. We can write $15=3\cdot 5$ and $30=2\cdot 3\cdot 5$. They have two prime factors in common, so their greatest common divisor is the product of these factors: $\gcd(15, 30)=15$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider $0$ and $6$. We can trivially write $0=0\cdot 6$, and so $\gcd(0, 6) = 6$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider $3$ and $7$. We cannot simplify either number further since they are both prime. Clearly they have no factors in common other than $1$, so $\gcd(3, 7)=1$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider $4$ and $9$. Neither is prime, so we can write $4=2\cdot 2$ and $9=3\cdot 3$. There are still no common factors other than $1$, so $\gcd(4,9)=1$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That last example is particularly interesting, since we had two integers which were not prime, and whose greatest common divisor was still 1. We have a special name for this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Two integers $a$ and $b$ are &lt;strong&gt;coprime&lt;/strong&gt; (or &lt;strong&gt;relatively prime&lt;/strong&gt;) if $\gcd(a,b)=1$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I will not prove it, since we are taking properties of the integers for granted, but the greatest common divisor of two integers always exists as long as they are not both zero.&lt;/p&gt;
&lt;p&gt;We now have the tools to choose a preferred representation for each rational number.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A fractional representation $\frac{a}{b}$ for a rational number is in &lt;strong&gt;canonical form&lt;/strong&gt; if $a$ and $b$ are coprime and $b&amp;gt;0$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This definition should hopefully make sense to you. It means, for instance, that $\frac{1}{2}$ is the canonical form for the rational number which is also represented by $\frac{2}{4}$, $\frac{-1}{-2}$, etc.&lt;/p&gt;
&lt;p&gt;However, whenever we make a definition like this it is important to determine two things. Does it always exist? And if it exists, is it unique? In this case, the answer to both questions is yet. There is always exactly one canonical form for each rational number, and so we may refer to it as &lt;em&gt;the&lt;/em&gt; canonical form. Let's prove this!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; Every rational number has a unique canonical form.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose a rational number $q$. We argue first that a canonical form for $q$ exists. Let $\frac{a}{b}$ be some fraction which represents $q$. Note that $\gcd(a,b)$ is guaranteed to exist because $b\neq 0$. Thus there are integers $m$ and $n$ for which&lt;/p&gt;
&lt;p&gt;$$\frac{a}{b} = \frac{m\cdot\gcd(a,b)}{n\cdot\gcd(a,b)}.$$&lt;/p&gt;
&lt;p&gt;We argue that $\frac{a}{b}=\frac{m}{n}$. This is easily done by remembering the above rule for determining whether two fractions represent the same rational number. From the first equality above, we have that&lt;/p&gt;
&lt;p&gt;$$a \cdot n \cdot \gcd(a,b) = b \cdot m \cdot \gcd(a,b).$$&lt;/p&gt;
&lt;p&gt;Using the left cancellation property of the integers, we may cancel $\gcd(a,b)$ from both sides to obtain&lt;/p&gt;
&lt;p&gt;$$an=bm.$$&lt;/p&gt;
&lt;p&gt;Again using the cross multiplication rule for equivalent fractions, we see that $\frac{a}{b}=\frac{m}{n}$. If $n$ is positive, this is certainly a canonical form for $q$. Otherwise, we may easily obtain a canonical form by negating both slots of the fraction: $\frac{-m}{-n}$. It is easy to show from here that $\frac{a}{b}=\frac{-m}{-n}$, so I will not bother with it.&lt;/p&gt;
&lt;p&gt;We have demonstrated that a canonical form for $q$ exists. It remains to show that it is unique. That is, we aim to show that if $\frac{m_1}{n_1}$ and $\frac{m_2}{n_2}$ are both canonical forms for $q$, then $m_1=m_2$ and $n_1=n_2$.&lt;/p&gt;
&lt;p&gt;This is not too difficult. Since both fractions are already in canonical form, we know that $\gcd(m_1,n_1)=1$ and $\gcd(m_2,n_2)=1$. Furthermore, since they both represent $q$, we also know that $\frac{m_1}{n_1} = \frac{m_2}{n_2}$ and thus $m_1n_2=n_1m_2$.&lt;/p&gt;
&lt;p&gt;Certainly this indicates that $m_1$ is a factor of $n_1m_2$. But $m_1$ and $n_1$ are coprime by hypothesis, so it must be the case that $m_1$ is a factor of $m_2$. Reversing the argument, we see that $m_2$ is a factor of $m_1$. This can only be true if $m_1=m_2$. The argument that $n_1=n_2$ is exactly analogous. It follows then that the canonical form is unique, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So now we know that every rational number can be represented in a way that is in this sense &amp;quot;most desirable.&amp;quot; It certainly aligns with the simplification of fractions that we all saw in elementary school.&lt;/p&gt;
&lt;p&gt;Now, remember that we already defined addition and multiplication of rational numbers in my previous post. Let's review those definitions here.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;sum&lt;/strong&gt; of two rational numbers $\frac{a}{b}$ and $\frac{c}{d}$ is the rational number $\frac{ad+bc}{bd}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;product&lt;/strong&gt; of two rational numbers $\frac{a}{b}$ and $\frac{c}{d}$ is the rational number $\frac{ac}{bd}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It's worth mentioning that taking the sum or product of two fractions in canonical form does not always result in a canonical form fraction.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The fraction $\frac{1}{2}$ is certainly in canonical form. However,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\frac{1}{2}+\frac{1}{2} &amp;amp;= \frac{1\cdot 2 + 2\cdot 1}{2\cdot 2} \\&lt;br&gt;
&amp;amp;= \frac{4}{4}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;This is certainly not in canonical form, since $\gcd(4,4)=4$.&lt;/p&gt;
&lt;p&gt;However, this is not really an issue since we can just rewrite it in canonical form post-computation. In this case, the canonical form for the result is $\frac{1}{1}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="whereiszanamewhereisz"&gt;Where is $\Z$?&lt;a name="where-is-z"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Ordinarily, when we are not being as ridiculously pedantic as we are in this post, we would consider the integers to be a subset of the rational numbers. But technically this is not the case of our construction, since our rationals are equivalence classes of pairs of integers and thus not integers themselves. It's our goal, therefore, to identify a subset of rationals that looks and behaves like the integers.&lt;/p&gt;
&lt;p&gt;The above example may have already given this away. Technically the rational number whose canonical form is $\frac{1}{1}$ is not the same thing as the integer $1$. But I mean, come on... they're pretty darn similar. This leads us to the following identification.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;rational integers&lt;/strong&gt; are the set of rational numbers whose canonical form is $\frac{n}{1}$, where $n$ is an integer. We refer to $\frac{n}{1}$ as the &lt;strong&gt;rational integer corresponding to $n$&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I don't actually think &amp;quot;rational integer&amp;quot; is a phrase that anyone ever uses, but it will serve our purposes for this post to distinguish between an integer and its corresponding rational number.&lt;/p&gt;
&lt;p&gt;Rational integers behave just as we would expect under addition and multiplication. That is, their sums and products are also rational integers. Moreover, they are the &lt;em&gt;correct&lt;/em&gt; rational integers. Let's make this a bit more formal:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; If $m$ and $n$ are integers, then the sum of their corresponding rational integers is the rational integer corresponding to their sum. That is,&lt;/p&gt;
&lt;p&gt;$$\frac{m}{1}+\frac{n}{1}=\frac{m+n}{1}.$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; The result actually follows immediately from our definition of rational addition, since&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\frac{m}{1}+\frac{n}{1} &amp;amp;= \frac{m\cdot 1 + 1\cdot n}{1\cdot 1} \\&lt;br&gt;
&amp;amp;= \frac{m+n}{1}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can do exactly the same sort of thing for multiplication:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; If $m$ and $n$ are integers, then the product of their corresponding rational integers is the rational integer corresponding to their product. That is,&lt;/p&gt;
&lt;p&gt;$$\frac{m}{1}\cdot\frac{n}{1}=\frac{mn}{1}.$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Again, the result follows directly from our definition of rational multiplication. Note that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\frac{m}{1}\cdot\frac{n}{1} &amp;amp;= \frac{m\cdot n}{1\cdot 1} \\&lt;br&gt;
&amp;amp;= \frac{mn}{1}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So these rational integers really do correspond to the integers in a one-to-one manner. Since there's no functional difference between rational integers and integers, we might as well just say they're the same thing.&lt;/p&gt;
&lt;h3 id="orderingtherationalsanameorderingtherationals"&gt;Ordering the Rationals&lt;a name="ordering-the-rationals"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;One of the requirements we imposed on our construction last post was that the rationals should be ordered in a way that's compatible with the integers. That means that if $n$ and $m$ are integers with $n &amp;lt; m$, then we definitely want it to be the case that $\frac{n}{1} &amp;lt; \frac{m}{1}$. That is, integers and rational integers should have the same order.&lt;/p&gt;
&lt;p&gt;But in addition to rational integers, all the other rational numbers should be comparable as well. This is done easily enough.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given two rational numbers $\frac{a}{b}$ and $\frac{c}{d}$ in canonical form, their &lt;strong&gt;order&lt;/strong&gt; is determined by saying that $\frac{a}{b} &amp;lt; \frac{c}{d}$ if and only if $ad&amp;lt;bc$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Just as a quick check that this definition of order makes any sense, let's look at a couple simple examples.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the fractions $\frac{1}{2}$ and $\frac{2}{3}$. These are already in canonical form, and we would certainly expect $\frac{1}{2}$ to be less than $\frac{2}{3}$. Let's check that this is indeed the case.&lt;/p&gt;
&lt;p&gt;According to the definition, we just need to make sure that $1 \cdot 3 &amp;lt; 2\cdot 2$. This is obviously true ($3&amp;lt;4$) and so we're done.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the fractions $\frac{0}{1}$ and $\frac{-3}{2}$. Of course we hope it should be the case that $\frac{-3}{2}$ is less than $\frac{0}{1}$.&lt;/p&gt;
&lt;p&gt;Again, plugging these straight into the definition, we see that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
-3\cdot 1 &amp;amp;= -3 \\&lt;br&gt;
&amp;amp;&amp;lt; 0 \\&lt;br&gt;
&amp;amp;= 2\cdot 0.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These sanity checks indicate that we're on the right track with our definition of order. But we need to guarantee that our order extends that of the integers. Let's do that now.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; If $m$ and $n$ are integers with $m&amp;lt;n$ then $\frac{m}{1} &amp;lt; \frac{n}{1}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; There's really not much to prove. Since $m&amp;lt;n$ we certainly have that $m\cdot 1 = 1\cdot n$. By the definition of order, this means that $\frac{m}{1} &amp;lt; \frac{n}{1}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So this order is compatible with the order on the integers. Yay! Now that our rational numbers are ordered, we're allowed to put them on the number line if we so choose.&lt;/p&gt;
&lt;h3 id="fillingthegapsanamefillingthegaps"&gt;Filling the Gaps&lt;a name="filling-the-gaps"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Our motivation for inventing rational numbers was to fill the two types of gaps we identified in the previous post as being missing from the integers. Namely, we required that our rational numbers satisfy the following properties:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;If $a$ and $b$ are integers with $a\ne 0$, there exists a rational number $x$ for which $ax=b$.&lt;/li&gt;
&lt;li&gt;If $p$ and $q$ are rational numbers with $p&amp;lt;q$, there exists a rational number $x$ for which $p&amp;lt;x&amp;lt;q$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;It's actually pretty straightforward to show that our construction guarantees these properties. Let's get straight to work!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; If $a$ and $b$ are integers with $a\ne 0$, there exists a rational number $x$ for which $ax=b$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Technically we should consider the rational integers $\frac{a}{1}$ and $\frac{b}{1}$. We need to show that there is a rational number $\frac{p}{q}$ for which $\frac{a}{1}\cdot\frac{p}{q}=\frac{b}{1}$.&lt;/p&gt;
&lt;p&gt;We argue that taking $p=b$ and $q=a$ will yield the desired result. That is, we only need to verify that $\frac{a}{1}\cdot\frac{b}{a}=\frac{b}{1}$. Notice that, from the definition of rational multiplication,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\frac{a}{1}\cdot\frac{b}{a} &amp;amp;= \frac{ab}{1a} \\&lt;br&gt;
&amp;amp;= \frac{ab}{a}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Since $\gcd(ab, a)=a$, the canonical form for the result is $\frac{b}{1}$, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Neat! We plugged one type of gap and now have solutions to lots of equations. All that's left is to check that we've filled the other type of gap.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; If $p$ and $q$ are rational numbers with $p&amp;lt;q$, there exists a rational number $x$ for which $p&amp;lt;x&amp;lt;q$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose $p=\frac{p_1}{p_2}$ and $q=\frac{q_1}{q_2}$ are in canonical form. We argue that taking $x=\frac{p_1q_2 + p_2q_1}{2p_2q_2}$ will suffice. To see that we did not pull this out of thin air, notice that $x$ is really just the &amp;quot;average&amp;quot; of $p$ and $q$ put into fractional form, and thus we should expect it to sit directly between them on the number line.&lt;/p&gt;
&lt;p&gt;We need to verify that $p&amp;lt;x$ and that $x&amp;lt;q$. To do so, we use the definition of order.&lt;/p&gt;
&lt;p&gt;For the first inequality, note first that since $p&amp;lt;q$, we have that $p_1q_2&amp;lt;p_2q_1$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(p_1)(2p_2q_2) &amp;amp;= (p_2)(2p_1q_2) \\&lt;br&gt;
&amp;amp;&amp;lt; (p_2)(p_1q_2 + p_2q_1),&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;It follows then that $\frac{p_1}{p_2} &amp;lt; \frac{p_1q_2 + p_2q_1}{2p_2q_2}$. That is, $p&amp;lt;x$.&lt;/p&gt;
&lt;p&gt;The proof that $x&amp;lt;q$ is completely analagous to the above.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And there we have it. Our construction of the rational numbers satisfies all the properties we wanted it to!&lt;/p&gt;
&lt;p&gt;So why did we do all this again? The short answer is because we can, and because it's kind of cool. The real answer is that we construct things in mathematics from the ground up so that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We know that the objects we are working with actually exist.&lt;/li&gt;
&lt;li&gt;We know exactly what properties our objects satisfy.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I don't feel comfortable working with anything I can't get a feel for in this way.&lt;/p&gt;
&lt;h3 id="fieldaxiomsanamefieldaxioms"&gt;Field Axioms&lt;a name="field-axioms"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Our rational numbers actually have a few more properties than I've mentioned. Technically, they form what is called an ordered field. I won't prove all of the field axioms here, but from what I've done already and the properties of the integers, you should be able to fill in the gaps pretty easily now. I'll just give you the axioms and then call it quits.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;field&lt;/strong&gt; is a set $\mathbb{F}$ equipped with two operations, addition $+$ and multiplication $\cdot$, which satisfy the following properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Associativity of Addition&lt;/strong&gt;: For all $a,b,c\in\mathbb{F}$, $a+(b+c)=(a+b)+c$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Commutativity of Addition&lt;/strong&gt;: For all $a,b\in\mathbb{F}$, $a+b=b+a$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Additive Identity&lt;/strong&gt;: There exists $0\in\mathbb{F}$ for which $0+a=a$ for any $a\in\mathbb{F}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Additive Inverses&lt;/strong&gt;: For all $a\in\mathbb{F}$, there exists $-a\in\mathbb{F}$ for which $a+(-a)=0$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Associativity of Multiplication&lt;/strong&gt;: For all $a,b,c\in\mathbb{F}$, $a\cdot (b\cdot c)=(a\cdot b)\cdot c$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Commutativity of Multiplication&lt;/strong&gt;: For all $a,b\in\mathbb{F}$, $a\cdot b=b\cdot a$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multiplicative Identity&lt;/strong&gt;: There exists $1\in\mathbb{F}$ with $1\ne 0$ for which $1\cdot a=a$ for any $a\in\mathbb{F}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multiplicative Inverses&lt;/strong&gt;: For all $a\in\mathbb{F}$ with $a\ne 0$, there exists $a^{-1}\in\mathbb{F}$ for which $a\cdot a^{-1}=1$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributivity of Multiplication over Addition&lt;/strong&gt;: For all $a,b,c\in\mathbb{F}$, $a\cdot (b+c) = a\cdot b+a\cdot c$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Furthermore, $\mathbb{F}$ is an &lt;strong&gt;ordered field&lt;/strong&gt; if there is an order $&amp;lt;$ on $\mathbb{F}$ which is compatible with the field structure in the following sense: If $a,b\in\mathbb{F}$ and $a,b\ge 0$, then $a+b\ge 0$ and $a\cdot b\ge 0$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's pretty much it. It's not too hard to show that the rationals $\Q$ form an ordered field under the definitions we gave for addition and multiplication. The additive identity is $\frac{0}{1}$, the multiplicative identity is $\frac{1}{1}$, the additive inverse of $\frac{a}{b}$ is $\frac{-a}{b}$, and the multiplicative inverse of $\frac{a}{b}$ is $\frac{b}{a}$ (provided $a\ne 0$).&lt;/p&gt;
&lt;p&gt;The integers $\Z$ do &lt;em&gt;not&lt;/em&gt; form an ordered field because they are lacking multiplicative inverses. This is precisely the first &amp;quot;gap&amp;quot; that we filled.&lt;/p&gt;
&lt;p&gt;So... yeah.&lt;/p&gt;
</content:encoded></item><item><title>Connectedness</title><description>In this post, I'm going to prove the Intermediate Value Theorem and the One-Dimensional Brouwer Fixed Point Theorem, which are two results that are undeniably and unreasonably useful. In order to prove them, however, we will need to study the notion of connectedness.</description><link>http://localhost:2368/connectedness/</link><guid isPermaLink="false">5c6e5add0ba2bb003f86050b</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Thu, 21 Feb 2019 08:02:07 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#connected-spaces"&gt;Connected Spaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#some-cool-results"&gt;Some Cool Results&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="connectedspacesanameconnectedspaces"&gt;Connected Spaces&lt;a name="connected-spaces"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I decided to postpone the second half of my construction of the rational numbers because I needed to break up the monotony a bit.&lt;/p&gt;
&lt;p&gt;I'm finally about to show you some really cool stuff. In this post, I'm going to prove the Intermediate Value Theorem and the One-Dimensional Brouwer Fixed Point Theorem, which are two results that are undeniably and unreasonably useful. In order to prove them, however, we will need to study the notion of connectedness. I'll probably save path-connectedness for a later time, since this post will be long enough as is.&lt;/p&gt;
&lt;p&gt;Connectedness is pretty much exactly what you'd expect it to be - if something is connected then that means it is all one coherent piece. This is a very hand-wavy statement though, so let's try to come up with a real definition that makes sense in the context of topological spaces. It might be better to start by considering what it is that makes a space disconnected. Let's look at this picture of a space $X$ which we should certainly call disconnected (as a subspace of the plane $\R^2$ with the standard topology):&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/disconnectedspace1.svg" alt="disconnectedspace1"&gt;&lt;/p&gt;
&lt;p&gt;The space $X$ is very clearly comprised of two distinct components that are separated from each other. Now here is the key insight: each of these components is both open and closed in $X$. For this particular example, we can see that each component is open because it is the intersection of $X$ with an open set in $\R^2$ (remember that this is the definition of an open set in the subspace topology).&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/disconnectedspace2.svg" alt="disconnectedspace2"&gt;&lt;/p&gt;
&lt;p&gt;Similarly, each component is closed because it is the intersection of $X$ with a closed set in $\R^2$.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/disconnectedspace3.svg" alt="disconnectedspace3"&gt;&lt;/p&gt;
&lt;p&gt;So in disconnected spaces, there can be numerous sets that are both open and closed. This is something that is not true of connected spaces!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A space $X$ is &lt;strong&gt;connected&lt;/strong&gt; if its only subsets which are both open and closed are $\varnothing$ and $X$ itself.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A space $X$ is &lt;strong&gt;disconnected&lt;/strong&gt; if it is not connected, i.e., there exists a nonempty proper subset $A\subset X$ which is both open and closed in $X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We may occasionally also wish to ask whether subsets of a topological space are connected, and there is an easy way to look at this by using definitions we already have:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A subset $A$ of a topological space $X$ is &lt;strong&gt;connected&lt;/strong&gt; if $A$ is connected when viewed as a subspace of $X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is just one of several equivalent and common ways that we may define connectedness. Let's take a look at two more, since they will be convenient for proofs later in this post. For the first equivalent definition, it is somewhat easier to talk about disconnectedness.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; A topological space $X$ is disconnected if and only if there exist nonempty open sets $U,V\subset X$ for which $U\cap V=\varnothing$ and $U\cup V=X$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose first that $X$ is disconnected. That is, there exists a nonempty proper subset $A\subset X$ which is both open and closed in $X$. Then by definition, $X-A$ is also both open and closed, and $X-A$ is nonempty because $A\ne X$. We observe that $A\cap(X-A)=\varnothing$ and $A\cup(X-A)=X$. Thus, we have demonstrated nonempty open sets $U=A$ and $V=X-A$ for which $U\cap V=\varnothing$ and $U\cup V=X$.&lt;/p&gt;
&lt;p&gt;Suppose next that there exist nonempty open sets $U,V\subset X$ for which $U\cap V=\varnothing$ and $U\cup V=X$. Then $V=X-U$ is also closed since it is the complement of the open set $U$. Note also that $V$ is nonempty by hypothesis, and $V\ne X$ because $U$ is nonempty. It follows that $X$ is disconnected.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Just looking at this new phrasing of disconnectedness, we see that it is immediately applicable to my above example. Clearly that space $X$ is the union of two disjoint, nonempty open sets. Before I move on, let me formally state this new definition in the context of connectedness.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; If a space $X$ is disconnected, then a &lt;strong&gt;separation&lt;/strong&gt; of $X$ is a pair of nonempty open sets $U,V\subset X$ for which $U\cap V=\varnothing$ and $U\cup V=X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Corollary.&lt;/strong&gt; A topological space $X$ is &lt;strong&gt;connected&lt;/strong&gt; if it does not have a separation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The next equivalent definition of connectedness may look a little bit strange to you right now, but it will make our lives a lot easier in two of the proofs that lie ahead. Recall that a constant function $f$ has $f(x_1)=f(x_2)$ for all $x_1,x_2$ in its domain.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; A topological space $X$ is connected if and only if every continuous function $f:X\to\{0,1\}$ is constant, where $\{0,1\}$ is equipped with the discrete topology.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We prove each direction by contraposition. We argue first that if a continuous function $f:X\to\{0,1\}$ is not constant, then $X$ is not connected.&lt;/p&gt;
&lt;p&gt;Suppose there exists a nonconstant continuous function $f:X\to\{0,1\}$. Since $f$ is not constant, it takes on at least two values in its codomain and thus it must be the case that $f[X]=\{0,1\}$, so clearly $f$ is surjective. Define $U=f^{-1}\big[{0}\big]$ and $V=f^{-1}\big[{1}\big]$. Certainly $U\cap V=\varnothing$ and $U\cup V=X$. Also, $U$ and $V$ are nonempty because $f$ is surjective. It follows that $U$ and $V$ form a separation of $X$, and so $X$ is not connected.&lt;/p&gt;
&lt;p&gt;We argue next that if $X$ is not connected, then there exists a continuous function $f:X\to\{0,1\}$ which is not constant.&lt;/p&gt;
&lt;p&gt;Suppose $X$ is not connected. Then there exists a separation $U,V$ of $X$. Define $f:X\to\{0,1\}$ by&lt;/p&gt;
&lt;p&gt;$$f(n) =&lt;br&gt;
\begin{cases}&lt;br&gt;
0 &amp;amp; \text{if } x\in U, \\&lt;br&gt;
1 &amp;amp; \text{if } x\in V.&lt;br&gt;
\end{cases}$$&lt;/p&gt;
&lt;p&gt;This function is nonconstant because both $U$ and $V$ are nonempty. It is obvious that $f^{-1}\big[\{1\}\big]=U$ and $f^{-1}\big[\{0\}\big]=V$, which are both open in $X$. Furthermore, $f^{-1}\big[\{0,1\}\big]=X$ and $f^{-1}[\varnothing]=\varnothing$ are both open in $X$. We have demonstrated that the preimage of every open set in $\{0,1\}$ is open in $X$, and so $f$ is continuous by definition.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's rewrite this as yet another definition of connectedness.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A topological space $X$ is &lt;strong&gt;connected&lt;/strong&gt; if every continuous function $f:X\to\{0,1\}$ is constant.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This definition may seem a little bit stranger than the other two, but it is perfectly natural if we recall that continuous functions map points that are close together to points that are close together. In a connected space, all points are close together and thus a continuous function cannot bridge the gap between $0$ and $1$ - it must stay constant for all values of the domain. Of course, there is nothing special about $\{0,1\}$. We could just has easily used any other discrete two-point space. I'm about to prove a useful theorem whose proof is made much easier by this new definition of connectedness, but first we will need the following lemma.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lemma.&lt;/strong&gt; Let $X,Y$ denote topological spaces, let $A$ denote a subspace of $X$ and let $f:X\to Y$ be a continuous map. The restriction $f\mid_A:A\to Y$, defined by $f\mid_A(a)=f(a)$ for each $a\in A$, is continuous.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose any open set $U\subseteq Y$. Since $f$ is continuous, $f^{-1}[U]$ is open in $X$. Thus, $f\mid_A^{-1}[U]=A\cap f^{-1}[U]$ is open in $A$ by the definition of the subspace topology, and thus $f\mid_A$ is continuous.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now we have the machinery to prove the following intuitive theorem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ denote a topological space and suppose $\{A_i\}_{i=1}^n$ is a collection of $n$ connected subsets of $X$. If $A_i\cap A_{i+1}$ is nonempty for each $1\leq i&amp;lt; n$, then the set $\bigcup\limits_{i=1}^n A_i$ is connected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We will proceed by induction on $n$, with our base case $n=1$ being too obvious to require proof.&lt;/p&gt;
&lt;p&gt;Suppose $\{A_i\}_{i=1}^{n+1}$ are connected with $A_i\cap A_{i+1}\ne\varnothing$ for every $1\le i&amp;lt; n+1$, and that $\bigcup\limits_{i=1}^n A_i$ is connected. Suppose also that $f:\bigcup\limits_{i=1}^{n+1}A_i\to\{0,1\}$ is continuous. Then $f\mid_{\bigcup\limits_{i=1}^n A_i}:\bigcup\limits_{i=1}^n A_i\to\{0,1\}$ and $f\mid_{A_{n+1}}:A_{n+1}\to\{0,1\}$ are also continuous, and they are thus constant because their domains are connected. Since $A_n\cap A_{n+1}$ is nonempty, certainly $\left(\bigcup\limits_{i=1}^n A_i\right)\cap A_{n+1}$ is nonempty, and so there exists a point $x\in\left(\bigcup\limits_{i=1}^n A_i\right)\cap A_{n+1}$. Thus, because $f$ must be well defined, $f\mid_{\bigcup\limits_{i=1}^n A_i}(x)=f\mid_{A_{n+1}}(x)$. Since $f\mid_{\bigcup\limits_{i=1}^n A_i}$ and $f\mid_{A_{n+1}}$ are both constant and they agree at one point, they must agree at every point. Thus, $f$ is constant and so $\bigcup\limits_{i=1}^{n+1} A_i$ is connected.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The proof is simple but looks a bit ugly rendered this way, for which I apologize. The theorem itself should be fairly obvious, and is in fact quite easy to think of visually. Take the following diagram, for instance:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/connectedsequence.svg" alt="connectedsequence"&gt;&lt;/p&gt;
&lt;p&gt;This is a collection of six connected subsets of $\R^2$, each of which intersects the next in precisely one point. (They may or may not correspond to the six Bagel Bites I am about to enjoy.) From our theorem, it follows that their union is connected. Of course, we could have any number of connected subsets that intersect each other in different ways, as long as the successive pairwise intersections are nonempty. I will not prove it here, but this result also applies for any collection of connected subsets (even uncountably infinite).&lt;/p&gt;
&lt;p&gt;I mentioned a few posts back that much of the study of topology is focused on distinguishing between non-homeomorphic spaces. Connectedness offers a way of doing exactly that, because it is a topological property. What I mean by that is this: if two spaces are homeomorphic and one is connected, then so is the other.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X,Y$ denote topological spaces, let $f:X\to Y$ be a homeomorphism and suppose $X$ is connected. Then $Y$ is connected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We will prove the contrapositive. Suppose $Y$ is not connected, so that there exist open sets $U,V$ that separate $Y$. We argue that $f^{-1}[U]$ and $f^{-1}[V]$ form a separation of $X$.&lt;/p&gt;
&lt;p&gt;Clearly $f^{-1}[U]$ and $f^{-1}[V]$ are open because $f$ is continuous, and they are each nonempty because $f$ is surjective. In addition, because $U\cap V=\varnothing$, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f^{-1}[U]\cap f^{-1}[V]&amp;amp;=f^{-1}[U\cap V] \\&lt;br&gt;
&amp;amp;=f^{-1}[\varnothing] \\&lt;br&gt;
&amp;amp;=\varnothing.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Similarly, because $U\cup V=Y$, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f^{-1}[U]\cup f^{-1}[V]&amp;amp;=f^{-1}[U\cup V] \\&lt;br&gt;
&amp;amp;=f^{-1}[Y] \\&lt;br&gt;
&amp;amp;=X.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;It follows that $f^{-1}[U]$ and $f^{-1}[V]$ form a separation of $X$, so $X$ is not connected.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Really all we used was continuity and surjectivity of $f$, so saying that $f$ was a homeomorphism actually weakened the result a little bit. It's actually true that all continuous functions preserve connectedness! I won't prove it though, because it's really just a teensy modification of the above proof.&lt;/p&gt;
&lt;p&gt;Clearly we can distinguish between topological spaces as follows: if one space is connected and another isn't, then the spaces are not homeomorphic. However, we can also use connectedness in a somewhat trickier way to distinguish between certain connected spaces!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;cutset&lt;/strong&gt; $A$ of a connected topological space $X$ is a subset of $X$ for which $X-A$ is disconnected.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;cutpoint&lt;/strong&gt; $x$ of a connected topological space $X$ is a point of $X$ for which $\{x\}$ is a cutset, i.e., $X-\{x\}$ is disconnected.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Let's look again the connected set $A=\bigcup\limits_{i=1}^6 A_i$ depicted below, and consider the points $x$ and $y$.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/cutset.svg" alt="cutset"&gt;&lt;/p&gt;
&lt;p&gt;Is $x$ a cutpoint of $A$? No, it is not because $$A-\{x\}=(A_1-\{x\})\cup\bigcup\limits_{i=2}^5 A_i\cup(A_6-\{x\}).$$ Each of the sets being unioned are connected and the intersection of each set with the next is nonempty, so $A-\{x\}$ is connected. If we permute the indices of the $A_i$, the same logic tells us that $y$ is not a cutpoint of $A$.&lt;/p&gt;
&lt;p&gt;However, $\{x,y\}$ is a cutset of $A$ (or, equivalently, $y$ is a cutpoint of $A-\{x\}$). That's because&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U &amp;amp;= (A_1-\{x\})\cup(A_2-\{y\}) \\&lt;br&gt;
V &amp;amp;= (A_3-\{y\})\cup\bigcup\limits_{i=4}^5 A_i\cup (A_6-\{x\})&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;is a separation of $A-\{x,y\}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How can we use cutsets to distinguish between connected spaces? I'm glad you ask.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem. Let $f:X\to Y$ be a homeomorphism between connected spaces $X$ and $Y$. If $A$ is a cutset of $X$, then $f[A]$ is a cutset of $Y$.&lt;/p&gt;
&lt;p&gt;Proof. Since $X-A$ is disconnected, there exists a separation $U,V$ of $X-A$. We argue that $f[U]$ and $f[V]$ form a separation of $Y-f[A]$. Because $f$ is a homeomorphism, $f^{-1}$ is continuous and thus $f[U]=(f^{-1})^{-1}[U]$ and $f[V]=(f^{-1})^{-1}[V]$ are open in $Y$. Since $U$ and $V$ are nonempty, certainly $f[U]$ and $f[V]$ are nonempty. Observe that, because $U\cap V=\varnothing$,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f[U]\cap f[V]&amp;amp;=(f^{-1})^{-1}[U]\cap(f^{-1})^{-1}[V]\\&lt;br&gt;
&amp;amp;=(f^{-1})^{-1}[U\cap V]\\&lt;br&gt;
&amp;amp;=f[U\cap V]\\&lt;br&gt;
&amp;amp;=f[\varnothing]\\&lt;br&gt;
&amp;amp;=\varnothing.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Furthermore, because $U\cup V=X-A$ and $f$ is bijective,&lt;/p&gt;
&lt;p&gt;$$\begin{align}f[U]\cup f[V]&amp;amp;=f[U\cup V]\\&lt;br&gt;
&amp;amp;=f[X-A]\\&lt;br&gt;
&amp;amp;=f[X]-f[A]\\&lt;br&gt;
&amp;amp;=Y-f[A].&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Thus, $f[U]$ and $f[V]$ separate $Y-f[A]$. It follows that $f[A]$ is a cutset of $Y$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To see how this can be applied, let's take a look at another example.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The subspace $[0,2\pi)$ of $\R$ and the unit circle $S^1=\{(x,y)\in\R^2\mid x^2+y^2=1\}$ as a subspace of $\R^2$ are not homeomorphic. I have not yet shown that either of these sets is connected, but for now we will take it for granted that they both are. Any point $x\in [0,2\pi)$ with $x\ne 0$ is a cutpoint of $[0,2\pi)$ because $$[0,2\pi)-\{x\}=[0,x)\cup (x,2\pi),$$ which is already expressed as the union of its separation. On the other hand, $S^1$ has no cutpoints. For any $x\in S^1$, the space $S^1-\{x\}$ is homeomorphic to an open interval (which is connected). Any cutset of $S^1$ would therefore consist of at least two points, and so cannot be the homeomorphic image of a cutpoint for $[0,2\pi)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I was just very hand-wavy by claiming that certain sets were connected without proving it. Let me at least prove that $\R$ is connected, from which it will immediately follow that open intervals are connected (because they are homeomorphic to $\R$). The proof will require the following fact about the real numbers, which itself needs to be prefaced with the following definitions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given a subset $A$ of $\R$, a number $x\in\R$ is an &lt;strong&gt;upper bound&lt;/strong&gt; for $A$ if $x\ge a$ for every $a\in A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A subset $A$ of $\R$ is &lt;strong&gt;bounded above&lt;/strong&gt; if an upper bound for $A$ exists.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given a subset $A$ of $\R$, a number $x\in\R$ is the &lt;strong&gt;supremum&lt;/strong&gt; (or &lt;strong&gt;least upper bound&lt;/strong&gt;) of $A$ if $x$ is an upper bound for $A$ and $x\le y$ for every upper bound $y$ of $A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Least Upper Bound Property.&lt;/strong&gt; Any nonempty subset of $\R$ which is bounded above has a supremum.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is one of the defining properties of the real numbers, and it is in fact equivalent to the fact that every Cauchy sequence in $\R$ converges (if you've taken an analysis class, this should seem familiar). We will take the least upper bound property for granted, although it can be proven from the construction of the real numbers as either Dedekind cuts or equivalence classes of rational Cauchy sequences. You don't need to understand any of this to move forward. Just smile and nod while you wait for me to shut up.&lt;/p&gt;
&lt;p&gt;Here's the proof that $\R$ is connected. It's pretty gross.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; The set $\R$ of real numbers, equipped with the standard topology, is connected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We proceed by contradiction, supposing that $\R$ is not connected. This means that there exist open sets $U$ and $V$ which separate $\R$. Choose $u\in U$ and $v\in V$, and assume without loss of generality that $u&amp;lt;v$. Define $U'=[u,v]-V$ and $V'=[u,v]-U$. Using De Morgan's law and the fact that $V\cap U=\varnothing$, it follows that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U'\cup V'&amp;amp;=\big([u,v]-V\big)\cup\big([u,v]-U\big)\\&lt;br&gt;
&amp;amp;=[u,v]-(V\cap U)\\&lt;br&gt;
&amp;amp;=[u,v]-\varnothing\\&lt;br&gt;
&amp;amp;=[u,v].&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;The set $U'$ is nonempty because $u\in U'$ and it is bounded above by $v$, and so $U'$ has a supremum, $s$. Clearly $s\in [u,v]$, so either $s\in U'$ or $s\in V'$.&lt;/p&gt;
&lt;p&gt;Suppose first that $s\in U'$. Then because $U'$ is open in $[u,v]$ and $v\notin U'$, there exists $d\in[u,v]$ for which $[s,d)\subseteq U'$. Then for every $x\in (s,d)$, we have that $x\in U'$ and $x&amp;gt;s$, which is a contradiction because $s$ is the supremum of $U'$.&lt;/p&gt;
&lt;p&gt;Suppose then that $s\in V'$. Then because $V'$ is open in $[u,v]$ and $u\notin V'$, there exists $d\in (u,s)$ for which $(d,s]\subseteq V'$. Then for every $x\in (d,s)$, we have that $x&amp;lt;s$ and $x&amp;gt;y$ for every $y\in U'$, which is again a contradiction because $s$ is the supremum of $U'$.&lt;/p&gt;
&lt;p&gt;Since $s\notin U'$ and $s\notin V'$, it must be true that $s\notin [u,v]$, which contradicts its definition.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="somecoolresultsanamesomecoolresults"&gt;Some Cool Results&lt;a name="some-cool-results"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;It's time for some fun stuff now. Let's start with the intermediate value theorem, which you may have experienced and ignored in a calculus class.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Intermediate Value Theorem.&lt;/strong&gt; Let $X$ denote a connected space and let $f:X\to\R$ be continuous. If $x,y\in f[X]$ and $c\in [x,y]$, then $c\in f[X]$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; It suffices to consider $c\in (x,y)$, because if $x\in\{x,y\}$ there is nothing to prove. Since $X$ is connected and $f$ is continuous, $f[X]$ is connected in $\R$. We proceed by contradiction.&lt;/p&gt;
&lt;p&gt;Suppose $c\notin f[X]$ and define $U'=(-\infty,c)$ and $V'=(c,\infty)$. Clearly $U'\cap V'=\varnothing$ and $f[X]\subseteq U'\cup V'$. We argue that $U=U'\cap f[x]$ and $V=V'\cap f[X]$ form a separation of $f[X]$. Since $x\in U'$ and $y\in V'$, we have that $U$ and $V$ are nonempty. Furthermore,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U\cap V &amp;amp;= (U'\cap f[X])\cap(V'\cap f[X])\\&lt;br&gt;
&amp;amp;= (U'\cap V')\cap f[X]\\&lt;br&gt;
&amp;amp;= \varnothing\cap f[X]\\&lt;br&gt;
&amp;amp;= \varnothing.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;In addition, because $f[X]\subseteq U'\cup V'$, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U\cup V &amp;amp;= (U'\cap f[X])\cup(V'\cap f[X])\\&lt;br&gt;
&amp;amp;= (U'\cup V')\cap f[X]\\&lt;br&gt;
&amp;amp;= f[X].&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Thus, $U$ and $V$ separate $f[X]$, which is a contradiction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This theorem has many important consequences, but I won't spend too long talking about them right now because I want to get to the Brouwer fixed point theorem. We'll see the intermediate value theorem again though, I promise.&lt;/p&gt;
&lt;p&gt;Of course, before I can talk about a fixed point theorem, it would help if I told you what fixed points were.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given a set $X$, a function $f:X\to X$ has a &lt;strong&gt;fixed point&lt;/strong&gt; $x\in X$ if $f(x)=x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So fixed points are points which stay fixed after a function acts on them.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;One-Dimensional Brouwer Fixed Point Theorem.&lt;/strong&gt; Every continuous map $f:[-1,1]\to[-1,1]$ has a fixed point.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We proceed by contradiction. Suppose $f:[-1,1]\to[-1,1]$ is continuous but has no fixed point. That is, $f(x)\ne x$ for every $x\in[-1,1]$. Define $g:[-1,1]\to\{-1,1\}$ by&lt;/p&gt;
&lt;p&gt;$$g(x) =&lt;br&gt;
\begin{cases}&lt;br&gt;
-1 &amp;amp; \text{if } f(x) &amp;gt; x, \\&lt;br&gt;
\phantom{-}1 &amp;amp; \text{if } f(x) &amp;lt; x.&lt;br&gt;
\end{cases}$$&lt;/p&gt;
&lt;p&gt;There is an alternative way of writing this: $$g(x)=\frac{f(x)-x}{\abs{f(x)-x}}.$$ Because $f(x)\ne x$ for every $x\in[-1,1]$, clearly $f(x)-x\ne 0$ and $g$ is thus a continuous function. Because $g$ is continuous and its codomain is a discrete two-point set, it must be that $g$ is constant. However, we also have that $g(-1)=1$ and $g(1)=-1$, and so $g$ is not constant - a contradiction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This might all seem a bit abstract right now, so maybe a diagram will shed some light on why this theorem makes sense.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/brouwer1d.svg" alt="brouwer1d"&gt;&lt;/p&gt;
&lt;p&gt;Here's a graph of some continuous function $f:[-1,1]\to[-1,1]$. Clearly any such function must intersect the diagonal of the square in at least one point. Any such intersection is a fixed point of $f$.&lt;/p&gt;
&lt;p&gt;There is a more general Brouwer fixed point theorem which works for any dimension, although we cannot prove it yet. The beginning of the proof is very similar to the above proof, though, so let's get as far as we can go. (Recall the $\partial A$ denotes the boundary of a set $A$.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Brouwer Fixed Point Theorem.&lt;/strong&gt; Let $B^n$ denote the closed unit ball in $\R^n$. Then every continuous function $f:B^n\to B^n$ has a fixed point.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Beginning of Proof.&lt;/strong&gt; Again, we proceed by contradiction and suppose that $f$ has no fixed points, i.e., $f(x)\ne x$ for every $x\in B^n$. We define a new function $g:B^n\to\partial B^n$ by $$g(x)=\frac{f(x)-x}{d\big(f(x),x\big)},$$ where $d$ is the standard metric.&lt;/p&gt;
&lt;p&gt;We can visualize this function as follows: since every point $x$ is distinct from its image $f(x)$, there is a unique line segment in $B^n$ starting at $f(x)$ and passing through $x$ which intersects the boundary $\partial B^n$ in exactly one point, $g(x)$.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/brouwernd.svg" alt="brouwernd"&gt;&lt;/p&gt;
&lt;p&gt;Since $f(x)\ne x$ for every $x\in B^n$, $d\big(f(x),x)&amp;gt;0$ for every $x\in B^n$ and thus $g$ is continuous. Furthermore, $g(x)=x$ for every $x\in\partial B^n$ This should lead to a contradiction, but we can't show this right now.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The contradiction is that there is no continuous function which maps the ball to its boundary while keeping the boundary fixed. You can probably visualize why this is in your head - any such &amp;quot;retraction&amp;quot; would require tearing a hole in the ball. Unfortunately, we need the idea of homology in order to establish that there is no retraction to the ball's boundary, and we are a long way from that.&lt;/p&gt;
&lt;p&gt;I think there's time for one last proof. Basically, we will prove that fixed point properties are preserved by homeomorphism.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Suppose $X$ is a topological space for which every continuous function $f:X\to X$ has a fixed point, and that $Y$ is a space homeomorphic to $X$. Then every continuous function $g:Y\to Y$ has a fixed point.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Let $h:X\to Y$ be a homeomorphism, and let $g:Y\to Y$ be continuous. By hypothesis, $h$ and $h^{-1}$ are continuous, so $h^{-1}\circ g\circ h:X\to X$ is continuous since it is the composition of continuous functions. Thus, $h^{-1}\circ g\circ h$ has a fixed point. That is, there exists some $x\in X$ for which $\big(h^{-1}\circ g\circ h\big)(x)=x$. But then $\big(g\circ h\big)(x)=h(x)$, and thus $h(x)$ is a fixed point of $g$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I'll leave you with the following interesting fact that might occasionally occur to you as you stir hot beverages: the Brouwer fixed point theorem, together with the theorem above, tells us that some particle of your drink will end up in the same place it began. This is because the space occupied by the liquid (presumably a cylinder) is homeomorphic to the three-dimensional closed unit ball.&lt;/p&gt;
</content:encoded></item><item><title>Constructing the Rational Numbers (1)</title><description>We work with number systems every day, but we just sort of take their existence for granted. However, it is possible to construct all of these number systems from scratch.</description><link>http://localhost:2368/constructing-the-rational-numbers-1/</link><guid isPermaLink="false">5c6ce0a7a4e2f60287eae2a9</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Wed, 20 Feb 2019 05:08:09 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#construction"&gt;The Construction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="introductionanameintroduction"&gt;Introduction&lt;a name="introduction"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;It's been a very long time since I've posted, so I figured I'd kick things off again with one of my favorite topics.&lt;/p&gt;
&lt;p&gt;We work with number systems every day, but we just sort of take their existence for granted. However, it is possible to construct all of these number systems from scratch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The natural numbers are built from sets.&lt;/li&gt;
&lt;li&gt;The integers are built from natural numbers.&lt;/li&gt;
&lt;li&gt;The rational numbers are built from integers.&lt;/li&gt;
&lt;li&gt;The real numbers are built from rational numbers.&lt;/li&gt;
&lt;li&gt;The complex numbers are built from real numbers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We could go on and on, since there are also quaternions, octonions and god knows what else.&lt;/p&gt;
&lt;p&gt;There is an obvious hierarchy here, and if I wanted to do things right I would start off at the very lowest level by constructing the natural numbers. Maybe I'll do a post on each of these constructions at some point, but for now I think I'll start in the middle.&lt;/p&gt;
&lt;p&gt;We are going to assume that we already have the set $\mathbb{Z}$ of integers and all of their properties, and we will work from there. That means we know everything about their arithmetic (addition, subtraction and multiplication), as well as their other properties such as order.&lt;/p&gt;
&lt;p&gt;The first thing to consider whenever we want to construct a new number system is what is missing from what we already have? What is not present in the set of integers that we would like to be there? What sorts of problems can we phrase in terms of integers that don't have integer solutions but should have some sort of solution?&lt;/p&gt;
&lt;p&gt;What immediately springs to mind is the following sort of equation:&lt;/p&gt;
&lt;p&gt;$$2x = 1.$$&lt;/p&gt;
&lt;p&gt;Obviously we want to scream out to the heavens that $x=\frac{1}{2}$. However, there is no such thing as a half in the set of integers. We have no concept of fractions or division, and so the above equation actually has no solution right now.&lt;/p&gt;
&lt;p&gt;So we'd like our rational numbers to be able to fill in this sort of gap and provide answers to equations of the form $ax = b$, where $a$ and $b$ are integers. But there's an even more obvious criterion we would like our new numbers to satisfy.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2019/02/integer_line.svg" alt="integer_line"&gt;&lt;/p&gt;
&lt;p&gt;Here's a traditional illustration of the &amp;quot;number line.&amp;quot; It may seem weird to even think about this since it's so ingrained in us after years of doing mathematics, but why do we draw our numbers on a line? Well the reason they can be layed out linearly to begin with is their order: $2$ comes after $1$ and before $3$, etc. But putting them on a line like this suggests something else – that there should be something &lt;em&gt;between&lt;/em&gt; them.&lt;/p&gt;
&lt;p&gt;That is, we would like our new rational numbers to have the property that between any two rational numbers there is another rational number. This is certainly something that the integers don't obey. There is no integer between $1$ and $2$.&lt;/p&gt;
&lt;p&gt;Lastly, we would like our rational numbers to &lt;em&gt;extend&lt;/em&gt; the integers in such a way that we can view the integers as sitting &amp;quot;inside&amp;quot; them, as on the number line. Furthermore, we would like the rational numbers to extend their arithmetic as well. Addition, multiplication and subtraction should all be defined and compatible when we restrict ourselves to talking about integers.&lt;/p&gt;
&lt;p&gt;Let's summarize all of that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Desired Properties of the Rational Numbers&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a subset of the rational numbers which behaves exactly like the integers.&lt;/li&gt;
&lt;li&gt;Rational numbers can be multiplied, added or subtracted in a way that extends the integers and has all the usual properties.&lt;/li&gt;
&lt;li&gt;The rational numbers can be ordered in a way that extends the integers.&lt;/li&gt;
&lt;li&gt;If $a$ and $b$ are integers with $a\ne 0$, there exists a rational number $x$ for which $ax=b$.&lt;/li&gt;
&lt;li&gt;If $p$ and $q$ are rational numbers with $p&amp;lt;q$, there exists a rational number $x$ for which $p&amp;lt;x&amp;lt;q$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;If whatever construction we come up with has all of the above properties, we'll have been successful. With all of that in mind, let's get started.&lt;/p&gt;
&lt;h3 id="theconstructionanameconstruction"&gt;The Construction&lt;a name="construction"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We have a significant advantage here in that we already know exactly what our rational numbers should end up looking like: they should resemble fractions $\frac{p}{q}$ where $p$ and $q$ are integers and $q\ne 0$. We even know what their arithmetic should like like in terms of these fractions. Thus, we are able to draw inspiration from our preconceived notion of what they are and how they should behave. However, we will have to build the concept of &amp;quot;fraction&amp;quot; from the ground up, since it does not exist for us currently.&lt;/p&gt;
&lt;p&gt;We might not have fractions, but we already have the next best thing – &lt;strong&gt;cartesian products&lt;/strong&gt;. Basically a fraction is just a pair of integers, right? So instead of $\frac{p}{q}$, why not just write $(p, q)$? This is actually very close to what we'll end up doing, but it doesn't quite get us where we need to be.&lt;/p&gt;
&lt;p&gt;To see why, recall that any fraction has infinite equivalent representations. For example,&lt;/p&gt;
&lt;p&gt;$$\frac{1}{2}=\frac{2}{4}=\frac{-100}{-200}=\frac{1024}{2048}=\cdots$$&lt;/p&gt;
&lt;p&gt;Unfortunately, our ordered pair idea doesn't allow for this sort of equivalent representation. Certainly $(1, 2)\ne (2, 4)$. They are completely different ordered pairs because their components are different integers! However, we are already equipped with a way to identify these ordered pairs – as a quotient set, by defining an equivalence relation on them.&lt;/p&gt;
&lt;p&gt;At this point, if you have not read my post on &lt;a href="http://localhost:2368/equivalence-relations-and-quotient-sets/"&gt;Equivalence Relations and Quotient Sets&lt;/a&gt;, I would strongly encourage you to pause here and give it a thorough read. It is the main tool we will be using in our construction and is therefore of critical importance to the rest of this post.&lt;/p&gt;
&lt;p&gt;The idea here is to define an equivalence relation $\sim$ on $\mathbb{Z}\times\mathbb{Z}^*$ (the set of ordered pairs of integers whose second component is nonzero) for which&lt;/p&gt;
&lt;p&gt;$$(1, 2) \sim (2, 4) \sim (-100, -200) \sim (1024, 2048) \sim \cdots$$&lt;/p&gt;
&lt;p&gt;and more generally, if $(p, q)\in\mathbb{Z}\times\mathbb{Z}^*$ and $n\in\mathbb{Z}$ then $(p, q) \sim (np, nq)$.&lt;/p&gt;
&lt;p&gt;Let's look back to our intuitive understanding of fractions to see how we might define this equivalence relation. If two fractions are equal, we can &amp;quot;cross multiply&amp;quot; them to obtain an expression purely in terms of integers. That is, if $\frac{a}{b}=\frac{c}{d}$ then $ad=bc$. We will use this to define the following relation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; We define the relation $\sim_\mathbb{Q}$ on the set $\mathbb{Z}\times\mathbb{Z}^*$ as follows:&lt;/p&gt;
&lt;center&gt;$(a, b) \sim_\mathbb{Q} (c, d)$ if and only if $ad=bc$,&lt;/center&gt;&lt;br&gt;
&lt;p&gt;where $a,b,c$ and $d$ are integers with $b,d\ne 0$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the definition above, $\sim_\mathbb{Q}$ is just a symbol meant to distinguish this particular relation. This is not a standard notation or something you will ever need to use again outside of the construction in this post.&lt;/p&gt;
&lt;p&gt;Things are looking good so far. We've managed to rephrase equivalence of fractions solely in terms of ordered pairs of integers. Let's not get too far ahead of ourselves, though. We need to show that this is actually an equivalence relation!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; The relation $\sim_\mathbb{Q}$ is an equivalence relation on the set $\mathbb{Z}\times\mathbb{Z}^*$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We need to show that $\sim_\mathbb{Q}$ is symmetric, reflexive and transitive.&lt;/p&gt;
&lt;p&gt;We argue first that it is symmetric. Choose $a, b\in Z$ with $b\ne 0$. Certainly $ab=ba$ since multiplication of integers is commutative and so $(a, b) \sim_\mathbb{Q} (a, b)$ by the definition of this relation.&lt;/p&gt;
&lt;p&gt;We argue next that it is reflexive. Choose $a,b,c,d\in\mathbb{Z}$ with $b,d\ne 0$ and suppose that $(a,b) \sim_\mathbb{Q} (c,d)$. Then, by definition, we have that $ad=bc$. Again, from the commutativity of integer multiplication, we have that $cb=da$. Thus $(c,d) \sim_\mathbb{Q} (a,b)$.&lt;/p&gt;
&lt;p&gt;Lastly, we argue that it is transitive. Choose $a,b,c,d,e,f\in\mathbb{Z}$ with $b,d,f\ne 0$. Suppose that $(a,b) \sim_\mathbb{Q} (c,d)$ and that $(c,d) \sim_\mathbb{Q} (e,f)$. Then $ad=bc$ and $cf=de$ by definition. Since $cf$ and $de$ are equal, we may multiply the respective sides of the equation $ad=bc$ by these quantities without affecting the equality. That is, $adcf=bcde$. By commutativity, we then have $afdc=bedc$, and so $af=be$. Thus $(a,b) \sim_\mathbb{Q} (e,f)$, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I would be remiss if I failed to mention that in proving the transitivity of $\sim_\mathbb{Q}$ above, we used a property of the integers that might require some explaining. We cancelled the quantity $dc$ from both sides of an equation involving only integers. However, the integers don't have a concept of division! What gives?&lt;/p&gt;
&lt;p&gt;We may not be able to divide integers, but we can still cancel them as we did above. Even though I said before we would assume perfect knowledge of all properties of the integers, I think this one merits special mention since it is not mentioned often outside of a modern algebra course.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Right Cancellation Property of the Integers.&lt;/strong&gt; If $a,b$ and $c$ are integers with $c\ne 0$ and $ac=bc$, then $a=b$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $ac=bc$, we may subtract $bc$ from both sides to obtain the equation $ac-bc=0$. Factoring this yields $(a-b)c=0$. This can only be the case if either $a-b=0$ or $c=0$, since the integers have no zero divisors. However, $c\ne 0$ by hypothesis, and so it must be that $a-b=0$. That is, $a=b$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We used this cancellation property above to cancel the quantity $dc$. There is an analagous left cancellation property, but I think that is obvious and symmetrical enough that I do not need to go into it in detail here.&lt;/p&gt;
&lt;p&gt;Anyway, back to business! We've demonstrated that $\sim_\mathbb{Q}$ is an equivalence relation. This allows us to construct the following quotient set:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; We define the set of rational numbers to be the quotient set&lt;/p&gt;
&lt;p&gt;$$\mathbb{Q}=(\mathbb{Z}\times\mathbb{Z}^*)/\negthickspace\sim_\mathbb{Q}.$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is simultaneously a really beautiful idea and a really ugly expression. And if you're confused by this, let's take a step back and examine what this definition really means.&lt;/p&gt;
&lt;p&gt;Recall that the quotient set defined by an equivalence relation is the set of all of its equivalence classes. What do the equivalence classes look like in this case? Well, they are the sets of all ordered pairs which are equivalent. For instance, the following are all elements of $\mathbb{Q}$:&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
[(1,2)] &amp;amp;= \{(1,2),(-2,4),(3,-6),\ldots\}, \\&lt;br&gt;
[(-3,4)] &amp;amp;= \{(-3,4),(3,-4),(-30,40),\ldots\}, \\&lt;br&gt;
[(5,1)] &amp;amp;= \{(5,1),(15,3),(45,9),\ldots\}, \\&lt;br&gt;
[(0,1)] &amp;amp;= \{(0,1),(0,2),(0,-4),\ldots\}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;And that's because&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(1,2) &amp;amp; \sim_\mathbb{Q} (-2,4) \sim_\mathbb{Q} (3,-6) \sim_\mathbb{Q} \cdots, \\&lt;br&gt;
(-3,4) &amp;amp; \sim_\mathbb{Q} (3,-4) \sim_\mathbb{Q} (-30,40) \sim_\mathbb{Q} \cdots, \\&lt;br&gt;
(5,1) &amp;amp; \sim_\mathbb{Q} (15,4) \sim_\mathbb{Q} (45,9) \sim_\mathbb{Q} \cdots, \\&lt;br&gt;
(0,1) &amp;amp; \sim_\mathbb{Q} (0, 2) \sim_\mathbb{Q} (0, -4) \sim_\mathbb{Q} \cdots.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Essentially all we've done is taken, for instance, all of the pairs which we think should correspond to $\frac{1}{2}$ and we've collapsed them down into a single equivalence class called $[(1,2)]$. In this manner, every rational number is an equivalence class of these ordered pairs of integers.&lt;/p&gt;
&lt;p&gt;Now that we have our rational numbers, we still need to define their arithmetic. We could technically do this however we wanted, but obviously we would like their arithmetic to coincide with our preconceived ideas of fractional arithmetic.&lt;/p&gt;
&lt;p&gt;For example, we could try to define addition as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Incorrect Definition.&lt;/strong&gt; Given two rational numbers $[(a,b)]$ and $[(c,d)]$, we &lt;em&gt;incorrectly&lt;/em&gt; define their &lt;strong&gt;sum&lt;/strong&gt; to be $$[(a,b)] + [(c,d)] = [(a+c,b+d)].$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are two reasons why this is a bad definition. First, because in the language of fractions this would translate to $\frac{a}{b}+\frac{c}{d}=\frac{a+b}{c+d}$, which our elementary school teachers drilled into our heads was &lt;em&gt;WRONG&lt;/em&gt;. (Maybe the phrase &amp;quot;common denominator&amp;quot; is echoing around your head right now.) This definition simply does not correspond to our physical intuition of what should happen when we add fractions.&lt;/p&gt;
&lt;p&gt;But there is an even more fundamental reason why this definition cannot be correct. And it's a little bit subtle, so I'll try to break it down the best that I can.&lt;/p&gt;
&lt;p&gt;To see why this &amp;quot;addition&amp;quot; doesn't even work, let's try to add the rational numbers $[(1,2)]$ and $[(1,4)]$. According to the above definition,&lt;/p&gt;
&lt;p&gt;$$[(1,2)] + [(1,3)] = [(1+1, 2+3)] = [(2,5)].$$&lt;/p&gt;
&lt;p&gt;However, we know that $[(1,2)]=[(2,4)]$. But the above definition gives us&lt;/p&gt;
&lt;p&gt;$$[(2,4)] + [(1,3)] = [(2+1, 4+3)] = [(3,7)].$$&lt;/p&gt;
&lt;p&gt;Obviously $[(2,5)] \ne [(3,7)]$ since $(2,5)\not\sim_\mathbb{Q} (3,7)$, and so we have a serious problem here. The issue is that this notion of addition is not &lt;strong&gt;well defined&lt;/strong&gt;. That is, the result of our addition is different depending on which representative ordered pairs we choose for our equivalence classes. This is unacceptable, because it leads to nonsensical results.&lt;/p&gt;
&lt;p&gt;With that in mind, let's work toward the correct definition. In terms of fractions, we would expect $\frac{a}{b}+\frac{c}{d}=\frac{ad+bc}{bd}$. And in fact, this is exactly how we shall proceed.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given two rational numbers $[(a,b)]$ and $[(c,d)]$, we define their &lt;strong&gt;sum&lt;/strong&gt; to be $$[(a,b)] + [(c,d)] = [(ad+bc,bd)].$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In light of the disaster that was the previous attempt at a definition, we need to verify that this notion of addition is well defined. That is, the result does not depend on our choice of representatives.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; Addition of rational numbers is well defined.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose that $[(a_1,b_1)]=[(a_2,b_2)]$ and $[(c_1,d_1)]=[(c_2,d_2)]$. We need to show that $[(a_1,b_1)]+[(c_1,d_1)]=[(a_2,b_2)]+[(c_2,d_2)]$.&lt;/p&gt;
&lt;p&gt;Since $(a_1,b_1)\sim_\mathbb{Q}(a_2,b_2)$, we have that $a_1b_2=a_2b_1$. Similarly, since $(c_1,d_1)\sim_\mathbb{Q}(c_2,d_2)$, we have that $c_1d_2=c_2d_1$. We note that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(a_1d_1+b_1c_1)b_2d_2 &amp;amp;= a_1b_2d_1d_2 + b_1b_2c_1d_2 \\&lt;br&gt;
&amp;amp;= a_2b_1d_1d_2 + b_1b_2c_2d_1 \\&lt;br&gt;
&amp;amp;= b_1d_1(a_2d_2+b_2c_2).&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;But by definition this means that $(a_1d_1+b_1c_2, b_1d_1) \sim_\mathbb{Q} (a_2d_2+b_2c_2, b_2d_2)$. That is, $[(a_1,b_1)]+[(c_1,d_1)]=[(a_2,b_2)]+[(c_2,d_2)]$, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thank goodness! This definition of addition is both mathematically legal and matches what we would intuitively expect. So let's move on.&lt;/p&gt;
&lt;p&gt;Logically, the next thing to do is work toward defining multiplication of rational numbers. This is very similar to defining addition. Given what we know about fractions, we expect $\frac{a}{b}\cdot\frac{c}{d}=\frac{ac}{bd}$. By now, hopefully you can guess what the definition will look like.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given two rational numbers $[(a,b)]$ and $[(c,d)]$, we define their &lt;strong&gt;product&lt;/strong&gt; to be $$[(a,b)] \cdot [(c,d)] = [(ac,bd)].$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Just like we did for addition, we need to show that multiplication is well defined.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; Multiplication of rational numbers is well defined.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose that $[(a_1,b_1)]=[(a_2,b_2)]$ and $[(c_1,d_1)]=[(c_2,d_2)]$. We need to show that $[(a_1,b_1)] \cdot [(c_1,d_1)] = [(a_2,b_2)] \cdot [(c_2,d_2)]$.&lt;/p&gt;
&lt;p&gt;Since $(a_1,b_1)\sim_\mathbb{Q}(a_2,b_2)$, we have that $a_1b_2=a_2b_1$. Similarly, since $(c_1,d_1)\sim_\mathbb{Q}(c_2,d_2)$, we have that $c_1d_2=c_2d_1$. We note that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
a_1c_1b_2d_2 &amp;amp;= (a_1b_2)(c_1d_2) \\&lt;br&gt;
&amp;amp;= (a_2b_1)(c_2d_1) \\&lt;br&gt;
&amp;amp;= b_1d_1a_2c_2.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;But by definition this means that $(a_1c_1, b_1d_1) \sim_\mathbb{Q} (a_2c_2, b_2d_2)$. That is, $[(a_1,b_1)]\cdot[(c_1,d_1)]=[(a_2,b_2)]\cdot[(c_2,d_2)]$, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This post is getting long, so I'm going to leave it here for now and continue the construction in a later post, along with the verification of our desired properties of the rational numbers. Until next time! :)&lt;/p&gt;
</content:encoded></item><item><title>Sequences, Hausdorff Spaces and Nets</title><description>I'm now going to talk about sequences and nets, which often provide an alternative way of describing topological phenomena. I'll also talk about Hausdorff spaces, which have all sorts of nice properties.</description><link>http://localhost:2368/sequences-hausdorff-spaces-and-nets/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae222</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Thu, 20 Apr 2017 21:11:25 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#sequences"&gt;Sequences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hausdorff-spaces"&gt;Hausdorff Spaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#nets"&gt;Nets&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I'm now going to talk about sequences and nets, which often provide an alternative way of describing topological phenomena. I'll also talk about Hausdorff spaces, which have all sorts of nice properties. I was originally planning to include filters in this discussion as well, but I think if I did that this post might become long enough to break the internet.&lt;/p&gt;
&lt;h3 id="sequencesanamesequences"&gt;Sequences&lt;a name="sequences"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;If you've taken a calculus class (or maybe even if you haven't) then you probably already have some notion of what sequences are. They're basically just lists of elements that go on forever. For instance,&lt;/p&gt;
&lt;p&gt;$$\begin{gather}&lt;br&gt;
\begin{aligned}&lt;br&gt;
(0,1,2,3,4,5,6,7,\dotsc)\\&lt;br&gt;
(1,1,2,3,5,8,13,\dotsc)\\&lt;br&gt;
(\text{cat},\text{cat},\text{cat},\text{cat},\dotsc)&lt;br&gt;
\end{aligned}&lt;br&gt;
\end{gather}$$&lt;/p&gt;
&lt;p&gt;are all sequences. The first two have entries in $\mathbb{N}$ and the third takes values in some set of animals.&lt;/p&gt;
&lt;p&gt;Notice that there is always one entry for each natural number. That is, there is a zeroth entry, a first entry, a second entry, and so on. The order in which these entries appear does matter, so put them in parentheses rather than set brackets to distinguish them from sets. Sequences have two main differences from countable infinite sets: they are ordered, and the same point can appear more than once. This important point leads us to the following rigorous definition of a sequence:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;sequence&lt;/strong&gt; in a topological space $X$ is a function $x:\mathbb{N}\to X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is perhaps a bit confusing to actually think of sequences as functions. The definition above is simply meant to give the &amp;quot;ordered list of points&amp;quot; idea some rigorous footing. We generally write $x_n$, rather than $x(n)$, to denote the $n$th term in a sequence. This means we can write a sequence as $(x_0,x_1,x_2,\dotsc)$. This is sometimes shortened to either $(x_n)_{n=0}^\infty$ or $(x_n)_{n\in\mathbb{N}}$.&lt;/p&gt;
&lt;p&gt;Next, let's talk about convergence. This can be a tricky business, and it is the bane of many Calculus II students' existence. The concept of convergence is not itself terribly complicated — it is the process of figuring out whether a specific sequence converges which can sometimes be unreasonably challenging. To start, let's look at convergence in metric spaces so that we can make use of the familiar notion of distance.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A sequence $(x_n)_{n\in\mathbb{N}}$ in a metric space $X$ converges to a point $x\in X$ if for every real number $\epsilon&amp;gt;0$ there is some natural number $N$ for which $d(x,x_n)&amp;lt; \epsilon$ whenever $n&amp;gt;N$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; If a sequence $(x_n)_{n\in\mathbb{N}}$ converges to a point $x$, we say that $x$ is the &lt;strong&gt;limit&lt;/strong&gt; of that sequence and we write $\lim\limits_{n\to\infty}x_n=x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's a bit of a mouthful, so let's spend a little bit of time making sure we know what we're getting ourselves into. Essentially what I mean when I say that a sequences converges to a point $x$ is that eventually everything in the sequence becomes as close to $x$ as I want. More precisely, given $\epsilon &amp;gt; 0$, I want everything beyond the $N$th entry in the sequence to be within the open ball $B(x,\epsilon)$, where I get to choose $N$. If I can find such an $N$ for every $\epsilon$, then the sequence converges. Generally, $N$ will need to be very large when $\epsilon$ is very close to zero.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the sequence $(x_n)_{n=1}^\infty$ in $\mathbb{R}$ where each $x_n=\frac{1}{n}$. We can visualize this sequence in the following manner:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/1-over-n.svg" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Notice that the points in the sequence all lie on the graph of the function $f:\mathbb{R}^+\to\mathbb{R}$ defined by $f(x)=\frac{1}{x}$. This is not surprising, considering we originally defined sequences as functions themselves. That is, this sequence is really the restriction of $f$ to the positive integers, $f\negmedspace\mid_{\mathbb{Z}^+}:\mathbb{Z}^+\to\mathbb{R}$. If you have any experience with this function, you'll believe me when I say that it becomes extremely close to zero and always grows closer to it. It makes sense then that our sequence does the same, so we might guess that it converges to zero. Let's prove this!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; The sequence $(x_n)_{n=1}^\infty$ given by $x_n=\frac{1}{n}$ converges to $0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose $\epsilon&amp;gt;0$ and let $N&amp;gt;\frac{1}{\epsilon}$. If $n&amp;gt;N$, then certainly $n&amp;gt;\frac{1}{\epsilon}$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
d(x_n, 0) &amp;amp;= \vert x_n - 0\vert \\&lt;br&gt;
&amp;amp;= \tfrac{1}{n} \\&lt;br&gt;
&amp;amp;&amp;lt; \epsilon.&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You don't really need to remember the proof of this fact, although it's incredibly easy to reproduce — the candidate for $N$ in this case is more obvious than usual. Just remember that $\lim\limits_{n\to\infty}\frac{1}{n}=0$, which should hopefully make a lot of sense to you anyway. This is an important sequence which we will occasional use in the future.&lt;/p&gt;
&lt;p&gt;Also, notice that the sequence we just looked at doesn't actually &lt;em&gt;quite&lt;/em&gt; fit the definition I gave for sequences. That is, it doesn't have an entry for every natural number (in particular, there is no $x_0$). We could easily remedy that by rewriting each term as $\frac{1}{n+1}$ and shifting each entry's index down by one. I chose to write it the way I did because it looks a bit nicer. It is somewhat common to allow sequences to start at any index we like, as we can always translate it into starting at zero using a similar substitution.&lt;/p&gt;
&lt;p&gt;Now, in a calculus or analysis class you would study lots of properties and characteristics of sequences in $\mathbb{R}$ and learn a bunch of tricks to help you show that certain types of sequences in $\mathbb{R}$ converge. However, all of that stuff bores me and I want to talk generally about convergent sequences in topological spaces, not just about $\mathbb{R}$ with the standard topology. This will require a slight reworking of the definition of convergence to eliminate the concept of distance that we have in metric spaces.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A sequence $(x_n)_{n\in\mathbb{N}}$ in a topological space $X$ converges to a point $x\in X$ if for every neighborhood $U$ of $x$, there is a natural number $N$ for which $x_n\in U$ whenever $n&amp;gt;N$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This definition basically replaces open balls with neighborhoods, and shouldn't require too much explanation other than that. It should be clear that this definition, when $X$ is a metric space, is equivalent to the old one because open sets are just unions of open balls.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; If a sequence $(x_n)_{n\in\mathbb{N}}$ in a topological space converges to a point $x$, we say that $x$ is a &lt;strong&gt;limit&lt;/strong&gt; of that sequence and we write $\lim\limits_{n\to\infty}x_n=x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Notice that I've said &amp;quot;a limit,&amp;quot; rather than &amp;quot;the limit&amp;quot; like I did for metric spaces. That's because a convergent sequence in a topological space might actually converge to multiple points. &lt;center&gt;&lt;h1&gt;😱&lt;/h1&gt;&lt;/center&gt; The simplest example of this phenomenon that I can think of is as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Let $X$ be any nonempty set equipped with the trivial topology.&lt;sup class="footnote-ref"&gt;&lt;a href="#fn1" id="fnref1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; Then for any point $x\in X$, the only neighborhood of $x$ is $X$ itself. Certainly for any sequence $(x_n)_{n\in\mathbb{N}}$ in $X$, all terms of the sequence are in $X$. If follows that every sequence in $X$ converges to every point of $X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This might strike you as a bit odd, and I'd agree with you. At the very least, this business of every sequence converging to every point is not very desirable behavior for a topological space. After all, we'd like limits of sequences to be unique. Luckily for us, there is a specific type of space for which this behavior is guaranteed!&lt;/p&gt;
&lt;h3 id="hausdorffspacesanamehausdorffspaces"&gt;Hausdorff Spaces&lt;a name="hausdorff-spaces"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A topological space $X$ is &lt;strong&gt;Hausdorff&lt;/strong&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="#fn2" id="fnref2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; if for every pair of points $x,y\in X$ with $x\ne y$, there exists a neighborhood $U$ of $x$ and a neighborhood $V$ of $y$ such that $U\cap V=\varnothing$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So in a Hausdorff space, distinct points have disjoint neighborhoods. This is clearly not true for spaces with two or more points under the trivial topology, so we're off to a good start. Before I show how this property guarantees uniqueness of limits, I will prove that every metric space is Hausdorff.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ denote a metric space with metric $d:X\times X\to\mathbb{R}$. Then $X$ is Hausdorff when equipped with the topology induced by the metric $d$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose $x,y\in X$ with $x\ne y$. By the definition of a metric, $d(x,y)&amp;gt;0$. Let $r=\frac{d(x,y)}{2}$ and define $U=B(x,r)$ and $V=B(y,r)$. It suffices to show that $U$ and $V$ are disjoint, which we will argue by contradiction.&lt;/p&gt;
&lt;p&gt;Suppose $U\cap V\ne\varnothing$. Then there exists some point $p\in U\cap V$, so $d(x,p)&amp;lt; r$ and $d(y,p)&amp;lt; r$ by the definitions of these open balls. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
d(x,p)+d(y,p) &amp;amp;&amp;lt; 2r \\&lt;br&gt;
&amp;amp;= d(x,y),&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;which violates the triangle inequality. We have reached a contradiction, so the proof is complete.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This tells us right away that things like $\mathbb{R}$ in the standard topology are Hausdorff. Now if we can just show that convergent sequences in Hausdorff spaces have unique limits, then I will definitely have been justified earlier in claiming that metric spaces have unique limits. Let's prove this right now.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ be a nonempty Hausdorff space and let $(x_n)_{n\in\mathbb{N}}$ be a convergent sequences in $X$. Then $(x_n)_{n\in\mathbb{N}}$ has exactly one limit.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $(x_n)_{n\in\mathbb{N}}$ is convergent, we know that it has at least one limit. Thus, it suffices to show that it also has at most one limit. We proceed by contradiction.&lt;/p&gt;
&lt;p&gt;Suppose $(x_n)_{n\in\mathbb{N}}$ converges to both $p_1$ and $p_2$, where $p_1\ne p_2$. Since $X$ is Hausdorff, there exist disjoint neighborhoods $U_1$ of $p_1$ and $U_2$ of $p_2$. From the definition of convergence, we have that $x_n\in U_1$ whenever $n&amp;gt;N_1$ and $x_n\in U_2$ whenever $n&amp;gt;N_2$ for some natural numbers $N_1$ and $N_2$ Let $N=\max\{N_1,N_2\}$. Then clearly $x_n\in U_1\cap U_2$ whenever $n&amp;gt;N$. This is a contradiction, since $U_1$ and $U_2$ are disjoint.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So Hausdorff spaces are desirable in that if a sequence converges, it does so as we'd generally expect it to. I won't go into this in too much detail right now, but all of the thinks we actually think of as &amp;quot;space&amp;quot; are Hausdorff. In fact, the definition of a manifold explicitly requires this property, which we shall see if I ever manage to get that far.&lt;/p&gt;
&lt;p&gt;There are a few more properties of Hausdorff spaces which I'd like to prove before moving on, just because they're interesting. The first is the fact that singleton sets in Hausdorff spaces are closed. Its proof is quite straightforward.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ be a nonempty Hausdorff space. Then for every point $x\in X$, the set $\{x\}$ is closed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $X$ is Hausdorff, for every $y\in X$ with $y\ne x$ there exist disjoint neighborhoods $U_y$ of $x$ and $V_y$ of $y$. It follows from the union lemma that&lt;/p&gt;
&lt;p&gt;$$\bigcup\limits_{y\ne x}V_y = X-\{x\},$$&lt;/p&gt;
&lt;p&gt;and this set is open because it is the union of open sets. Thus, $\{x\}$ is closed because its complement is open.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The next property is a little bit more interesting&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces and suppose $Y$ is Hausdorff. Then the graph of any continuous function $f:X\to Y$, given by&lt;/p&gt;
&lt;p&gt;$$G=\left\{\big(x,f(x)\big)\mid x\in X\right\}$$&lt;/p&gt;
&lt;p&gt;is closed in the product space $X\times Y$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; It suffices to show that $(X\times Y)-G$ is open in $X\times Y$. Choose $(x,y)\in (X\times Y)-G$. Clearly $y\ne f(x)$, so because $Y$ is Hausdorff there exist disjoint neighborhoods $U$ of $y$ and $V$ of $f(x)$. Furthermore, because $f$ is continuous we have that $f^{-1}[V]$ is open in $X$. Notice that $x\in f^{-1}[V]$ by definition.&lt;/p&gt;
&lt;p&gt;Next, choose any point $\big(g,f(g)\big)\in G$, and let us consider separately the cases where $g\in f^{-1}[V]$ and $g\notin f^{-1}[V]$. If $g\in f^{-1}[V]$ then by definition $f(g)\in V$. Thus, $f(g)\notin U$ because $U$ and $V$ are disjoint. It follows that $\big(g,f(g)\big)\notin f^{-1}[V]\times U$. If, on the other hand, $g\notin f^{-1}[V]$ then it follows immediately that $\big(g,f(g)\big)\notin f^{-1}[V]\times U$ from the definition of the Cartesian product.&lt;/p&gt;
&lt;p&gt;Either way, $\big(g,f(g)\big)\notin f^{-1}[V]\times U$ and so we have that $(f^{-1}[V]\times U)\cap G=\varnothing$. Clearly $f^{-1}\times U$ is open as it is the product of open sets. Thus every point $(x,y)\in (X\times Y)-G$ is contained in the open set $f^{-1}[V]\times U$, which is itself contained in $(X\times Y)-G$. It follows that $(X\times Y)-G$ is open in $X\times Y$, so $G$ is closed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a pretty nice result, although it isn't too useful to us right now. At the very least, it tells us that continuous real-valued functions have closed graphs because $\mathbb{R}$ is Hausdorff. The next two theorems should immediately seem useful to you.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Any subspace of a Hausdorff space is Hausdorff.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Let $A$ be a subspace of a Hausdorff space $X$ and choose points $x,y\in A$. Then there exist disjoint neighborhoods in $X$, $U$ of $x$ and $V$ of $y$. It follows that $A\cap U$ is a neighborhood of $x$ in $A$ and $A\cap V$ is a neighborhood of $y$ in $A$. Furthermore,&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
(A\cap U)\cap (A\cap V) &amp;amp;= A\cap (U\cap V) \\&lt;br&gt;
&amp;amp;= A\cap\varnothing \\&lt;br&gt;
&amp;amp;= \varnothing,&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;so $A$ is Hausdorff.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; The product of two Hausdorff spaces is Hausdorff.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Let $X$ and $Y$ denote Hausdorff spaces and choose distinct points $(x_1,y_1)$ and $(x_2,y_2)$ in $X\times Y$. Without loss of generality (the other case is so similar) suppose $x_1\ne x_2$. Then because $X$ is Hausdorff, there exist disjoint neighborhoods $U_1$ of $x_1$ and $U_2$ of $x_2$ in $X$. Note that $U_1\times Y$ and $U_2\times Y$ are both open in $X\times Y$, and that $(x_1,y_1)\in U_1\times Y$ while $(x_2,y_2)\in U_2\times Y$. Furthermore,&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}&lt;br&gt;
(U_1\times Y)\cap (U_2\times Y) &amp;amp;= (U_1\cap U_2)\times Y \\&lt;br&gt;
&amp;amp;= \varnothing\times Y \\&lt;br&gt;
&amp;amp;= \varnothing,&lt;br&gt;
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;so $X\times Y$ is Hausdorff.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It can be shown by induction that the product of any finite number of Hausdorff spaces is Hausdorff. It is also possible to show, in fact, that the product of &lt;em&gt;any&lt;/em&gt; collection of Hausdorff spaces is Hausdorff, but I try to avoid talking about infinite Cartesian products unless I have no other choice.&lt;/p&gt;
&lt;p&gt;Given that products and subspaces of Hausdorff spaces inherit Hausdorffness from their parents, you might be tempted to guess that quotients of Hausdorff spaces are Hausdorff. This is wrong in general, although I won't provide a counterexample because this post is already very long and I haven't even started discussing nets yet.&lt;/p&gt;
&lt;p&gt;Unfortunately, before I get to nets I have a few more things about sequences that I would like to talk about. In particular, it would be a shame for me not to prove the following beautiful theorem for you.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces and let $(x_n)_{n\in\mathbb{N}}$ be a sequence which converges to the point $x\in X$. Then for any continuous function $f:X\to Y$, the sequence $\big(f(x_n)\big)_{n\in\mathbb{N}}$ converges to the point $f(x)\in Y$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose any neighborhood $U\subseteq Y$ of $f(x)$. Since $f$ is continuous, $f^{-1}[U]\subseteq X$ is open and clearly $x\in f^{-1}[U]$, so $f^{-1}[U]$ is a neighborhood of $x$. Since $(x_n)_{n\in\mathbb{N}}$ converges to $x$, there exists $N\in\mathbb{N}$ for which $x_n\in f^{-1}[U]$ whenever $n&amp;gt;N$. It follows that $f(x_n)\in U$ whenever $n&amp;gt;N$. Thus, $\big(f(x_n)\big)_{n\in\mathbb{N}}$ converges to $f(x)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This theorem is great because it tells us that continuous functions preserve convergent sequences! It would be even better if the converse was true, because that would give us yet another alternative characterization of continuous functions. Unfortunately, this is not the case without additionally assuming that both spaces are first-countable (a property that I haven't mentioned yet, but that every metric space has). For general spaces, it is also possible for function which aren't continuous to preserve convergent sequences.&lt;/p&gt;
&lt;p&gt;This hints that sequences might not be exactly the right tool to study continuity. The problem is that they are too specific a concept. Let's next look at a generalization of sequences that will solve all of our problems.&lt;/p&gt;
&lt;h3 id="netsanamenets"&gt;Nets&lt;a name="nets"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Before I start trying to explain nets to you, let me state the main theorem we eventually want to prove about them.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces. A function $f:X\to Y$ is continuous if and only if for every net $(x_a)_{a\in A}$ that converges to $x$, the net $\big(f(x_a)\big)_{a\in A}$ converges to $f(x)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In stating this theorem of things to come, I've already given away a fair amount of information about the nature of nets. Namely, the fact that nets look almost exactly like sequences, except perhaps that their entries are indexed over sets other than $\mathbb{N}$. However, nets aren't indexed over just any kind of set — after all, we would still like the entries of a net to progress in some order. Thus, we will define them over sets with a specific type of relation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;preorder&lt;/strong&gt; on a set $X$ is a reflexive and transitive relation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That is, a preorder on $X$ is a relation $\le$ such that $x\le x$ for every $x\in X$, and $x\le z$ whenever $x\le y$ and $y\le z$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;directed set&lt;/strong&gt; is a nonempty set $X$ together with a preorder $\le$ which satisfies the additional property that for any $x,y\in X$, there exists $z\in X$ such that $x\le z$ and $y\le z$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A shorter way of describing this final property of directed sets might be to say that every pair of elements has an upper bound. This ensures that, although some pairs of elements may not be related to each other, they are at least related to some third element. In turn, this guarantees that strange behavior, as in the following example, does not occur.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Just to make sure there's no confusion, this will be an example of a set with a preorder that is &lt;em&gt;not&lt;/em&gt; a directed set, because pairs of elements will not necessarily have upper bounds.&lt;/p&gt;
&lt;p&gt;We will define preorders $\le_1$ on the set $\mathbb{N}\times\{1\}$ and $\le_2$ on the set $\mathbb{N}\times\{2\}$ that act similarly to the standard &amp;quot;less than or equal to&amp;quot; relation on $\mathbb{N}$. Recall that we previously defined $\le$ on $\mathbb{N}$ so that $n\le m$ if and only if $m=n+k$ for some $k\in\mathbb{N}$.&lt;/p&gt;
&lt;p&gt;Notice that every element of $\mathbb{N}\times\{1\}$ is of the form $(n,1)$ for some $n\in\mathbb{N}$. Thus it makes sense to define $\le_1$ using the rule that $(n,1)\le_1 (m,1)$ if and only if $n\le m$. Similarly, we define $\le_2$ using the rule that $(n,2)\le_2 (m,2)$ if and only if $n\le m$.&lt;/p&gt;
&lt;p&gt;It is obvious that both $\le_1$ and $\le_2$ are preorders on their respective sets because they both inherit their reflexivity and transitivity from $\le$.&lt;/p&gt;
&lt;p&gt;Let's use these to define a preorder on $(\mathbb{N}\times\{1\})\cup(\mathbb{N}\times\{2\})$. We can define $\le_3$ on this union using the rule that $n\le_3 m$ if and only if either $n\le_1 m$ or $n\le_2 m$. Using the rigorous set-theoretic definition of relations, we could alternatively define this by $\le_3=\le_1\cup\le_2$. Again, it's easy to see that $\le_3$ is a preorder because it inherits its reflexivity and transitivity from $\le_1$ and $\le_2$.&lt;/p&gt;
&lt;p&gt;Basically what we have is two disjoint copies of things that act identically to $\mathbb{N}$, which have been glued together, but are related to each other in absolutely no way. In particular, if we choose $n_1\in\mathbb{N}\times\{1\}$ and $n_2\in\mathbb{N}\times\{2\}$, there is certainly no element of $(\mathbb{N}\times\{1\})\cup (\mathbb{N}\times\{2\})$ which serves as an upper bound for both $n_1$ and $n_2$. Thus, this example does not constitute a directed set.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; On the other hand, the set $\mathbb{N}$ of natural numbers equipped with $\le$, the standard &amp;quot;less than or equal to&amp;quot; relation, is a directed set. I proved in my post on quotient sets that this relation is reflexive and transitive, so it is certainly a preorder. The fact that all pairs of natural numbers have an upper bound is easy to show. For any $x,y\in\mathbb{N}$, choose $x=\max\{x,y\}$. Then clearly $x\le z$ and $y\le z$. This is a particularly easy example because every natural number is either less than or greater than every other natural number.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Another interesting directed set can be formed as follows. Let $X$ denote any nonempty topological space and pick a point $x\in X$. The set $N_x$ of all neighborhoods of $x$ forms a directed set when equipped with the preorder $\le$ defined by $U\le V$ is and only if $V\subseteq U$.&lt;/p&gt;
&lt;p&gt;This relation is reflexive because for any neighborhood $U$ of $x$, it is clear that $U\subseteq U$ and so $U\le U$.&lt;/p&gt;
&lt;p&gt;It is only a tad more difficult to see that $\le$ is transitive. Suppose we have neighborhoods $U, V$ and $W$ of $x$ for which $U\le V$ and $V\le W$. Then $W\subseteq V\subseteq U$, so certainly $W\subseteq U$. Thus, $U\le Q$.&lt;/p&gt;
&lt;p&gt;Lastly, we need to show that any pair of neighborhoods of $x$ has an upper bound, which in this case simply means they both contain a common neighborhood of $x$. Again, this is easy to show. Choose any two neighborhoods $U$ and $V$ of $x$. Clearly $x\in U\cap V$, and by the definition of a topology $U\cap V$ is open. Thus it is a neighborhood of $x$. It is obvious that $U\cap V\subseteq U$ and $U\cap V\subseteq V$, so $U\le U\cap V$ and $V\le U\cap V$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that we have some examples of directed sets in our arsenal, it's finally time to define nets. You've likely already guessed how we'll proceed.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;net&lt;/strong&gt; in a topological space $X$ is a function $x:A\to X$, where $A$ is any directed set.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Again, we generally write $x_a$ rather than $x(a)$, and we denote a net itself by $(x_a)_{x\in A}$. Since we've already established that $\mathbb{N}$ is a directed set, it should be clear that sequences are a special type of net.&lt;/p&gt;
&lt;p&gt;Convergence of nets is extremely similar to convergence of sequences.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A net $(x_a)_{x\in A}$ in a topological space $X$ &lt;strong&gt;converges&lt;/strong&gt; to a point $x\in X$ if for every neighborhood $U$ of $x$, there exists $b\in A$ for which $x_a\in U$ whenever $a\ge b$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; If a net $(x_a)_{x\in A}$ in a topological space converges to a point $x$, we say that $x$ is a &lt;strong&gt;limit&lt;/strong&gt; of that net and we write $\lim x_a=x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It's fairly easy to come up with a convergent net that is not a sequence, using an example I've already given.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Given a topological space $X$ and a point $x\in X$, let $N_x$ denote the directed set of neighborhoods of $x$ as detailed above. We can construct a net $(x_U)_{U\in N_x}$ by choosing a point $x_U\in U$ for each neighborhood $U$ of $x$. (Notice that this action requires the Axiom of Choice). Intuition tells us that this net should converge to $x$ because the neighborhoods of $x$ get &amp;quot;smaller&amp;quot; the further out we go in our directed set $N_x$. This claim is super easy to verify, so let's just do it.&lt;/p&gt;
&lt;p&gt;Choose any neighborhood $U$ of $x$. From our construction of the net $(x_U)_{U\in N_x}$, it is clear that $x_U\in U$. Furthermore, for any neighborhood $V$ of $x$ with $V\ge U$, we have that $V\subseteq U$ and thus $x_V\in X\subseteq U$. It follows that $(x_U)_{U\in N_x}$ converges to $x$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This post is already so ridiculously long that I'm just going to prove the theorem that I promised you and then be done. Unfortunately, the proof is a little bit on the longer side.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces. Then a function $f:X\to Y$ is continuous if and only if for every net $(x_a)_{a\in A}$ that converges to $x$, the net $\big(f(x_a)\big)_{a\in A}$ converges to $f(x)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; The forward direction is practically identical for the analogous result for series. Suppose $f$ is continuous and that the net $(x_a)_{a\in A}$ converges to the point $x\in X$. Choose any neighborhood $U$ of $f(x)$. Since $f$ is continuous, $f^{-1}[U]\subseteq X$ is open and clearly $x\in f^{-1}[U]$, so $f^{-1}[U]$ is a neighborhood of $x$. Thus, there exists $b\in A$ for which $x_a\in f^{-1}[U]$ whenever $a\ge b$. It follows that $f(x_a)\in U$ whenever $a\ge b$, so the net $\big(f(x_a)\big)_{a\in A}$ converges to $f(x)$.&lt;/p&gt;
&lt;p&gt;I will prove the reverse direction by contradiction. Suppose that for every net $(p_a)_{a\in A}$ that converges to $p$, the net $\big(f(p_a)\big)_{a\in A}$ converges to $f(p)$, but that $f$ is not continuous. Then there exists a point $x\in X$ and a neighborhood $V$ of $f(x)$ for which $f^{-1}[V]$ is not a neighborhood of $x$. Thus, we can construct a net $(x_U)_{U\in N_x}$ for which each $x_U\notin f^{-1}[V]$. Clearly each $f(x_U)\notin V$. Choose any neighborhood $W$ of $x$. Then for any neighborhood $T\ge W$, i.e., $T\subseteq W$, and so $x_T\in W$. It follows that $(x_U)_{U\in N_x}$ converges to $x$, and thus $\big(f(x_U)\big)_{U\in N_x}$ converges to $f(x)$. However, the interior of $V$ is a neighborhood of $f(x)$ and thus $f(x_U)$ is eventually in this interior and therefore also in $V$, but this is a contradiction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So continuity is equivalent to the preservation of convergent nets, which is pretty cool. It's also true that being Hausdorff is equivalent to the existence of unique limits for nets, but I'm going to end this post here because it's really just getting ridiculous at this point.&lt;/p&gt;
&lt;hr class="footnotes-sep"&gt;
&lt;section class="footnotes"&gt;
&lt;ol class="footnotes-list"&gt;
&lt;li id="fn1" class="footnote-item"&gt;&lt;p&gt;Recall that in the trivial topology the only open sets are $\varnothing$ and $X$. &lt;a href="#fnref1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn2" class="footnote-item"&gt;&lt;p&gt;Or &lt;strong&gt;separated&lt;/strong&gt;, or $\mathbf{T}_2$. &lt;a href="#fnref2" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content:encoded></item><item><title>Quotient Spaces</title><description>The notion of a quotient space will effectively allow us to glue pieces of topological spaces together. This corresponds to the collapsing of equivalent subsets to points which occurs in quotient sets, as I mentioned in my last post.</description><link>http://localhost:2368/quotient-spaces/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae221</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Sun, 09 Apr 2017 14:16:40 GMT</pubDate><content:encoded>&lt;p&gt;Now that we've defined quotient sets, let's talk about quotient sets of topological spaces. The notion of a quotient space will effectively allow us to glue pieces of topological spaces together. This corresponds to the collapsing of equivalent subsets to points which occurs in quotient sets, as I mentioned in my last post. These are useful tools, so I'm going to jump right into it. It may be a bit difficult to see where I'm going with this at first, but bear with me and hopefully it'll become clear.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $X$ denote a topological space, let $A$ be a set and let $f:X\to A$ be a surjective function. The &lt;strong&gt;quotient topology&lt;/strong&gt; induced by $f$ has as its open sets all sets $U$ such that $f^{-1}[U]$ is open in $X$. We call $f$ a &lt;strong&gt;quotient map&lt;/strong&gt; and $A$ a &lt;strong&gt;quotient space&lt;/strong&gt; when equipped with this topology.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Supposing the quotient topology is truly a topology, we get for free that quotient maps are always continuous, simple from the way they're defined.  However, we still need to verify that quotient topologies satisfy the requirements of a topology. We need to show that $\varnothing$ and $A$ are open, and that unions and finite intersections of open sets are open.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ denote a topological space, let $A$ be a set and let $f:X\to A$ be a surjective function. Then the quotient topology defined above is a topology on $A$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; It is clear that $f^{-1}[\varnothing]=\varnothing$ and so the empty set is open in $A$. Furthermore, since $f$ is surjective, we know that $f^{-1}[A]=X$ is open in $X$ and so $A$ is also open in $A$.&lt;/p&gt;
&lt;p&gt;Next, suppose that $I$ is an indexing set and $U_i\subseteq A$ are such that $f^{-1}[U_i]$ is open in $X$ for each $i\in I$. Then certainly&lt;/p&gt;
&lt;p&gt;$$f^{-1}\left[\bigcup\limits_{i\in I}U_i\right]=\bigcup\limits_{i\in I}f^{-1}[U_i]$$&lt;/p&gt;
&lt;p&gt;is open in $X$ since it is the union of open sets. Thus, $\bigcup\limits_{i\in I}U_i$ is open in A.&lt;/p&gt;
&lt;p&gt;Finally, suppose that $n\in\mathbb{Z}^+$ and that $U_i\subseteq A$ are such that $f^{-1}[U_i]$ is open in $X$ for each $1\le i\le n$. Then&lt;/p&gt;
&lt;p&gt;$$f^{-1}\left[\bigcap\limits_{i=1}^n U_i\right]=\bigcap\limits_{i=1}^n f^{-1}[U_i]$$&lt;/p&gt;
&lt;p&gt;is open in $X$ since it is the intersection of a finite number of open sets. Thus, $\bigcap\limits_{i=1}^n U_i$ is open in $A$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is also an equivalent, more intuitive way to define quotient spaces which will probably look more familiar to you after our discussion on quotient sets last post.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $X$ denote a topological space and let $\sim$ be an equivalence relation on $X$. The &lt;strong&gt;quotient space&lt;/strong&gt; $X/\negthickspace\sim$ is this quotient set where the open sets are sets of equivalence classes whose unions are open in $X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We've established that quotient maps induce a topology in this way, so let's take a look at what they can do for us.&lt;/p&gt;
&lt;p&gt;While it's true that any set $A$ can be turned into a quotient space by defining a suitable surjection as our quotient map, we generally restrict our interest to partitions of $A$. Let's first visualize the construction of a simple quotient space, which you may have seen before. We will then figure out how to formally document this process.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the square $[0,1]^2$ in the standard topology. We can first create a cylinder from the square by gluing two opposite edges together.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/square_to_cylinder.svg" alt="square to cylinder"&gt;&lt;/p&gt;
&lt;p&gt;Next, we can glue the two open circular ends of the cylinder together to form a torus.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/cylinder_to_torus.svg" alt="cylinder to torus"&gt;&lt;/p&gt;
&lt;p&gt;What we've really done here is glue the opposite edges of the square together, one pair at a time. A much more concise diagram representing this act simply identifies opposite edges of the square with each other, and the gluing is implied.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/square_to_torus.svg" alt="square to torus"&gt;&lt;/p&gt;
&lt;p&gt;In such diagrams, it is understood that the two edges with one arrow get glued to each other and the two edges with two arrows get glued to each other. In this case everything is symmetrical so it doesn't matter which pair gets glued first, but there are probably cases in which it does matter. It is conventional to first glue together sides with one arrow, then two arrows and so on.&lt;/p&gt;
&lt;p&gt;It is this last diagram which provides us with something we can really use. This &lt;em&gt;identification&lt;/em&gt; of sides is crucial to defining the torus as a quotient space of the square. What we are really doing here is partitioning the square in such a way that certain pairs of points on the boundary get grouped into the same equivalence class. More precisely, we partition the square into many sets, namely every point in the interior of the square and each pair of opposite points on the square's boundary:&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
B_{x,y} &amp;amp;= \{(x,y)\}\text{ for } (x,y)\in (0,1)^2, \\&lt;br&gt;
C_x     &amp;amp;= \{(x,0),(x,1)\}\text{ for } x\in (0,1), \\&lt;br&gt;
D_y     &amp;amp;= \{(0,y),(1,y)\}\text{ for } y\in (0,1), \\&lt;br&gt;
E        &amp;amp;= \{(0,0),(0,1),(1,0),(1,1)\}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Allow me to explain these choices a little bit. We have defined one set $B_{x,y}$ in the partition for every point $(x,y)$ in the interior of the square. This is because in our quotient space, we do not want the interior of the square to collapse at all, and so every point should be in its own equivalence class. There is one set $C_x$ for every pair of points along the bottom and top of the square, and one set $C_y$ for every pair along the left and right edges. Putting these pairs into the same equivalence classes ensures that they will become one thing, i.e. 'glued together,' in the quotient space. Lastly, $E$ includes the corners of the square separately to avoid double-counting them.&lt;sup class="footnote-ref"&gt;&lt;a href="#fn1" id="fnref1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; This also ensures explicitly that the four corners of the square will all end up being glued together.&lt;/p&gt;
&lt;p&gt;Going back to our original definitions for quotient maps and spaces, we define the set $A$ as the collection of all sets in this partition and the quotient map $f$ as the surjective function mapping each point in the square to the set in $A$ which contains it. Then it is easy to show that the resulting quotient space is (homeomorphic to) the torus $S^1\times S^1$, so we have accomplished what we set out to.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Many, many more examples of quotient spaces can be generated easily in much the same manner. We can define the sphere as a quotient space of pretty much any polygon, for instance. However, it is tedious and difficult to draw diagrams and they can be found all over the internet anyway. This post is also nice and short for a change, so I'm going to stop here. The good news is that I've now introduced most of the common methods for constructing new topological spaces!&lt;/p&gt;
&lt;hr class="footnotes-sep"&gt;
&lt;section class="footnotes"&gt;
&lt;ol class="footnotes-list"&gt;
&lt;li id="fn1" class="footnote-item"&gt;&lt;p&gt;Remember that the sets in a partition must be pairwise disjoint. &lt;a href="#fnref1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content:encoded></item><item><title>Equivalence Relations and Quotient Sets</title><description>Quotient sets of $A$ are comprised not of elements of $A$, but of the equivalence classes they fall into. This gives us a powerful method to collapse a set into a smaller set that is in some way still representative of the original set.</description><link>http://localhost:2368/equivalence-relations-and-quotient-sets/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae220</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Sun, 09 Apr 2017 00:41:51 GMT</pubDate><content:encoded>&lt;p&gt;The next topological construction I'm going to talk about is the quotient space, for which we will certainly need the notion of quotient sets. However, equivalence relations and quotient sets show up all over the place in mathematics and are worth studying on their own because of their tremendous importance and ubiquitousness.&lt;/p&gt;
&lt;p&gt;The first concept I should introduce is that of a relation. A special type of relation, called an equivalence relation, will be vital to all of the content in this post.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;relation&lt;/strong&gt; on a set $A$ is a subset of the Cartesian product $A\times A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So technically any subset of $A\times A$ is a relation on $A$, but most of them are usually boring and have little meaning. Relations are generally used to compare two elements in some way. That is, we use them to determine whether two elements are 'related' in the manner specified. We should take a look at some characteristics that relations may possess which make them more interesting, but first I'd like to give some examples of familiar relations.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Take $A=\mathbb{N}$, the set of natural numbers. The &amp;quot;less than or equal to&amp;quot; relation on $\mathbb{N}$, usually written $\le$, is defined so that $(a,b)\in\le$ if and only if $b=a+x$ for some $x\in\mathbb{N}$. Clearly&lt;/p&gt;
&lt;p&gt;$$\le \; := \{(a,b)\in\mathbb{N}^2\mid b=a+x\text{ for some }x\in\mathbb{N}\}$$&lt;/p&gt;
&lt;p&gt;fits our definition of a relation because it is a subset of $\mathbb{N}\times\mathbb{N}$. However, no one ever writes things this way. Normal people use infix notation. That is, they write $a\le b$ rather than $(a,b)\in\le$. I will pretty much use infix notation for the rest of time since it tends to simplify things a great deal, and it is probably what you're used to seeing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Take $A=\mathbb{Z}$, the set of integers, and choose some $n\in\mathbb{Z}$. We can define a relation $=_n$ on $\mathbb{Z}$ so that for any $a,b\in\mathbb{Z}$, we have that $a=_n b$ if and only if $b-a=kn$ for some $k\in\mathbb{Z}$. We can write this more formally as&lt;/p&gt;
&lt;p&gt;$$=_n \; := \{(a,b)\in\mathbb{Z}^2\mid b-a=kn\text{ for some }k\in\mathbb{Z}\}$$&lt;/p&gt;
&lt;p&gt;but there isn't usually any benefit in doing things that way, and I think it's even a little bit confusing to look at. By the way, this relation is called &lt;strong&gt;congruence modulo $n$&lt;/strong&gt; and it is of tremendous importance to many fields of mathematics. You can bet we'll be seeing this again at some point soon.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; For any set $A$, both $\varnothing$ and $A\times A$ are relations on $A$. The $\varnothing$ relation doesn't relate elements of $A$ to anything, not even themselves. On the other hand, the relation $A\times A$ relates every element to every element of $A$. These are two extreme sorts of relations, but neither is particularly interesting or important.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that I've given you a few examples, hopefully the definition of a relation has had time to sink in and begun to make a bit of sense. As promised I'll now discuss some important qualities that a relation may or may not have.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A relation $\sim$ on a set $A$ is &lt;strong&gt;reflexive&lt;/strong&gt; if $a\sim a$ for every $a\in A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The relation $\le$ on $\mathbb{N}$ is reflexive because every natural number is less than or equal to itself, i.e. $n\le n$ for every $n\in\mathbb{N}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Another example of a reflexive relation is that of congruence modulo $n$. This is because $a-a=0n$ for every $a\in\mathbb{Z}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The reflexive property holds for many important relations, and is in general quite easy to verify. This is because in most relations of interest to mathematicians, elements tend to be related to themselves. In fact, the only relation we discussed above that is not reflexive is the empty relation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A relation $\sim$ on a set $A$ is &lt;strong&gt;symmetric&lt;/strong&gt; if $b\sim a$ whenever $a\sim b$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The relation $=_n$ on $\mathbb{Z}$ of congruence modulo some $n\in\mathbb{Z}$ is symmetric. To see this, suppose $a=_n b$ for some $a,b\in\mathbb{Z}$. Then by definition, $b-a=kn$ for some integer $k$. Negating each side, we see that $a-b=-kn$ and since $-k$ is also an integer it follows that $b=_n a$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The relation $\le$ on $\mathbb{N}$, on the other hand, is not symmetric. We can see this by examining a single counterexample. Clearly $3\le 5$ but it is not true that $5\le 3$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The final property I'm going to talk about is generally the most difficult to demonstrate holds for a particular relation. It sort of resembles the triangle inequality.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A relation $\sim$ on a set $A$ is &lt;strong&gt;transitive&lt;/strong&gt; if $a\sim c$ whenever both $a\sim b$ and $b\sim c$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; It's not too hard to show that the relation $\le$ on $\mathbb{N}$ is transitive. Consider $a,b,c\in\mathbb{N}$ such that $a\le b$ and $b\le c$. Then by definition, $b=a+x$ and $c=b+y$ for some $x,y\in\mathbb{N}$. Substituting the first equation into the second, we see that $c=a+x+y$. And since $x+y\in\mathbb{N}$, it follows that $a\le c$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Next, consider again the relation $=_n$ on $\mathbb{Z}$ of congruence modulo some integer $n$. Suppose $a=_n b$ and $b=_n c$. Then $b-a=k_1 n$ and $c-b=k_2 n$ for some integers $k_1$ and $k_2$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
c-a &amp;amp;= (c-b) + (b-a) \\&lt;br&gt;
&amp;amp;= k_2 n - k_1 n \\&lt;br&gt;
&amp;amp;= (k_2-k_1)n.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Since $k_2-k_1\in\mathbb{Z}$, it follows that $a=_n c$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that we have established these three types of relation, it is a piece of cake to define an equivalence relation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; An &lt;strong&gt;equivalence relation&lt;/strong&gt; is a relation which is reflexive, symmetric and transitive.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; We've established above that congruence modulo $n$ satisfies each of these properties, which automatically makes it an equivalence relation on the integers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The relation &amp;quot;is the same age as&amp;quot; on the set of all people is an equivalence relation. Every person is the same age as him/herself. If person $A$ is the same age as person $B$, then certainly person $B$ is the same age as person $A$. And transitivity also holds, but I'm too lazy to type that one out right now.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; The relation &amp;quot;has shaken hands with&amp;quot; on the set of all people is &lt;em&gt;not&lt;/em&gt; an equivalence relation because it is not transitive. For instance, it is entirely possible that Bob has shaken Fred's hand and Fred has shaken hands with the president, yet this does not necessarily mean that Bob has shaken the president's hand.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As you will learn, equivalence relations pop up constantly in every area of mathematics. This is because they give sets a very nice kind of structure. To explain this further, we first need the following concepts:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given an equivalence relation $\sim$ on a set $A$ and an element $a\in A$, the &lt;strong&gt;equivalence class&lt;/strong&gt; of $A$ is the set $[a]=\{x\in A\mid a\sim x\}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Given an equivalence relation $\sim$ on a set $A$, the set of equivalence classes corresponding to $\sim$ is called a &lt;strong&gt;quotient set&lt;/strong&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="#fn1" id="fnref1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; and is written $A/\negthickspace\sim$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So quotient sets of $A$ are comprised not of elements of $A$, but of the equivalence classes they fall into. This gives us a powerful method to collapse a set into a smaller set that is in some way still representative of the original set. Hopefully the following example will help make some sense of this.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Let's take another look at the set $\mathbb{Z}$ and the relation $=_3$ of congruence modulo $3$.&lt;sup class="footnote-ref"&gt;&lt;a href="#fn2" id="fnref2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; Under this relation, two integers $a$ and $b$ are related if $b-a=3k$ for some integer $k$. Put more plainly, two integers are congruent if their difference is a multiple of $3$.&lt;/p&gt;
&lt;p&gt;How many equivalence classes does this relation create? It isn't too tough to see that there are three: $[0], [1]$ and $[2]$. This is because all multiples of three, i.e. elements of the form $3k$ for some integer $k$, are congruent. Similarly, all elements of the form $3k+1$ are congruent and all elements of the form $3k+2$ are congruent. And these are all the possible options, really. I have chosen $0, 1$ and $2$ as the &lt;strong&gt;representatives&lt;/strong&gt; of these equivalence classes, but this choice was arbitrary (albeit standard). I could just as easily have named them $[3000], [16]$ and $[-1]$.&lt;/p&gt;
&lt;p&gt;Lastly, we see that the quotient set $\mathbb{Z}/\negthickspace=_3$ is just the set $\big\{[0],[1],[2]\big\}$. That is, all congruent elements are essentially collapsed to a single point in the quotient set.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is one final topic I need to talk about here, which is the fact that equivalence classes form partitions of sets. This is called the Fundamental Theorem of Equivalence Relations, but I'm getting ahead of myself. Before I prove it, I need to tell you what a partition is.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;partition&lt;/strong&gt; $\cal P$ of a set $A$ is a collection of subsets of $A$ which satisfies the following properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The empty set is not in $\cal P$.&lt;/li&gt;
&lt;li&gt;For any two sets $X,Y$ in $\cal P$, either $X$ and $Y$ are disjoint or $X=Y$.&lt;/li&gt;
&lt;li&gt;The union $\bigcup\limits_{X\in\cal P}X$ of all subsets in the partition is equal to $A$ itself.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is all basically a fancy way of saying that a partition is a method of breaking a set into non-overlapping subsets. Now it's time to prove that Fundamental Theorem thingy I mentioned a moment ago.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Fundamental Theorem of Equivalence Relations.&lt;/strong&gt; Let $\sim$ be an equivalence relation on a set $A$. Then the quotient set $A/\negthickspace\sim$ is a partition of $A$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; All we need to do is show that the three properties of partitions hold. Clearly the empty set $\varnothing$ is not in the quotient set $A/\negthickspace\sim$ because the reflexive property of equivalence relations tells us that $x\sim x$, and thus $x\in [x]$ for every $x\in A$.&lt;/p&gt;
&lt;p&gt;Next, suppose that $[x]$ and $[y]$ are equivalences classes in $A/\negthickspace\sim$ and that they are disjoint, i.e. $[x]\cap [y]\ne\varnothing$. Then there exists some $a\in A$ such that $a\in [x]\cap [y]$. That is, $a\in [x]$ and $a\in [y]$, so $a\sim x$ and $a\sim y$. By the symmetric property of equivalence relations, $x\sim a$. And by the transitive property, $x\sim y$. Thus, it follows that $[x]=[y]$.&lt;/p&gt;
&lt;p&gt;Finally, it is clear from the union lemma that the union of all equivalence classes, $\bigcup\limits_{[x]\in A/\sim}[x]$, is the entire set $A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Notice that we used all three properties of equivalence relations (reflexivity, symmetry and transitivity) to prove this result. This indicates that equivalence relations are the only relations which partition sets in this manner. It turns out that this is true, and it's very easy to prove. I won't do that here because this post is already longer than I intended, but I will at least state the theorem.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $\cal P$ be a partition of a set $A$ and define a relation $\sim$ by $x\sim y$ if and only if $x,y\in X$ for some $X\in\cal P$. Then $\sim$ is an equivalence relation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What this theorem really tells us is that every partition is the quotient set of some equivalence relation, and that's a really cool idea.&lt;/p&gt;
&lt;hr class="footnotes-sep"&gt;
&lt;section class="footnotes"&gt;
&lt;ol class="footnotes-list"&gt;
&lt;li id="fn1" class="footnote-item"&gt;&lt;p&gt;More formally, it is called the &lt;strong&gt;quotient set of $A$ modulo $\sim$.&lt;/strong&gt; &lt;a href="#fnref1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn2" class="footnote-item"&gt;&lt;p&gt;It's easy to extend this example to integers other than $3$, but I feel like a more concrete example is useful here. &lt;a href="#fnref2" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content:encoded></item><item><title>Product Spaces</title><description>Next let's talk about an intuitive way to combine topological spaces to create new spaces which inherit certain characteristics from their parents. We've talked about Cartesian products before in the context of set theory, but what happens if we take the Cartesian product of topological spaces?</description><link>http://localhost:2368/product-spaces/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae21f</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Sat, 08 Apr 2017 20:54:38 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#definition"&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples"&gt;Examples&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="definitionanamedefinition"&gt;Definition&lt;a name="definition"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Next let's talk about an intuitive way to combine topological spaces to create new spaces which inherit certain characteristics from their parents. We've talked about Cartesian products before in the context of set theory, but what happens if we take the Cartesian product of topological spaces? What should the topology look like?&lt;/p&gt;
&lt;p&gt;Obviously there are many options (to name a few, the discrete or trivial topologies can be defined on any set), but we would like to choose a topology that is as natural as possible and inherits its properties from the spaces from which it is built. This decision actually has its roots in category theory, but I hope that the choice will make some sense to you nonetheless.&lt;/p&gt;
&lt;p&gt;Let's say we are given topological spaces $X$ and $Y$ and we want to construct a &amp;quot;natural&amp;quot; topology on $X\times Y$. Our first instinct might be to choose as the open sets all products $U\times V$ where $U$ is open in $X$ and $V$ is open in $Y$. But even in $\mathbb{R}\times\mathbb{R}$ we can see that this doesn't result in a topology, since the union of products of open sets isn't necessarily itself a product of open sets, as illustrated below.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/nonono.svg" alt="no no no"&gt;&lt;/p&gt;
&lt;p&gt;So clearly the products of open sets aren't going to form a topology by themselves, since they are not closed under unions. We don't throw out this idea entirely, though. It just so happens that the products of open sets do form a &lt;em&gt;basis&lt;/em&gt; for a topology on $X\times Y$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $(X,{\cal T}_X)$ and $(Y,{\cal T}_Y)$ denote topological spaces. Then ${\cal B} = \{U\times V\mid U\in {\cal T}_X, V\in {\cal T}_V\}$ is a basis for a topology on $X\times Y$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We argue first that for every $(x,y)\in X\times Y$ there exists a basis element $B\in {\cal B}$ with $(x,y)\in B$. Take $B=X\times Y$. Since $x\in X$ and $y\in Y$, clearly $(x,y)\in B$.&lt;/p&gt;
&lt;p&gt;Next, suppose we are given basis elements $B_1, B_2\in{\cal B}$ for which $B_1\cap B_2\ne\varnothing$. Then by definition $B_1=U_1\times V_1$ and $B_2=U_2\times V_2$ for some open sets $U_1, U_2\in{\cal T}_X$ and $V_1, V_2\in{\cal T}_Y$. Note that $U=U_1\cap U_2\in{\cal T}_X$ and $V=V_1\cap V_2\in{\cal T}_Y$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
B_1\cap B_2 &amp;amp;= (U_1\times V_1)\cap (U_2\times V_2) \\&lt;br&gt;
&amp;amp;= (U_1\cap U_2)\times (V_1\cap V_2) \\&lt;br&gt;
&amp;amp;= U\times V \\&lt;br&gt;
&amp;amp;\in {\cal B}.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;It follows that $B_1\cap B_2$ is a basis element contained in itself, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that we have a natural basis for a topology on the product of two spaces, defining the product topology is a piece of cake.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let Let $(X,{\cal T}_X)$ and $(Y,{\cal T}_Y)$ denote topological spaces. The &lt;strong&gt;product topology&lt;/strong&gt; on $X\times Y$ is the topology generated by the basis ${\cal B} = \{U\times V\mid U\in {\cal T}_X, V\in {\cal T}_V\}$. We call $X\times Y$ a &lt;strong&gt;product space&lt;/strong&gt; when equipped with this topology.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Just to refresh your memory, the open sets in the topology generated by a basis are the empty set and all unions of basis elements. This also guarantees that the entire space is open as a result of the union lemma, as we saw several posts ago.&lt;/p&gt;
&lt;p&gt;The product topology can easily be extended in the obvious way to the Cartesian product of a finite numbers of sets. This basis even generates a topology for an infinite number of sets, but in that case it is actually not the topology we generally use. For an infinite number of sets, the product topology has a few extra restrictions. The basis we just gave extends to what is called the &lt;strong&gt;box topology&lt;/strong&gt; for an infinite product, and it has some undesirable properties. However, I'm fairly confident that I will never need to talk about infinite products on this blog, so I'm going to leave the discussion at that for now.&lt;/p&gt;
&lt;h3 id="examplesanameexamples"&gt;Examples&lt;a name="examples"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; For our first example, consider $\mathbb{R}$ with the standard topology. What is the product topology on $\mathbb{R}\times\mathbb{R}=\mathbb{R}^2$? Well we know that a basis for this topology is all products of open intervals. If $(a,b)$ and $(c,d)$ are open intervals in $\mathbb{R}$ then $(a,b)\times(c,d)$ can be viewed as an open rectangle in $\mathbb{R}^2$. But open rectangles, just like open balls, generate the standard topology on $\mathbb{R}^2$. So the product topology on $\mathbb{R}^2$ is actually the standard topology, and the same holds for any finite product of $\mathbb{R}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; For our next example, consider the closed interval $[0,1]$ as a subspace of $\mathbb{R}$. The product $[0,1]\times[0,1]$ is just the unit square in $\mathbb{R}^2$. Open sets in the square are unions of products of open sets in $[0,1]$. That is, they are unions of open rectangles.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider the unit circle $S^1=\{(x,y)\in\mathbb{R}^2\mid x^2+y^2=1\}$ as a subspace of $\mathbb{R}^2$. Let's begin by visualizing $S^1\times S^1$. Recalling the definition of the Cartesian product, we can think loosely of each point on the first circle $S^1$ as corresponding to an entire circle. We thus obtain the &lt;strong&gt;torus&lt;/strong&gt;, which is a donut-shaped subset of $\mathbb{R}^3$.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/torus-1.svg" alt="torus"&gt;&lt;/p&gt;
&lt;p&gt;Notice that the torus is hollow. If we wanted a solid torus, we would take $S^1\times D^2$, where $D^2=\{(x,y)\in\mathbb{R}^2\mid x^2+y^2\le 1\}$ is the closed unit ball.&lt;/p&gt;
&lt;p&gt;Before we can think about the topology on the torus $S^1\times S^1$, we should first consider the topology on the circle $S^1$. Since it's a subspace of $\mathbb{R}^2$, open sets in the circle are intersections of $S^1$ with open sets in $\mathbb{R}^2$. These open sets basically look like unions of &amp;quot;open intervals&amp;quot; wrapped around the circle. In fact, they are all homeomorphic to open intervals, except for $S^1$ itself (which I will prove when I talk about connectedness).&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/circle_open_sets.svg" alt="circle open sets"&gt;&lt;/p&gt;
&lt;p&gt;Products of these open sets somewhat resemble open rectangles wrapped around the surface of the torus. We'll call them open patches, and the unions of these open patches form the open sets on the torus.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/torus_open_sets.svg" alt="torus open sets"&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Since $S^1\times S^1$ can also be viewed as a subspace of $\mathbb{R}^3$, we could also view the open sets on the torus as intersections of open sets in $\mathbb{R}^3$ with $S^1\times S^1$. This statement may seen obvious, but I haven't proved it yet.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces with $A\subseteq X$ and $B\subseteq Y$. The topology on $A\times B$ as a subspace of $X\times Y$ is the same as the product topology where $A$ is a subspace of $X$ and $B$ is a subspace of $Y$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We argue that any open set in either topology is also open in the other. Choose an open set $U$ in the subspace topology on $A\times B$. By definition, there exists some open set $V$ in $X\times Y$ such that $U=(A\times B)\cap B$. Since it is open in the product topology on $X\times Y$, we have that $V$ must be a union of products of open sets. That is,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
V &amp;amp;= \bigcup\limits_{i\in I}(S_i\times T_i) \\&lt;br&gt;
&amp;amp;= \bigcup\limits_{i\in I}S_i\times\bigcup\limits_{i\in I}T_i,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $I$ is an indexing set such that $S_i$ is open in $X$ and $T_i$ is open in $Y$ for every $i\in I$. But this means that $U$ is open in the product topology on $A\times B$.&lt;/p&gt;
&lt;p&gt;The proof of the reverse direction is completely symmetrical.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So any open set on the torus can also be expressed as the intersection of open balls in $\mathbb{R}^3$ with $S^1\times S^1$. This may or may not be a simpler way of viewing the topology on the torus, depending on the application.&lt;/p&gt;
&lt;p&gt;I would like to conclude with the proof I promised you in my last post, which greatly simplified the task of showing that the $x$-axis as a subspace of $\mathbb{R}^2$ is homeomorphic to $\mathbb{R}$. This proof closely mimics the corresponding proof in my last post, although I have defined the homeomorphism in the opposite direction just to spice things up a bit. Notice first that the $x$-axis may be written as $\mathbb{R}\times\{0\}$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $A$ and $B$ denote topological spaces with $b\in B$ and consider $A\times\{b\}$ as a subspace of $A\times B$ with the product topology. Then $A$ is homeomorphic to $A\times\{b\}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We will argue that $f:A\to A\times\{b\}$ defined by $f(a)=(a,b)$ is a homeomorphism. Certainly $f$ is bijective and its inverse function $f^{-1}:A\times\{b\}\to A$ is given by $f^{-1}\big((a,b)\big)=a$.&lt;/p&gt;
&lt;p&gt;First we'll show that $f$ is continuous. Let $U$ denote an open set in $A\times\{b\}$. We can write $U$ as the intersection of $A\times\{b\}$ with some union of basis elements of $A\times B$, which are themselves products of open sets. That is, for some indexing set $I$,&lt;/p&gt;
&lt;p&gt;$$U=(A\times B)\cap\bigcup\limits_{i\in I}(A_i\times B_i),$$&lt;/p&gt;
&lt;p&gt;where $A_i\subseteq A$ and $B_i\subseteq B$ are open for every $i\in I$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f^{-1}[U] &amp;amp;= f^{-1}\left[(A\times B)\cap\bigcup_{i\in I}(A_i\times B_i)\right]\\&lt;br&gt;
&amp;amp;= f^{-1}[A\times B]\cap f^{-1}\left[\bigcup_{i\in I}(A_i\times B_i)\right]\\&lt;br&gt;
&amp;amp;= f^{-1}[A\times B]\cap \bigcup_{i\in I}f^{-1}[A_i\times B_i]\\&lt;br&gt;
&amp;amp;= A\cap\bigcup_{i\in I}A_i\\&lt;br&gt;
&amp;amp;= \bigcup_{i\in I}A_i,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;which is certainly open in $A$ since it is the union of open sets.&lt;/p&gt;
&lt;p&gt;It is easier to show that $f^{-1}$ is continuous. Let $V$ denote an open set in $A$. Note that $V\times B$ is a basis element for $A\times B$ and is thus open in $A\times B$. Therefore,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(f^{-1})^{-1}[V] &amp;amp;= f[V] \\&lt;br&gt;
&amp;amp;= V\times\{b\} \\&lt;br&gt;
&amp;amp;= (A\times\{b\})\cap(V\times B).&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;This is open in $A\times\{b\}$ because it is the intersection of $A\times\{b\}$ with an open set in $A\times B$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Using this result, we can immediately construct homeomorphisms&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\mathbb{R}&amp;amp;\to\mathbb{R}\times\{b\}\subset\mathbb{R}^2 \\&lt;br&gt;
\mathbb{R}^2&amp;amp;\to\mathbb{R}^2\times\{b\}\subset\mathbb{R}^3 \\&lt;br&gt;
\mathbb{R}^3&amp;amp;\to\mathbb{R}^3\times\{b\}\subset\mathbb{R}^4 \\&lt;br&gt;
&amp;amp;\;\; \vdots \\&lt;br&gt;
\mathbb{R}^n&amp;amp;\to\mathbb{R}^n\times\{b\}\subset\mathbb{R}^{n+1} \\&lt;br&gt;
&amp;amp;\;\; \vdots&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;between $\mathbb{R}^i$ and 'horizontal' hyperplanes in $\mathbb{R}^{i+1}$. It is also possible to show that arbitrary hyperplanes (formally $n$-dimensional subspaces in the linear algebraic sense) of $\mathbb{R}^{i+1}$ are homeomorphic to $\mathbb{R}^i$, but the proof would require a change of basis and that isn't something we have the machinery to get into right now.&lt;/p&gt;
&lt;p&gt;Anyway, that's all the time I have right now and I think I've done enough to introduce product spaces. May this knowledge aid you in your quest and be your savior in many battles.&lt;/p&gt;
</content:encoded></item><item><title>Subspaces</title><description>A topological space is, at its core, just a set with some additional structure. So what if we want to keep the structure, but change the underlying set? There's an easy and somewhat obvious way to do this.</description><link>http://localhost:2368/subspaces/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae21e</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Sat, 08 Apr 2017 16:47:05 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#definition"&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples"&gt;Examples&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="definitionanamedefinition"&gt;Definition&lt;a name="definition"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In this post I'm going to introduce a classic method which allows us to construct new topological spaces from existing ones in a natural way.&lt;/p&gt;
&lt;p&gt;A topological space is, at its core, just a set with some additional structure. So what if we want to keep the structure, but change the underlying set? There's an easy and somewhat obvious way to do this.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $X$ denote a topological space with topology $\cal T$ and suppose $A\subseteq X$. Then ${\cal T}_A=\{A\cap U\mid U\in{\cal T}\}$ is called the &lt;strong&gt;subspace topology&lt;/strong&gt; on $A$. Equipped with this topology, we call $A$ a &lt;strong&gt;subspace&lt;/strong&gt; of $X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In standard English, this says that in the subspace topology on $A$, a set is open if it is the intersection of $A$ with some open set in $X$.&lt;/p&gt;
&lt;p&gt;Now we have to prove that we are justified in calling this thing a topology. We need to show that the empty set and the subspace itself are open, that unions of opens sets are open, and that finite intersections of open sets are open.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ denote a topological space with topology $\cal T$ and suppose $A\subseteq X$. Then ${\cal T}_A=\{A\cap U\mid U\in{\cal T}\}$ is a topology on $A$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Notice first that $\varnothing, X\in\cal T$ by the definition a topology. Certainly $\varnothing=A\cap\varnothing\in{\cal T}_A$, so the empty set is open in the subspace. Similarly, $A=A\cap X\in{\cal T}_A$ because $A\subseteq X$, so $A$ itself is open in the subspace.&lt;/p&gt;
&lt;p&gt;Next, suppose that $I$ is an indexing set such that $V_i\in{\cal T}_A$ is open in the subspace for each $i\in I$. Then for every $i\in I$ we have that $V_i=A\cap U_i$ for some open set $U_i\in{\cal T}$, by the definition of the subspace topology. Since $\bigcup\limits_{i\in I}U_i\in\cal T$ it is clear that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\bigcup\limits_{i\in I}V_i &amp;amp;= \bigcup\limits_{i\in I}(A\cap U_i) \\&lt;br&gt;
&amp;amp;= A\cap \bigcup\limits_{i\in I}U_i \\&lt;br&gt;
&amp;amp;\in{\cal T}_A,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;so we have shown that the union of any collection of open sets in the subspace is also open in the subspace.&lt;/p&gt;
&lt;p&gt;Finally, suppose that $V_i\in{\cal T}_A$ is open in the subspace for $1\leq i\leq n$ for some $n\in\mathbb{Z}^+$. Then for $1\leq i\leq n$, we again have that $V_i=A\cap U_i$ for some open set $U_i\in{\cal T}$, by the definition of the subspace topology. Since $\bigcap\limits_{i=1}^n U_i\in\cal T$, it is clear that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
\bigcap\limits_{i=1}^n V_i &amp;amp;= \bigcap\limits_{i=1}^n (A\cap U_i) \\&lt;br&gt;
&amp;amp;= A\cap \bigcap\limits_{i=1}^n U_i \\&lt;br&gt;
&amp;amp;\in {\cal T}_A,&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;so we have shown that the intersection of a finite collection of open sets in the subspace is also open in the subspace. It follows that the subspace topology on $A$ is a topology, as desired.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I have noticed that people often find the following observation confusing: If $A$ is a subset of a topological space $X$, then the set $A$ is both open and closed in the subspace topology on $A$. This is automatically true because of the definition of a topology. However, this says nothing at all about whether $A$ is open or closed as a &lt;em&gt;subset&lt;/em&gt; of $X$. Similarly, there are plenty of sets that may be open or closed in the subspace topology that may not have been that way in the original topology. Make sure you pay attention to the distinction between subspaces and subsets!&lt;/p&gt;
&lt;p&gt;Next, there's a way to figure out what sets in a subspace are closed which is sometimes more direct than trying to make sure that their complements are open.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $A$ be a subspace of a topological space $X$. Then $U\subseteq A$ is closed in $A$ if and only if $U=A\cap V$ for some closed set $V\subseteq X$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; First, suppose that $U$ is closed in $A$. Then $A-U$ is open by definition, so $A-U=A\cap W$ for some open set $W$ in $X$. Certainly $V=X-W$ is closed in $X$, so&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
U &amp;amp;= A-(A-U) \\&lt;br&gt;
&amp;amp;= A-(A\cap W) \\&lt;br&gt;
&amp;amp;= A-W \\&lt;br&gt;
&amp;amp;= A\cap (X-W) \\&lt;br&gt;
&amp;amp;= A\cap V.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Conversely, suppose that $U=A\cap V$ for some closed set $V$ in $X$. Then $X-V$ is open in $X$, so&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
A-U &amp;amp;= A-(A\cap V) \\&lt;br&gt;
&amp;amp;= A\cap (X-V)&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;is open in $A$. Thus, $U$ is closed in $A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I may not have explicitly proved all those set equalities in the past, but if they aren't immediately obvious to you then this might be a good time to go back and get some more practice with set-theoretic proofs.&lt;/p&gt;
&lt;h3 id="examplesanameexamples"&gt;Examples&lt;a name="examples"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Let's look at a few examples of subspaces of topological spaces we are already familiar with.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Consider $\mathbb{R}$ with the standard topology. Obviously the half-open interval $[0,1)$ is a subset of $\mathbb{R}$. So what is the subspace topology on $[0,1)$?&lt;/p&gt;
&lt;p&gt;Well, we already know it. The open sets are just intersections of $[0,1)$ with any open set in $\mathbb{R}$. Of course, the subspace $[0,1)$ is itself both open and closed.&lt;/p&gt;
&lt;p&gt;Is $(0,1)$ open in the subspace topology? Of course it is. Certainly $(0,1)$ is open in $\mathbb{R}$, and $(0,1)=[0,1)\cap (0,1)$, so it is open in the subspace as well.&lt;/p&gt;
&lt;p&gt;Play around with this a little bit more and you'll notice that the open sets in $[0,1)$ actually look a lot like the open sets in $\mathbb{R}$ which happen to be subsets of $[0,1)$. This is just a testament to the fact that the subspace topology is a very natural object. In fact, whenever I talk about an interval, I'll generally be assuming that it is equipped with the subspace topology induced by the standard topology on $\mathbb{R}$. Even though I won't always explicitly mention this, it will be especially important to remember when I talk about paths and homotopy in the future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's look at another example, shall we?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. Consider $\mathbb{R}^2$ in the standard topology, and the set $A=\{(x,0)\in\mathbb{R}^2\mid x\in\mathbb{R}\}$.&lt;sup class="footnote-ref"&gt;&lt;a href="#fn1" id="fnref1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; The open sets in the subspace topology on $A$ are, of course, any sets which can be expressed as the intersection of $A$ with unions of open balls in $\mathbb{R}^2$.&lt;/p&gt;
&lt;p&gt;This looks an awful lot like the standard topology on $\mathbb{R}$, but technically it isn't because $A\ne\mathbb{R}$. In reality, $A$ is just the $x$-axis in the two-dimensional Euclidean plane. So it's basically $\mathbb{R}$, and behaves exactly like it. What we have, then, is that $A$ is homeomorphic to $\mathbb{R}$ with the standard topology.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's prove the assertion I just made. It's going to be a bit of work, but I think it's worth proving things just for fun once in a while. The homeomorphism we will choose in our proof is the obvious choice, but there are actually others that could work just as well.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; Consider $\mathbb{R}$ with the standard topology and $A=\{(x,0)\in\mathbb{R}^2\mid x\in\mathbb{R}\}$ as a subspace of $\mathbb{R}^2$ with the standard topology. The spaces $\mathbb{R}$ and $A$ are homeomorphic.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We argue that the function $f:A\to\mathbb{R}$ defined by $f\big((x,0)\big)=x$ is a homeomorphism. It is trivial to check that $f$ is bijective and that its inverse function $f^{-1}:\mathbb{R}\to A$ is given by $f^{-1}(x)=(x,0)$.&lt;/p&gt;
&lt;p&gt;We will show first that $f^{-1}$ is continuous. Choose an open set $U$ in $A$. By definition, we can write $U$ as the intersection of $A$ with some union of open balls in $\mathbb{R}^2$. That is,&lt;/p&gt;
&lt;p&gt;$$U=A\cap\bigcup\limits_{i\in I} B\big((x_i,y_i),r_i\big)$$&lt;/p&gt;
&lt;p&gt;for some indexing set $I$, where $(x_i,y_i)\in\mathbb{R}^2$ and $r_i\in\mathbb{R}^+$ for each $i\in I$. It follows that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
(f^{-1})^{-1}[U] &amp;amp;= f[U] \\&lt;br&gt;
&amp;amp;= f\left[A\cap\bigcup\limits_{i\in I}B\big((x_i,y_i),r_i\big)\right] \\&lt;br&gt;
&amp;amp;= f\left[\bigcup\limits_{i\in I}A\cap B\big((x_i,y_i),r_i\big)\right] \\&lt;br&gt;
&amp;amp;= \bigcup\limits_{i\in I}f\Big[A\cap B\big((x_i,y_i),r_i\big)\Big].&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;A small amount of geometry and the Pythagorean Theorem gets us that for each $i\in I$, the set $f\Big[A\cap B\big((x_i,y_i),r_i\big)\Big]$ is equal to the open interval&lt;/p&gt;
&lt;p&gt;$$\left(x_i-\sqrt{r_i^2-y_i^2}, x_i+\sqrt{r_i^2-y_i^2}\right)$$&lt;/p&gt;
&lt;p&gt;if $r_i&amp;gt;y_i$, and it is empty if $r_i\leq y_i$. The union of open intervals is open in $\mathbb{R}$, so $(f^{-1})^{-1}[U]$ is open in $\mathbb{R}$ and thus $f^{-1}$ is continuous.&lt;/p&gt;
&lt;p&gt;Next, we will show that $f$ is continuous. Choose an open set $U$ in $\mathbb{R}$. By definition, $U$ is a union of open intervals in $\mathbb{R}$ (possibly $\mathbb{R}$ itself, in which case $f^{-1}[U]=f^{-1}[\mathbb{R}]=A$ is certainly open). More precisely, we can write&lt;/p&gt;
&lt;p&gt;$$U=\bigcup\limits_{i\in I}(x_i-r_i,x_i+r_i),$$&lt;/p&gt;
&lt;p&gt;where $I$ is some indexing set such that $x_i\in\mathbb{R}$ and $r_i\in\mathbb{R}^+$ for every $i\in I$. It follows that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
f^{-1}[U] &amp;amp;= f^{-1}\left[\bigcup\limits_{i\in I}(x_i-r_i,x_i+r_i)\right] \\&lt;br&gt;
&amp;amp;= f^{-1}\left[\bigcup\limits_{i\in I}\left\{x\in\mathbb{R}\bigg\vert \vert x-x_i\vert&amp;lt; r_i\right\}\right] \\&lt;br&gt;
&amp;amp;= \bigcup\limits_{i\in I}f^{-1}\left[\left\{x\in\mathbb{R}\bigg\vert \vert x-x_i\vert&amp;lt; r_i\right\}\right] \\&lt;br&gt;
&amp;amp;= \bigcup\limits_{i\in I}\left\{(x,0)\in\mathbb{R}^2\bigg\vert\vert x-x_i\vert&amp;lt; r_i\right\} \\&lt;br&gt;
&amp;amp;= \bigcup\limits_{i\in I}\bigg(A\cap B\big((x_i,0),r_i\big)\bigg) \\&lt;br&gt;
&amp;amp;= A\cap \bigcup\limits_{i\in I}B\big((x_i,0),r_i\big).&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;The union of open balls is open, so $f^{-1}[U]$ is the intersection of $A$ with an open set in $\mathbb{R}$ and is thus open in $A$. It follows that $f$ is continuous, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It ended up being a bit long, but not too difficult. And I think this example really gets the point across that even though homeomorphic spaces may not be identical, they behave in essentially the same way. Most people aren't as careful as I just was, and frequently write that $\mathbb{R}$ is a subspace of $\mathbb{R}^2$. This isn't strictly true, but at least you now know how to interpret it.&lt;/p&gt;
&lt;p&gt;Just so you know, we will see another proof of the above proposition when I talk about product spaces. I just really wanted to prove it this way once to give you some geometric intuition behind why it's true.&lt;/p&gt;
&lt;p&gt;The last thing I'm going to do in this post is show that an open interval in $\mathbb{R}$ is homeomorphic to $\mathbb{R}$ itself. This will reinforce the notion that stretching a space does not alter its topological properties.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Proposition.&lt;/strong&gt; Let $a,b\in\mathbb{R}$ with $a&amp;lt; b$. Then $\mathbb{R}$ and the interval $(a,b)$ are homeomorphic.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;quot;Proof.&amp;quot;&lt;/strong&gt; Define $f:(a,b)\to\mathbb{R}$ by&lt;/p&gt;
&lt;p&gt;$$f(x)=\frac{1}{x-a}+\frac{1}{x-b}.$$&lt;/p&gt;
&lt;p&gt;We will argue that $f$ is a homeomorphism. I am not going to check that $f$ is a bijection, although it is obvious just from looking at its graph. This could be done using the quadratic formula to find an explicit inverse function, being careful to restrict $f^{-1}$ so that it is surjective. It could also be done using calculus to show that this function is injective because it is monotonically decreasing and surjective by the intermediate value theorem because it approaches negative infinity at its left endpoint and positive infinity at its right endpoint.&lt;/p&gt;
&lt;p&gt;The function $f$ is clearly continuous because it is a rational function whose denominator is nonzero for every $x\in(a,b)$. Its inverse, $f^{-1}$ is also continuous for the same reason, but since I haven't explicitly computed it I will not show that this is the case.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Okay, so that really wasn't a proof at all. But again, a quick glance at the graph of $f$ should really be enough to convince you that it's a homeomorphism. I could be more precise in giving an argument that any two open intervals are homeomorphic, but I think this post has gone on long enough already, and hopefully this fact is obvious to you. On the other hand, closed intervals are &lt;em&gt;not&lt;/em&gt; homeomorphic to open intervals. We don't have enough tools to prove that at this point, but it will become trivial when I talk about compactness in a later post.&lt;/p&gt;
&lt;hr class="footnotes-sep"&gt;
&lt;section class="footnotes"&gt;
&lt;ol class="footnotes-list"&gt;
&lt;li id="fn1" class="footnote-item"&gt;&lt;p&gt;$(x,0)$ is not an open interval here, but a point in $\mathbb{R}^2$. That is, it's an ordered pair of real numbers. The fact that open intervals and ordered pairs have the same notation is unfortunate, but if we are careful it should always be clear which we are talking about. &lt;a href="#fnref1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content:encoded></item><item><title>Continuity and Homeomorphisms</title><description>The concept of continuity is central to the study of topology. So much so, in fact, that whenever anyone talks about a map between topological spaces, they generally expect you to know that they're talking about a continuous map.</description><link>http://localhost:2368/continuity-and-homeomorphisms/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae21d</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Sat, 08 Apr 2017 03:08:10 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#continuity"&gt;Continuity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#homeomorphisms"&gt;Homeomorphisms&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="continuityanamecontinuity"&gt;Continuity&lt;a name="continuity"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The concept of continuity is central to the study of topology. So much so, in fact, that whenever anyone talks about a map between topological spaces, they generally expect you to know that they're talking about a continuous map.&lt;sup class="footnote-ref"&gt;&lt;a href="#fn1" id="fnref1"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;I've heard it said that a function is continuous if you can draw its graph without lifting your pencil. This is an awful, imprecise definition, but it does give us a hint of intuition regarding the nature of continuity.&lt;/p&gt;
&lt;p&gt;So then what is continuity? Let's take a look at pieces of two functions, $f,g:\mathbb{R}\to\mathbb{R}$.&lt;sup class="footnote-ref"&gt;&lt;a href="#fn2" id="fnref2"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/i_can_haz_continuous.svg" alt="I can haz continuous?"&gt;&lt;/p&gt;
&lt;p&gt;Your gut instinct should tell you that $f$ is continuous on the interval depicted but $g$ is not. So what does your gut know that you don't? Here's a (very) rough description:&lt;/p&gt;
&lt;p&gt;If you zoom in far enough on any point in the graph of $f$, the whole graph behaves similarly in that region. On the other hand, there are pieces of the graph of $g$ which look nothing like nearby pieces.&lt;/p&gt;
&lt;p&gt;This is just an informal way of saying that if a function is continuous, points that are close together get mapped to points that are close together. This phrasing is not only much more revealing, it also allows us to extend the definition of continuity away from real-valued functions.&lt;/p&gt;
&lt;p&gt;Let's make this notion more precise, in the context of metric spaces. I'll give you the definition first, and then try to make sense of it for you.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $X$ and $Y$ denote metric spaces. A function $f:X\to Y$ is &lt;strong&gt;continuous at the point&lt;/strong&gt; $x\in X$ if for every real number $\epsilon&amp;gt;0$, there exists a real number $\delta&amp;gt;0$ such that $f\big[B(x,\delta)\big]\subseteq B\big(f(x),\epsilon\big)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A function $f:X\to Y$ is &lt;strong&gt;continuous&lt;/strong&gt; if it is continuous at every point $x\in X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All the definition really says is that if a function is continuous at $x$, no matter how closely you look at $f(x)$ there will always be points around $x$ which get mapped within that distance of $f(x)$. So like I said earlier, points that are close together get mapped to points that are close together.&lt;/p&gt;
&lt;p&gt;If that makes perfect sense to you, skip the next few paragraphs. Otherwise, I'm going to explain this definition to you very slowly through a story.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Think of the person you loathe the most in this world. I'll refer to this person as &amp;quot;the enemy&amp;quot; and use the pronoun &amp;quot;it.&amp;quot;&lt;/p&gt;
&lt;p&gt;Say you have two metric spaces $X$ and $Y$, and a continuous map $f:X\to Y$ between them:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/some_spaces.svg" alt="some spaces"&gt;&lt;/p&gt;
&lt;p&gt;You are omnipotent, as in real life, and you know everything there is to know about $X,Y$ and $f$. However, the enemy, being a foolish fool, mistakenly thinks that $f$ is not continuous. And so it tries to trick you. It plots a point $x\in X$ and its image $f(x)\in Y$. Then it draws an open ball $B$ of radius $\epsilon$ around $f(x)$.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/turn_1.svg" alt="turn 1"&gt;&lt;/p&gt;
&lt;p&gt;The enemy challenges you to find an open ball $B'$ around $x$ whose image is contained in $B$. So you think for a while and you use your information about $f$ and you quickly come up with such an open ball, with radius $\delta$.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/turn_2.svg" alt="turn 2"&gt;&lt;/p&gt;
&lt;p&gt;The enemy is angry. It chooses a new smaller ball $B$ and you draw a new, smaller $B'$ to compensate. You do it faster this time, because you've noticed a pattern. No matter what radius $\epsilon$ it chooses to make $B$, you've come up with a method to calculate the radius $\delta$ that $B'$ needs to be so that $f[B']$ will fit inside $B$.&lt;/p&gt;
&lt;p&gt;You tell the enemy your method, and it has no choice but to conclude that $f$ is continuous at $x$. Next, you demonstrate how to do this for any point $x\in X$, and reason that $f$ is continuous. The enemy's head promptly explodes.&lt;/p&gt;
&lt;p&gt;Let's look at a specific example. Let $X$ be a metric space and consider the identity function $i:X\to X$ defined by $i(x)=x$ for every $x\in X$. The enemy first chooses a point $x_0\in X$ and draws a ball of radius $\epsilon$ around $x_0=i(x_0)$. You realize immediately that&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
i\big[B(x_0,\epsilon)\big] &amp;amp;= B(x_0,\epsilon) \\&lt;br&gt;
&amp;amp;= B\big(i(x_0),\epsilon\big)&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;because $i$ is the identity function. So no matter what point $x_0$ or radius $\epsilon$ the enemy chooses, picking $\delta=\epsilon$ will always work. This argument shows that the identity function on any metric space is continuous!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Hopefully the definition of continuity makes sense to you now, because I'm about move on to an equivalent and simpler definition.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ and $Y$ denote metric spaces with $f: X\to Y$. Then $f$ is continuous if and only if the preimage $f^{-1}[U]$ of any open set $U\subseteq Y$ is open in $X$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose first that $f:X\to Y$ is continuous. If $f^{-1}[U]=\varnothing$ then we are done because this is open in $X$. Suppose then that $f^{-1}[U]\ne\varnothing$. Choose $x\in f^{-1}[U]$, so that $f(x)\in U$. Since $U$ is open, there exists a real number $\epsilon&amp;gt;0$ such that $B\big(f(x),\epsilon\big)\subseteq U$. Since $f$ is continuous, there exists a real number $\delta&amp;gt;0$ such that $f\big[B(x,\delta)\big]\subseteq B\big(f(x),\epsilon\big)$. That is, $B(x,\delta)\subseteq f^{-1}\big[B\big(f(x),\epsilon\big)\big]\subseteq f^{-1}[U]$. Thus, there is an open ball centered at $x$ which is contained in $f^{-1}[U]$, so clearly $f^{-1}[U]$ is open in $X$.&lt;/p&gt;
&lt;p&gt;Suppose conversely that $f^{-1}[U]$ is open in $X$ whenever $U$ is open in $Y$. Choose $x\in X$ and a real number $\epsilon&amp;gt;0$. Since $B\big(f(x),\epsilon\big)$ is open in $Y$, we have that $f^{-1}\big[B\big(f(x),\epsilon\big)\big]$ is open in $X$. Since $x\in f^{-1}\big[B\big(f(x),\epsilon\big)\big]$, there exists a real number $\delta&amp;gt;0$ such that $B(x,\delta)\subseteq f^{-1}\big[B\big(f(x),\epsilon\big)\big]$. Thus, $f$ is continuous.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is most excellent. We've arrived at a &amp;quot;distance-free&amp;quot; definition of continuity, phrased entirely in terms of open sets. You know what that means! We're going to adopt this as the definition of continuity in a general topological setting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces. A function $f:X\to Y$ is &lt;strong&gt;continuous&lt;/strong&gt; if $f^{-1}[U]$ is open in $X$ whenever $U$ is open in $Y$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is also a corresponding definition for continuity at a point, but we will never need it.&lt;/p&gt;
&lt;p&gt;As is oft desired, we would like a way to determine whether a function between topological spaces is continuous, given only a basis for each space. This is done easily and naturally.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces, let ${\cal B}_X$ be a basis for the topology on $X$ and ${\cal B}_Y$ a basis for the topology on $Y$. A function $f:X\to Y$ is continuous if and only if $f^{-1}[B]$ is open in $X$ whenever $B\in{\cal B}_Y$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose first that $f$ is continuous. Since $B$ is a basis element, it is open in $Y$. It follows immediately that $f^{-1}[B]$ is open in $X$.&lt;/p&gt;
&lt;p&gt;Suppose next that the preimage of any basis element in $Y$ is open in $X$. It has been established that for any open set  $U\subseteq Y$, there exists an indexing set $I$ and basis elements $B_i\in{\cal{B}}_Y$ such that  $U=\bigcup\limits_{i\in I}B_i$. Thus,&lt;/p&gt;
&lt;p&gt;$$f^{-1}[U]=f^{-1}\left[\bigcup\limits_{i\in I}B_i\right]=\bigcup\limits_{i\in I}f^{-1}[B_i],$$&lt;/p&gt;
&lt;p&gt;which is open since it is the union of open sets.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yay! Now let's use our beautiful definition of continuity to prove an elementary property of continuous maps: that the composition of two continuous functions&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X,Y$ and $Z$ denote topological spaces with continuous functions $f:X\to Y$ and $g:Y\to Z$. Then their composition $(g\circ f):X\to Z$, defined by $(g\circ f)(x)=g\big(f(x)\big)$ for every $x\in X$, is continuous.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Suppose $U\subseteq Z$ is open. Then $g^{-1}[U]\subseteq Y$ is open, so $f^{-1}\big[g^{-1}[U]\big]\subseteq X$ is open. Thus, it suffices to show that $f^{-1}\big[g^{-1}[U]\big]=(g\circ f)^{-1}[U]$.&lt;/p&gt;
&lt;p&gt;Suppose $x\in f^{-1}\big[g^{-1}[U]\big]$. Then $f(x)\in g^{-1}[U]$, so $g\big(f(x)\big)=(g\circ f)(x)\in U$. Thus, $x\in (g\circ f)^{-1}[U]$.&lt;/p&gt;
&lt;p&gt;Suppose next that $x\in (g\circ f)^{-1}[U]$. Then $(g\circ f)(x)=g\big(f(x)\big)\in U$, so $f(x)\in g^{-1}[U]$ and thus $x\in f^{-1}\big[g^{-1}[U]\big]$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="homeomorphismsanamehomeomorphisms"&gt;Homeomorphisms&lt;a name="homeomorphisms"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Ages ago, when I first started talking about topological spaces, I mentioned the idea of topological equivalence. I believe I said something like &amp;quot;two spaces are topologically equivalent if they can be continuously deformed into each other,&amp;quot; and that &amp;quot;continuous deformations are ways we can bend, stretch and move spaces without tearing, cutting or gluing them.&amp;quot; We now have the machinery to formally define this notion.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $X$ and $Y$ denote topological spaces. A bijective function $f:X\to Y$ is a &lt;strong&gt;homeomorphism&lt;/strong&gt; if both $f$ and $f^{-1}:X\to Y$ are continuous. We say that the spaces are &lt;strong&gt;homeomorphic&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is particularly important that $f$ is bijective, since otherwise $f^{-1}$ would not be well defined.&lt;/p&gt;
&lt;p&gt;There is no consistent symbol for &amp;quot;is homeomorphic to,&amp;quot; although I've seen several used by different authors. For instance, $X\simeq Y$, $Y\approx Y$ or $X\sim Y$ could all be used. And I've seen them all used. I won't use any such notation, however, because I find it terribly confusing.&lt;/p&gt;
&lt;p&gt;It should go without saying that if a function is a homeomorphism then so is its inverse. The next result is perhaps a tad less obvious, although its proof is almost trivial.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Let $X, Y$ and $Z$ denote topological spaces with homeomorphisms $f:X\to Y$ and $g:Y\to Z$. Then $X$ is homeomorphic to $Z$ via $(g\circ f):X\to Z$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $f,g$ are bijective and continuous, $g\circ f$ is bijective and continuous. Furthermore, since $f^{-1},g^{-1}$ are bijective and continuous, $(g\circ f)^{-1}=f^{-1}\circ g^{-1}$ is bijective and continuous. Thus $g\circ f$ is a homeomorphism.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Unfortunately, it is difficult to give simple examples of homeomorphic spaces without first discussing the constructions of new spaces, so more examples of homeomorphisms won't come for a few posts. We can, however, talk about what homeomorphism really means.&lt;/p&gt;
&lt;p&gt;Given a homeomorphism $f$, we know that $f$ and its inverse $f^{-1}$ are continuous. This means that open sets get mapped to open sets in both directions. So essentially, homeomorphisms preserve open sets. Why is this nice? &lt;strong&gt;Because topological spaces are defined in terms of open sets.&lt;/strong&gt; This indicates that homeomorphic spaces are really the same, just with their open sets renamed in some way.&lt;/p&gt;
&lt;p&gt;There is a lot more to say about homeomorphisms. In fact, much of what topologists do is in an attempt to identify which spaces are homeomorphic.&lt;sup class="footnote-ref"&gt;&lt;a href="#fn3" id="fnref3"&gt;[3]&lt;/a&gt;&lt;/sup&gt; And it isn't always obvious directly from the definition. All the tools I bring up from now on will be useful on this quest.&lt;/p&gt;
&lt;p&gt;We will mainly be interested in things called &lt;strong&gt;topological invariants&lt;/strong&gt;, which are characteristics of spaces that are preserved under homeomorphisms. If two spaces have different invariants, this tells us that these spaces are &lt;em&gt;not&lt;/em&gt; homeomorphic. As we shall see, connectedness, compactness, countability, separation conditions, homology and homotopy groups are all topological invariants. These are all properties of the open sets of spaces, so this makes sense. Properties like boundedness and completeness, on the other hand, which rely on the notion of distance, are not topological invariants.&lt;/p&gt;
&lt;hr class="footnotes-sep"&gt;
&lt;section class="footnotes"&gt;
&lt;ol class="footnotes-list"&gt;
&lt;li id="fn1" class="footnote-item"&gt;&lt;p&gt;If I haven't mentioned this before, &amp;quot;map&amp;quot; is just another word for &amp;quot;function&amp;quot; and the two are often used interchangeably. &lt;a href="#fnref1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn2" class="footnote-item"&gt;&lt;p&gt;I'm not sure why the arrows in my diagram are on sideways, but I'm too lazy to fix it. &lt;a href="#fnref2" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn3" class="footnote-item"&gt;&lt;p&gt;Although we are often willing to settle for homotopy equivalence. &lt;a href="#fnref3" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content:encoded></item><item><title>Limit Points, Closure, Boundary and Interior</title><description>It's fairly common to think of open sets as sets which do not contain their boundary, and closed sets as sets which do contain their boundary. The trouble here lies in defining the word 'boundary.'</description><link>http://localhost:2368/limit-points-closure-boundary-and-interior/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae21c</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Sat, 08 Apr 2017 00:34:07 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#limit-points"&gt;Limit Points&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#closure"&gt;Closure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#boundary"&gt;Boundary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#interior"&gt;Interior&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;We are nearly ready to begin making some distinctions between different topological spaces. Distinguishing between fundamentally different spaces lies at the heart of the subject of topology, and it will occupy much of our time. However, before we can really dig in, we're going to need some additional tools.&lt;/p&gt;
&lt;p&gt;On a different note, and I've been meaning to mention this, I'd be willing to bet that you probably had a different sense of what open and closed sets were before learning any topology. It's fairly common to think of open sets as sets which do not contain their boundary, and closed sets as sets which do contain their boundary. The trouble here lies in defining the word 'boundary.' This is finally about to be addressed, first in the context of metric spaces because it is easier to see why the definitions are natural there. As in prior posts, these concepts generalize easily to topological space.&lt;/p&gt;
&lt;h3 id="limitpointsanamelimitpoints"&gt;Limit Points&lt;a name="limit-points"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;If you've had any calculus, you've probably had some experience with limits. Whether or not the formal $\epsilon$-$\delta$ definition of a limit is familiar to you is of no real consequence. Even though this definition is extremely insightful, it isn't really necessary for our purposes. In fact, if we aren't working in a metric space then this definition doesn't even apply. The good news it that many definitions in topology have a sort of &lt;em&gt;too-good-to-be-true&lt;/em&gt; feel to them, since they're often deceptively simple.&lt;/p&gt;
&lt;p&gt;Limit points are not the same type of limit that you encounter in a calculus or analysis class, but the underlying idea is similar. Informally, a point in a metric space is a limit point of some subset if it is arbitrarily close to other points in that subset. What exactly does this mean? It means that no matter how closely we zoom in on a limit point, there will always be another point in its immediate vicinity which belongs to the subset in question. Thus, we can in some sense approximate limit points to arbitrary accuracy using other points in the subset. This might be a little confusing or overwhelming, so I'll give you the definition now and you'll see that it isn't too bad. Its simplicity may even be a bit misleading.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $A$ denote a subset of a metric space $X$. A point $p\in X$ is a &lt;strong&gt;limit point&lt;/strong&gt; of $A$ if every open ball centered at $p$ contains a point $x\in A$ with $x\neq p$. We write $L(A)$ to denote the set of limit points of $A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The first thing that I will emphasize is that a limit point of a set does not need to belong to that set! All that is necessary is that there are points in the set &lt;em&gt;as close as we like&lt;/em&gt; to the limit point. This can be made more obvious by rephrasing the definition slightly. By &amp;quot;every open ball,&amp;quot; what we mean is that for every real number $\epsilon&amp;gt;0$ there exists some point $x\in A$ distinct from $p$ such that $x\in B(p,\epsilon)$. Take a moment to appreciate and understand that this definition ensures what I claim it does.&lt;/p&gt;
&lt;p&gt;There's a sort of dual notion as well, which is called an isolated point. The following definition makes it clear that any point in a subset of a metric space is either a limit point or an isolated point.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $A$ denote a subset of a metric space $X$. A point $p\in X$ is an &lt;strong&gt;isolated point&lt;/strong&gt; of $A$ if there exists an open ball centered at $p$ which contains no other points of $A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's look at an example, to make sure we're fully on board with these concepts before we press onward. I'm going to cordon off the entire example so that we don't accidentally make sweeping generalizations about every metric space based on this one scenario.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; Take $X=\mathbb{R}^2$ equipped, of course, with the standard metric $d:X\times X\to\mathbb{R}$. Let $A=B\big((0,0),1\big)$, the open unit ball centered at the origin. Consider the following points in $X$:&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
x &amp;amp;= (0,0), \\&lt;br&gt;
y &amp;amp;= (1,0), \\&lt;br&gt;
z &amp;amp;= (1,1). \\&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;I'll even throw in a diagram to make things crystal clear:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/limit_point_example.svg" alt="limit point example"&gt;&lt;/p&gt;
&lt;p&gt;Clearly $x\in A$ because $d(x,x)=0&amp;lt; r$. However, $y,z\notin A$ because $d(x,y)=r$ and $d(x,z)=\sqrt{2}&amp;gt;r$. Which, if any, of the points $x,y,z$ are limit points of $A$?&lt;/p&gt;
&lt;p&gt;Let's consider $x$ first. Does every open ball centered at $x$ contain a point in $A$ distinct from $x$? The simple answer is &amp;quot;well DUH,&amp;quot; but let's be a tad more rigorous.&lt;/p&gt;
&lt;p&gt;Choose a real number $\epsilon&amp;gt;0$. We consider separately the cases where $\epsilon\geq 1$ and $0&amp;lt;\epsilon&amp;lt;1$. If $\epsilon\geq 1$ then $A\subseteq B(x,\epsilon)$ and we are done because there are certainly points in $A$ distinct from $x$. If $0&amp;lt;\epsilon&amp;lt;1$, it is clear that the point $x_0=(0,\epsilon/2)\in B(x,\epsilon)$. Furthermore, $x_0\neq x$ because $d(x,x_0)&amp;gt;0$.&lt;/p&gt;
&lt;p&gt;So $x$ is a limit point of $A$, which was sort of obvious from the start. A similar argument could be made that any point $a\in A$ is a limit point of $A$ by adding a few more technical details to the above argument, but I won't bother because it feels like overkill.&lt;/p&gt;
&lt;p&gt;Next, let's take a look at $y$. You should have a sense that $y$ is a limit point of $A$, because even though it is not in $A$ it is &lt;em&gt;superextraclose&lt;/em&gt; to lots and lots of points in $A$. Let's show that our intuition is correct.&lt;/p&gt;
&lt;p&gt;Again, choose a real number $\epsilon&amp;gt;0$ and consider the open ball $B(y,\epsilon)$. The point $y_0=(0,1-\epsilon/2)$ is clearly in $B(y,\epsilon)$ because $d(y,y_0)=\epsilon/2&amp;lt;\epsilon$ and thus it is also distinct from $y$. Furthermore, $y_0\in A$ since $d(x,y_0)=1-\epsilon/2&amp;lt;1$.&lt;/p&gt;
&lt;p&gt;We've shown then that $y$ is a limit point of $A$, which is somewhat more interesting. A similar argument would show that any point $a\in X$ with $d(a,x)=1$ is a limit point of $A$. Sooo... even though we haven't defined the concept of &amp;quot;boundary&amp;quot; yet, it's looking like any point on what we intuitively perceive to be the boundary of $A$ is a limit point of $A$. This is an important realization.&lt;/p&gt;
&lt;p&gt;Finally, let's show that $z$ is not a limit point of $A$. To show that the necessary property is not true for every open ball centered at $z$, all we need to do is demonstrate a single open ball centered at $z$ which contains no points of $A$. This is easy! Just take as the radius any positive $\epsilon&amp;lt;\sqrt{2}-1$ and that's essentially all there is to it.&lt;/p&gt;
&lt;p&gt;More formally, choose $\epsilon&amp;lt;\sqrt{2}-1$ and consider $z_0\in B(z,\epsilon)$. By the definition of this open ball $d(z,z_0)&amp;lt;\epsilon$. By the triangle inequality,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
d(x,z) &amp;amp;= \sqrt{2} \\&lt;br&gt;
&amp;amp;\leq d(x,z_0) + d(z_0,z) \\&lt;br&gt;
&amp;amp;&amp;lt; d(x,z_0) + \epsilon \\&lt;br&gt;
&amp;amp;&amp;lt; d(x,z_0)+\sqrt{2}-1.&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;Rearranging the above inequality, we have that $d(x,z_0)&amp;gt;1$ and so $z_0\notin A$. Thus, $z$ is not a limit point of $A$. Once again, a similar argument would show that, in general, any point $p\in X$ with $d(p,x)&amp;gt;1$ is not a limit point of $A$.&lt;/p&gt;
&lt;p&gt;What can we conclude about this particular set $A$? The limit points of $A$ are every point in $A$ as well as every point on the unit circle $S^1$. That is, $L(A)=A\cup S^1=\overline{B}(x,r)$. This is the closed ball with the same center and radius as $A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We shall see soon enough that this is no accident. For any subset $A$ of a metric space $X$, it happens that the set of limit points $L(A)$ is closed. Let's prove something even better.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; A subset of a metric space is closed if and only if it contains all of its limit points.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We argue first that if $L(A)\subseteq A$ then $A$ is closed. It suffices to show that $X-A$ is open. Choose a point $x\in X-A$. Clearly $x$ is not a limit point of $A$ since $x\notin A$ and thus $x\notin L(A)\subseteq A$. Thus there exists some open ball $B$ centered at $x$ which does not contain any points in $A$. It follows that $B\subseteq X-A$ and so $X-A$ is open.&lt;/p&gt;
&lt;p&gt;We argue next that if $A$ is closed then $L(A)\subseteq A$. Since $A$ is closed, we know that $X-A$ is open. Thus for any $x\in X-A$ there exists some open ball centered at $x$ which is strictly contained in $X-A$ and therefore contains no points of $A$. It follows that $x$ is not a limit point of $A$, so any limit points of $A$ are contained in $A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hooray! We now have an alternative, and often easier to use, definition of a closed set in a metric space! What's awesome is that nothing we've done (other than our example) depends on the metric, so we can immediately abstract everything to the setting of topological spaces!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $A$ denote a subset of a topological space $X$. A point $p\in X$ is a &lt;strong&gt;limit point&lt;/strong&gt; of $A$ if every neighborhood of $p$ contains a point $x\in A$ with $x\neq p$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The theorem we just proved translates as well, if we simply replace open balls with neighborhoods each time the appear in the proof.&lt;/p&gt;
&lt;p&gt;As always, there's nothing contradictory about the fact that the empty set is closed. Certainly the empty set contains all of its limit points, since it contains no points at all.&lt;/p&gt;
&lt;h3 id="closureanameclosure"&gt;Closure&lt;a name="closure"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We will now define the closure of a subset of a topological space. We will see later that taking the closure of a set is equivalent to include the set's boundary.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $A$ denote a subset of a topological space $X$. The &lt;strong&gt;closure&lt;/strong&gt; of $A$ is the intersection of all closed set in $X$ which contain $A$. We denote the closure of a $A$ by $\overline A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The closure of a set is always closed, because it is the intersection of closed sets. Furthermore, it is obvious that any closed set must equal its own closure. Intuitively, $\overline A$ is the smallest closed set which contains $A$. This is because, by definition, any closed set containing $A$ must also contain $\overline A$. Even more intuitively, the closure of $A$ is the union of $A$ with all of its limit points. Let's prove that this is true.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $A$ is a subset of a topological space, then $\overline A=A\cup L(A)$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; We argue first that $A\cup L(A)\subseteq\overline A$. Clearly $A\subseteq\overline A$ since $\overline A$ is the intersection of all closed sets containing $A$, and thus itself contains $A$. It remains to show then that $L(A)\subseteq\overline A$, which we do by contraposition. Suppose $x\notin\overline A$ so that $x\in X-\overline A$. Since $\overline A$ is closed, $X-\overline A$ is open and is thus a neighborhood of $x$ which contains no points in $A$. Thus $x\notin L(A)$.&lt;/p&gt;
&lt;p&gt;We argue next that $\overline A\subseteq A\cup L(A)$. If $x\in A$ then the proof is immediate. If $x\in\overline A -A$, then by definition $x$ is in every closed set containing $A$. It follows that for any open set $U\subseteq X-A$, we have $x\notin U$. Thus any neighborhood of $x$ intersects $A$ in some point $p$. Since $x\notin A$, clearly $x\neq p$ and thus $x$ is a limit point of $A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It's a somewhat gross proof, but a nice and useful result, and one that we will use often.&lt;/p&gt;
&lt;h3 id="boundaryanameboundary"&gt;Boundary&lt;a name="boundary"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Next let's formalize the concept of boundary. It's easy and intuitive to think about the boundary of a ball, so let's start there. Let's say we have an open and closed ball of the same center and radius in some metric space. They obviously have the same boundary — the circle with the same radius as these balls. Even though these points don't belong to the open ball, they are &lt;em&gt;just&lt;/em&gt; touching its outer edge.&lt;/p&gt;
&lt;p&gt;Now what about this set, as a subset of $\mathbb{R}^2$ in the standard metric?&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/vomitous_mass.svg" alt="vomitous mass"&gt;&lt;/p&gt;
&lt;p&gt;Trying to calculate the boundary of this set is a bit more difficult than just drawing a circle. Does that loop at the top right count as boundary? What about the points sitting by themselves? Do those inner circles count as well, or does the boundary have to enclose the set? It turns out that, the way we define boundary, the answer to all of these questions is yes.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $A$ be a subset of a metric space $X$. A point $p\in X$ is a &lt;strong&gt;boundary point&lt;/strong&gt; of $A$ if every open ball centered at $p$ contains at least one point in $A$ and one point in $X-A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;boundary&lt;/strong&gt; of $A$ is the set of all boundary points of $A$. We denote it by $\partial A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This makes a lot of sense! No matter how tiny an open ball we choose around a boundary point, it will always intersect both $A$ and its complement. That is, it will always contain points that are in $A$ and points that are not in $A$.&lt;/p&gt;
&lt;p&gt;These definitions are identical in a topological space if we again replace open balls with neighborhoods. I won't even bother restating them.&lt;/p&gt;
&lt;p&gt;It turns out we could already have defined the boundary of a set without the notion of boundary points!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $A$ is a subset of a topological space $X$, then $\partial A=\overline A\cap\overline{X-A}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose $x\in\partial A$. By definition, every neighborhood of $x$ contains a point in $A$ and a point in $X-A$, so $x\in\overline A$ and $x\in\overline{X-A}$. Thus $x\in\overline A\cap\overline{X-A}$ and so $\partial A\subseteq \overline A\cap\overline{X-A}$.&lt;/p&gt;
&lt;p&gt;The proof that $\overline A\cap\overline{X-A}\subseteq\partial A$ is precisely the same as above, with the steps reversed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This means that the boundary of $A$ is the intersection of the smallest closed set containing $A$ and the smallest closed set containing its complement, which hopefully seems reasonable to you.&lt;/p&gt;
&lt;h3 id="interioranameinterior"&gt;Interior&lt;a name="interior"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I'm not going to bother with metric spaces for this part, since I think you get the idea. You can easily translate all the results about topological spaces back into metric spaces if you'd like. Intuitively, we can think of the interior of a set as everything in the set which does not belong to its boundary. This is actually not the definition we'll initially give, although we shall soon see that they are equivalent.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; Let $A$ denote a subset of a topological space $X$. The &lt;strong&gt;interior&lt;/strong&gt; of $A$ is the union of all open subsets of $A$. We write $\mathring A$ to denote the interior of $A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Clearly the interior of a set is always open because it is the union of open sets. We can think of the interior of a set as the largest open set contained in that set. Clearly every point in $\mathring A$ has a neighborhood contained in $A$. We call each such point an &lt;strong&gt;interior point&lt;/strong&gt; of $A$.&lt;/p&gt;
&lt;p&gt;Now that we have defined the interior and closure of any set $A$, we have a sort of set sandwich $\mathring A\subseteq A\subseteq\overline A$. Here, $\mathring A$ contains none of its boundary points, $\overline A$ contains them all, and $A$ can contain some, all or none.&lt;/p&gt;
&lt;p&gt;Let's finish up with the proof I promised you a minute ago.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; If $A$ is a subset of a topological space $X$, then $\mathring A=\overline A-\partial A$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Choose $x\in\mathring A$. Clearly $x\in\overline A$ because $\mathring A\subseteq\overline A$. Since $x$ is an interior point of $A$, there exists some neighborhood of $x$ which is contained in $A$. Thus $x$ is not a boundary point of $A$ and so $x\in\overline A-\partial A$.&lt;/p&gt;
&lt;p&gt;Next, choose $x\in\overline A-\partial A$. Then $x\in A$ or $x$ is a limit point of $A$, but $x$ is not a boundary point of $A$. Thus any neighborhood of $x$ contains a point in $A$ but no points in $X-A$, so $x\in\mathring A$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I'm probably boring you. I'm boring myself, actually, because I don't find any of this particularly interest. It's kind of necessary though, so I'm glad I went through it. I'm going to end the post here, and next time I'll finally talk about continuity, which is actually interesting and incredibly important.&lt;/p&gt;
</content:encoded></item><item><title>Groups and their Basic Properties</title><description>Essentially, a group is a set endowed with a very basic structure. This structure is enforced by an operation which governs how the elements in the group interact with each other.</description><link>http://localhost:2368/groups-and-their-basic-properties/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae21b</guid><dc:creator>Eric Shapiro</dc:creator><pubDate>Fri, 07 Apr 2017 23:05:25 GMT</pubDate><content:encoded>&lt;h3 id="contents"&gt;Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-definition"&gt;The Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples"&gt;Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#basic-properties"&gt;Basic Properties&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#subgroups"&gt;Subgroups&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id="introductionanameintroduction"&gt;Introduction&lt;a name="introduction"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I think it's probably time I wrote a post about something other than topology, so today I'm going to break way into the field of algebra.&lt;sup class="footnote-ref"&gt;&lt;a href="#fn1" id="fnref1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; It's difficult to explain exactly what algebra is until you've been exposed to some of it, but I like to think of algebra as the study of structure. Using algebraic techniques, seemingly different objects can be shown to be not so different after all, and many difficult problems can be solved relatively easy using such observations.&lt;/p&gt;
&lt;p&gt;The first algebraic object I'd like to introduce, as you may have guessed, is a group. This will not be my only post on groups, since there is a lot to say about them. Essentially, a group is a set endowed with a very basic structure. This structure is enforced by an operation which governs how the elements in the group interact with each other. Before I define a group, I need to talk a little bit about binary operations.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;binary operation&lt;/strong&gt; $\circ$ on a set $X$ is a function $\circ:X\times X\to X$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The definition nicely captures several important ideas. First, it guarantees that if $a,b\in X$ then $\circ(a,b)\in X$ as well. This is called &lt;strong&gt;closure&lt;/strong&gt; under the binary operation — when we combine two elements in $X$, we always get back something which is also in $X$. Next, because we have defined $\circ$ as a function, it is guaranteed that every pair of elements in $X$ can be combined to yield a new element.&lt;/p&gt;
&lt;p&gt;You're probably more used to &lt;strong&gt;infix notation&lt;/strong&gt; $a\circ b$, rather than the equivalent function notation $\circ(a,b)$, when it comes to denoting binary operations. That's good, since infix notation is more convenient and we will use it more often.&lt;/p&gt;
&lt;p&gt;As an example, the usual operations of addition and multiplication on $\mathbb{N},\mathbb{Z},\mathbb{Q},\mathbb{R}$ and $\mathbb{C}$ are binary operations on each of these sets. Let $X$ denote any of the aforementioned sets. That addition and multiplication on $X$ actually constitute binary operations is shown easily by noting that for any two elements $a,b\in X$, their sum $a+b$ and their product $a\cdot b$ are well defined and are also in $X$.&lt;/p&gt;
&lt;h3 id="thedefinitionanamethedefinition"&gt;The Definition&lt;a name="the-definition"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I'll jump straight into it, shall I?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;group&lt;/strong&gt; is a set $G$ together with a binary operation $\circ:G\times G\to G$ with the following properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Associativity.&lt;/strong&gt; For any $a,b,c\in G$, we have that $(a\circ b)\circ c=a\circ (b\circ c)$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Identity.&lt;/strong&gt; There exists $e\in G$ such that $e\circ x=x\circ e=x$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inverses.&lt;/strong&gt; For every $x\in G$ there exists $y\in G$ such that $x\circ y=e$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Stated in plain English, groups must have an identity element and inverses for every element, and the group operation must be associative. These all seem like reasonable requirements.&lt;/p&gt;
&lt;p&gt;In the future, we will occasionally be even more lax and denote $a\circ b$ as simply $ab$, when a group's binary operation is implicitly understood and no confusion can arise. When an element is 'multiplied' by itself numerous times, we will use exponential notation. For instance, $aa=a^2$ and $aaa=a^3$. It is consistent and convenient in this notation to denote the identity element $e=a^0$ for any element $a$ in a group.&lt;/p&gt;
&lt;p&gt;Furthermore, I will frequently refer to $G$ itself as a group, as this very rarely results in any confusion. Just remember that whenever I mention a group, there is always some binary operation lurking behind the curtain.&lt;/p&gt;
&lt;h3 id="examplesanameexamples"&gt;Examples&lt;a name="examples"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Now that groups have been defined, I'd like to walk you through a few simple examples of groups.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;trivial group&lt;/strong&gt; is the group containing only one element.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Technically there are infinitely many 'trivial groups' since that one element could be anything, but all trivial groups are really the same. Let's call the element $e$. To see that the trivial group is in fact a group, first note that $e$ must be the group's identity element. Thus, $(ee)e=ee=e(ee)$ and so we have associativity. Furthermore, since $ee=ee=e$, clearly $e$ is its own inverse.&lt;/p&gt;
&lt;p&gt;The set of integers under addition also forms a group. Associativity is a property of addition itself, and you probably use it every day without even thinking about it. The identity element is zero, since $x+0=0+x=x$ for any $x\in\mathbb{Z}$. Lastly, the inverse of any integer $x$ is $-x$, since $x+(-x)=0$.&lt;/p&gt;
&lt;p&gt;Similarly, the set $\mathbb{R}^+$ of positive real numbers under multiplication forms a group. Again, associativity is obvious.&lt;sup class="footnote-ref"&gt;&lt;a href="#fn2" id="fnref2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; The identity element is $1$ since $1\cdot x=x\cdot 1=x$ for any $x\in\mathbb{R}^+$. Lastly, the inverse of any $x\in\mathbb{R}^+$ is $\frac{1}{x}$. Notice that the set of all real numbers does not form a group under multiplication because zero has no multiplicative inverse!&lt;/p&gt;
&lt;p&gt;The last example I'd like to talk about is considerably more abstract, and probably not something you would ever have considered might be a group. It is called the &lt;strong&gt;dihedral group&lt;/strong&gt; (or &lt;strong&gt;group of symmetries&lt;/strong&gt;) of a regular polygon. We write $D_n$ to denote the dihedral group of the $n$-sided regular polygon. How is $D_n$ defined? I'm not quite ready to do it rigorously, since we haven't talked about permutations yet, but I'll explain it intuitively.&lt;/p&gt;
&lt;p&gt;Basically, $D_n$ is the set of all rotations and reflections which preserve the locations of the vertices. The group operation is composition of these rotations and reflections, which essentially amounts to performing them one after the other. For instance, consider the equilateral triangle (the $3$-sided regular polygon)&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/equilateral-triangle-1.svg" alt="equilateral triangle"&gt;&lt;/p&gt;
&lt;p&gt;where I've numbered the vertices to avoid the tremendous confusion that would otherwise ensue. Here's one of the vertex location preserving rotations we can perform, $r_1$:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/r_1.svg" alt="r_1"&gt;&lt;/p&gt;
&lt;p&gt;Here's another one, $r_2$:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/r_2-1.svg" alt="r_2"&gt;&lt;/p&gt;
&lt;p&gt;It's not too difficult to see that $r_2$ is really just $r_1$ applied twice! That is, $r_2=r_1\circ r_1$. Remember that this is the notation of function composition, so the first rotation applied is written on the right.&lt;/p&gt;
&lt;p&gt;Now that we have a better grasp of what's going on, let's think about the identity element of $D_3$. It should hopefully make sense that the identity element is the rotation by zero degrees. That is, the identity element is the act of doing nothing to the triangle. Call this element $e$. It should be clear that $e\circ r_1=r_1\circ e=r_1$ and likewise for $r_2$, so things are looking good so far.&lt;/p&gt;
&lt;p&gt;What are the inverses of $r_1$ and $r_2$? We just need to figure out how to get the vertices back to their original positions. It looks like $r_1\circ r_2=r_2\circ r_1=e$, so $r_1$ and $r_2$ are actually inverses!&lt;/p&gt;
&lt;p&gt;We've exhausted all the rotations, so now we need to look at the reflections. There are three of them, and each preserves the location of one vertex while swapping the other two. Let's call them $f_1, f_2$ and $f_3$, where the subscript denotes the vertex preserved under each. For instance, here's $f_1$:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/04/f_1.svg" alt="f_1"&gt;&lt;/p&gt;
&lt;p&gt;What is the inverse of each reflection? If you reflect something twice along the same axis, it goes back to its original position. That is, $f_1, f_2$ and $f_3$ are each their own inverse.&lt;/p&gt;
&lt;p&gt;It's important to remember that the elements of the group are these reflections and rotations, rather than the triangles they act upon. I'll make all this more precise later when I talk about permutations, but for now I think this visual explanation should suffice.&lt;/p&gt;
&lt;p&gt;Next, $D_4$ is the dihedral group of the square, with four distinct rotations (where I have counted the identity among the rotations) and four reflections.&lt;/p&gt;
&lt;p&gt;Likewise, $D_5$ is the dihedral group of the regular pentagon, with five rotations and five reflections. In general, $D_n$ consists of $n$ rotations and $n$ reflections, totaling $2n$ elements.&lt;/p&gt;
&lt;p&gt;I'll talk about dihedral groups again in the future, but for now let me conclude by noting that the rotations in $D_n$ form a group of their own, since they cannot be composed in such a way as to produce anything other than a rotation. The reflections are a different story, though. They do not form a group since, for example, in $D_3$ we have that $f_2\circ f_1=r_2$. That is, the composition of two reflections can be a rotation.&lt;/p&gt;
&lt;h3 id="basicpropertiesanamebasicproperties"&gt;Basic Properties&lt;a name="basic-properties"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Now that I've gotten a few examples of groups out of the way, I'd like to talk about some immediate consequences of the group axioms. Let's begin by showing that we are allowed to cancel terms from equations, as we are so used to doing in the familiar number systems.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Left Cancellation Law.&lt;/strong&gt; Let $G$ denote a group with $a,b,x\in G$. If $xa=xb$, then $a=b$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Since $x\in G$, there must exist an inverse $y\in G$ for $x$ and an identity element $e\in G$ such that $yx=e$. Thus,&lt;/p&gt;
&lt;p&gt;$$\begin{align}&lt;br&gt;
a &amp;amp;= e a &amp;amp; \scriptstyle\textit{identity}\\&lt;br&gt;
&amp;amp;=(y x) a &amp;amp; \scriptstyle\textit{inverses}\\&lt;br&gt;
&amp;amp;=y(x a) &amp;amp; \scriptstyle\textit{associativity}\\&lt;br&gt;
&amp;amp;=y(x b) &amp;amp;\scriptstyle{x a=x b}\\&lt;br&gt;
&amp;amp;=(y x) b &amp;amp;\scriptstyle\textit{associativity}\\&lt;br&gt;
&amp;amp;=e b &amp;amp;\scriptstyle\textit{inverses}\\&lt;br&gt;
&amp;amp;=b. &amp;amp;\scriptstyle{\textit{identity}}&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;This completes the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I have included my reasoning for each step to the right, because such proofs can be difficult to parse when you first encounter them. Do not expect me to continue being this nice in the future. The &lt;strong&gt;right cancellation law&lt;/strong&gt; and its proof are completely symmetric, so I will not even bother to state them. Now that these cancellation laws have been established, we will be using them frequently. For instance, let's use one to prove the following proposition about inverses.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; Each element in a group has a unique inverse.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Let $G$ denote a group with $x\in G$ and suppose that $y_1, y_2\in G$ are both inverses for $x$. Then $y_1x=y_2x=e$ by the definition of inverses, so by the right cancellation law it follows that $y_1=y_2$, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How does this proof establish that each element's inverse is unique? We are guaranteed the existence of at least one inverse by the group axioms. Furthermore, we just demonstrated that if an element has two inverses, then they must really be the same element!&lt;/p&gt;
&lt;p&gt;Next, let's prove a similar statement that there is only one identity element in any group. This is probably the simplest proof ever, but it's rather informative.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; The identity element in a group is unique.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Let $G$ denote a group and suppose that $e_1, e_2\in G$ are both identity elements. Then $e_1=e_1e_2=e_2$ by the definition of the identity, completing the proof.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I've already been saying 'the inverse' of an element and 'the identity' a lot prior to this, but now I'm actually justified in doing so. Furthermore, since each element $x$ only has one inverse, we can denote it unambiguously as $x^{-1}$. This plays well with the exponential notation I introduced earlier, i.e. $(x^{-1})^n=x^{-n}$ and $x^mx^{-m}=e$.&lt;/p&gt;
&lt;p&gt;Before moving on I'd like to mention a very nice property that certain groups may exhibit, but they do not necessarily have to.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A group $G$ is &lt;strong&gt;abelian&lt;/strong&gt; (or &lt;strong&gt;commutative&lt;/strong&gt;) if $xy=yx$ for every $x,y\in G$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Examples of abelian groups that we've already seen are the integers under addition and the positive real numbers under multiplication. On the other hand, the dihedral groups of order three and above are nonabelian.&lt;/p&gt;
&lt;h3 id="subgroupsanamesubgroups"&gt;Subgroups&lt;a name="subgroups"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I'm almost done now, and I know this has been a pretty long post. I just need to introduce one more important concept and then I promise I'll stop.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;subgroup&lt;/strong&gt; $H$ of a group $G$ is a subset of $G$ which is itself a group under the group operation on $G$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If $H$ is a subgroup of $G$, I will sometimes write $H\leq G$. This cannot really be confused with the &amp;quot;less than or equal to&amp;quot; relation because groups are not numbers and thus have no such concept.&lt;/p&gt;
&lt;p&gt;As an example, the even integers (denoted $2\mathbb{Z}$) are a subgroup of $\mathbb{Z}$ under addition. This is because the sum of two even integers is always even. However, the odd integers do not form a subgroup of $\mathbb{Z}$ under addition because the sum of two odd integers is not odd.&lt;sup class="footnote-ref"&gt;&lt;a href="#fn3" id="fnref3"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;I pointed out earlier that the rotations in $D_3$ themselves formed a group under function composition, and this means that they are a subgroup of $D_3$.&lt;/p&gt;
&lt;p&gt;In general, a group does not necessarily have any subgroups other than itself and the trivial group. We shall see later that if a subgroup (of a finite group) is to exist, it must contain a very predictable number of elements.&lt;/p&gt;
&lt;hr class="footnotes-sep"&gt;
&lt;section class="footnotes"&gt;
&lt;ol class="footnotes-list"&gt;
&lt;li id="fn1" class="footnote-item"&gt;&lt;p&gt;Although my true motivation is perhaps more sinister than you could possibly imagine. 😈 &lt;a href="#fnref1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn2" class="footnote-item"&gt;&lt;p&gt;It's really not. When I inevitably define the real numbers from scratch as equivalence classes of Cauchy sequences of rational numbers in a future post, showing associativity will actually be something of a chore. As will everything else. &lt;a href="#fnref2" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn3" class="footnote-item"&gt;&lt;p&gt;The odd integers form what is called a coset, $2\mathbb{Z}+1$ of the subgroup $2\mathbb{Z}$, but I will talk about this in a later post. &lt;a href="#fnref3" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content:encoded></item></channel></rss>