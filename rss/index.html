<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Algebrology]]></title><description><![CDATA[A gentle introduction to insanity.]]></description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>Algebrology</title><link>http://localhost:2368/</link></image><generator>Ghost 2.14</generator><lastBuildDate>Wed, 20 Feb 2019 06:25:27 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Sequences, Hausdorff Spaces and Nets]]></title><description><![CDATA[I'm now going to talk about sequences and nets, which often provide an alternative way of describing topological phenomena. I'll also talk about Hausdorff spaces, which have all sorts of nice properties.]]></description><link>http://localhost:2368/sequences-hausdorff-spaces-and-nets/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae222</guid><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Thu, 20 Apr 2017 21:11:25 GMT</pubDate><content:encoded><![CDATA[<h3 id="contents">Contents</h3>
<ol>
<li><a href="#sequences">Sequences</a></li>
<li><a href="#hausdorff-spaces">Hausdorff Spaces</a></li>
<li><a href="#nets">Nets</a></li>
</ol>
<p>I'm now going to talk about sequences and nets, which often provide an alternative way of describing topological phenomena. I'll also talk about Hausdorff spaces, which have all sorts of nice properties. I was originally planning to include filters in this discussion as well, but I think if I did that this post might become long enough to break the internet.</p>
<h3 id="sequencesanamesequences">Sequences<a name="sequences"></a></h3>
<p>If you've taken a calculus class (or maybe even if you haven't) then you probably already have some notion of what sequences are. They're basically just lists of elements that go on forever. For instance,</p>
<p>$$\begin{gather}<br>
\begin{aligned}<br>
(0,1,2,3,4,5,6,7,\dotsc)\\<br>
(1,1,2,3,5,8,13,\dotsc)\\<br>
(\text{cat},\text{cat},\text{cat},\text{cat},\dotsc)<br>
\end{aligned}<br>
\end{gather}$$</p>
<p>are all sequences. The first two have entries in $\mathbb{N}$ and the third takes values in some set of animals.</p>
<p>Notice that there is always one entry for each natural number. That is, there is a zeroth entry, a first entry, a second entry, and so on. The order in which these entries appear does matter, so put them in parentheses rather than set brackets to distinguish them from sets. Sequences have two main differences from countable infinite sets: they are ordered, and the same point can appear more than once. This important point leads us to the following rigorous definition of a sequence:</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>sequence</strong> in a topological space $X$ is a function $x:\mathbb{N}\to X$.</p>
</blockquote>
<p>It is perhaps a bit confusing to actually think of sequences as functions. The definition above is simply meant to give the &quot;ordered list of points&quot; idea some rigorous footing. We generally write $x_n$, rather than $x(n)$, to denote the $n$th term in a sequence. This means we can write a sequence as $(x_0,x_1,x_2,\dotsc)$. This is sometimes shortened to either $(x_n)_{n=0}^\infty$ or $(x_n)_{n\in\mathbb{N}}$.</p>
<p>Next, let's talk about convergence. This can be a tricky business, and it is the bane of many Calculus II students' existence. The concept of convergence is not itself terribly complicated â€” it is the process of figuring out whether a specific sequence converges which can sometimes be unreasonably challenging. To start, let's look at convergence in metric spaces so that we can make use of the familiar notion of distance.</p>
<blockquote>
<p><strong>Definition.</strong> A sequence $(x_n)_{n\in\mathbb{N}}$ in a metric space $X$ converges to a point $x\in X$ if for every real number $\epsilon&gt;0$ there is some natural number $N$ for which $d(x,x_n)&lt; \epsilon$ whenever $n&gt;N$.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> If a sequence $(x_n)_{n\in\mathbb{N}}$ converges to a point $x$, we say that $x$ is the <strong>limit</strong> of that sequence and we write $\lim\limits_{n\to\infty}x_n=x$.</p>
</blockquote>
<p>That's a bit of a mouthful, so let's spend a little bit of time making sure we know what we're getting ourselves into. Essentially what I mean when I say that a sequences converges to a point $x$ is that eventually everything in the sequence becomes as close to $x$ as I want. More precisely, given $\epsilon &gt; 0$, I want everything beyond the $N$th entry in the sequence to be within the open ball $B(x,\epsilon)$, where I get to choose $N$. If I can find such an $N$ for every $\epsilon$, then the sequence converges. Generally, $N$ will need to be very large when $\epsilon$ is very close to zero.</p>
<blockquote>
<p><strong>Example.</strong> Consider the sequence $(x_n)_{n=1}^\infty$ in $\mathbb{R}$ where each $x_n=\frac{1}{n}$. We can visualize this sequence in the following manner:</p>
<p><img src="http://localhost:2368/content/images/2017/04/1-over-n.svg" alt=""></p>
<p>Notice that the points in the sequence all lie on the graph of the function $f:\mathbb{R}^+\to\mathbb{R}$ defined by $f(x)=\frac{1}{x}$. This is not surprising, considering we originally defined sequences as functions themselves. That is, this sequence is really the restriction of $f$ to the positive integers, $f\negmedspace\mid_{\mathbb{Z}^+}:\mathbb{Z}^+\to\mathbb{R}$. If you have any experience with this function, you'll believe me when I say that it becomes extremely close to zero and always grows closer to it. It makes sense then that our sequence does the same, so we might guess that it converges to zero. Let's prove this!</p>
</blockquote>
<blockquote>
<p><strong>Theorem.</strong> The sequence $(x_n)_{n=1}^\infty$ given by $x_n=\frac{1}{n}$ converges to $0$.</p>
<p><strong>Proof.</strong> Choose $\epsilon&gt;0$ and let $N&gt;\frac{1}{\epsilon}$. If $n&gt;N$, then certainly $n&gt;\frac{1}{\epsilon}$. Thus,</p>
<p>$$\begin{aligned}<br>
d(x_n, 0) &amp;= \vert x_n - 0\vert \\<br>
&amp;= \tfrac{1}{n} \\<br>
&amp;&lt; \epsilon.<br>
\end{aligned}$$</p>
</blockquote>
<p>You don't really need to remember the proof of this fact, although it's incredibly easy to reproduce â€” the candidate for $N$ in this case is more obvious than usual. Just remember that $\lim\limits_{n\to\infty}=0$, which should hopefully make a lot of sense to you anyway. This is an important sequence which we will occasional use in the future.</p>
<p>Also, notice that the sequence we just looked at doesn't actually <em>quite</em> fit the definition I gave for sequences. That is, it doesn't have an entry for every natural number (in particular, there is no $x_0$). We could easily remedy that by rewriting each term as $\frac{1}{n+1}$ and shifting each entry's index down by one. I chose to write it the way I did because it looks a bit nicer. It is somewhat common to allow sequences to start at any index we like, as we can always translate it into starting at zero using a similar substitution.</p>
<p>Now, in a calculus or analysis class you would study lots of properties and characteristics of sequences in $\mathbb{R}$ and learn a bunch of tricks to help you show that certain types of sequences in $\mathbb{R}$ converge. However, all of that stuff bores me and I want to talk generally about convergent sequences in topological spaces, not just about $\mathbb{R}$ with the standard topology. This will require a slight reworking of the definition of convergence to eliminate the concept of distance that we have in metric spaces.</p>
<blockquote>
<p><strong>Definition.</strong> A sequence $(x_n)_{n\in\mathbb{N}}$ in a topological space $X$ converges to a point $x\in X$ if for every neighborhood $U$ of $x$, there is a natural number $N$ for which $x_n\in U$ whenever $n&gt;N$.</p>
</blockquote>
<p>This definition basically replaces open balls with neighborhoods, and shouldn't require too much explanation other than that. It should be clear that this definition, when $X$ is a metric space, is equivalent to the old one because open sets are just unions of open balls.</p>
<blockquote>
<p><strong>Definition.</strong> If a sequence $(x_n)_{n\in\mathbb{N}}$ in a topological space converges to a point $x$, we say that $x$ is a <strong>limit</strong> of that sequence and we write $\lim\limits_{n\to\infty}x_n=x$.</p>
</blockquote>
<p>Notice that I've said &quot;a limit,&quot; rather than &quot;the limit&quot; like I did for metric spaces. That's because a convergent sequence in a topological space might actually converge to multiple points. <center><h1>ðŸ˜±</h1></center> The simplest example of this phenomenon that I can think of is as follows:</p>
<blockquote>
<p><strong>Example.</strong> Let $X$ be any nonempty set equipped with the trivial topology.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> Then for any point $x\in X$, the only neighborhood of $x$ is $X$ itself. Certainly for any sequence $(x_n)_{n\in\mathbb{N}}$ in $X$, all terms of the sequence are in $X$. If follows that every sequence in $X$ converges to every point of $X$.</p>
</blockquote>
<p>This might strike you as a bit odd, and I'd agree with you. At the very least, this business of every sequence converging to every point is not very desirable behavior for a topological space. After all, we'd like limits of sequences to be unique. Luckily for us, there is a specific type of space for which this behavior is guaranteed!</p>
<h3 id="hausdorffspacesanamehausdorffspaces">Hausdorff Spaces<a name="hausdorff-spaces"></a></h3>
<blockquote>
<p><strong>Definition.</strong> A topological space $X$ is <strong>Hausdorff</strong><sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> if for every pair of points $x,y\in X$ with $x\ne y$, there exists a neighborhood $U$ of $x$ and a neighborhood $V$ of $y$ such that $U\cap V=\varnothing$.</p>
</blockquote>
<p>So in a Hausdorff space, distinct points have disjoint neighborhoods. This is clearly not true for spaces with two or more points under the trivial topology, so we're off to a good start. Before I show how this property guarantees uniqueness of limits, I will prove that every metric space is Hausdorff.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ denote a metric space with metric $d:X\times X\to\mathbb{R}$. Then $X$ is Hausdorff when equipped with the topology induced by the metric $d$.</p>
<p><strong>Proof.</strong> Choose $x,y\in X$ with $x\ne y$. By the definition of a metric, $d(x,y)&gt;0$. Let $r=\frac{d(x,y)}{2}$ and define $U=B(x,r)$ and $V=B(y,r)$. It suffices to show that $U$ and $V$ are disjoint, which we will argue by contradiction.</p>
<p>Suppose $U\cap V\ne\varnothing$. Then there exists some point $p\in U\cap V$, so $d(x,p)&lt; r$ and $d(y,p)&lt; r$ by the definitions of these open balls. Thus,</p>
<p>$$\begin{aligned}<br>
d(x,p)+d(y,p) &amp;&lt; 2r \\<br>
&amp;= d(x,y),<br>
\end{aligned}$$</p>
<p>which violates the triangle inequality. We have reached a contradiction, so the proof is complete.</p>
</blockquote>
<p>This tells us right away that things like $\mathbb{R}$ in the standard topology are Hausdorff. Now if we can just show that convergent sequences in Hausdorff spaces have unique limits, then I will definitely have been justified earlier in claiming that metric spaces have unique limits. Let's prove this right now.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ be a nonempty Hausdorff space and let $(x_n)_{n\in\mathbb{N}}$ be a convergent sequences in $X$. Then $(x_n)_{n\in\mathbb{N}}$ has exactly one limit.</p>
<p><strong>Proof.</strong> Since $(x_n)_{n\in\mathbb{N}}$ is convergent, we know that it has at least one limit. Thus, it suffices to show that it also has at most one limit. We proceed by contradiction.</p>
<p>Suppose $(x_n)_{n\in\mathbb{N}}$ converges to both $p_1$ and $p_2$, where $p_1\ne p_2$. Since $X$ is Hausdorff, there exist disjoint neighborhoods $U_1$ of $p_1$ and $U_2$ of $p_2$. From the definition of convergence, we have that $x_n\in U_1$ whenever $n&gt;N_1$ and $x_n\in U_2$ whenever $n&gt;N_2$ for some natural numbers $N_1$ and $N_2$ Let $N=\max\{N_1,N_2\}$. Then clearly $x_n\in U_1\cap U_2$ whenever $n&gt;N$. This is a contradiction, since $U_1$ and $U_2$ are disjoint.</p>
</blockquote>
<p>So Hausdorff spaces are desirable in that if a sequence converges, it does so as we'd generally expect it to. I won't go into this in too much detail right now, but all of the thinks we actually think of as &quot;space&quot; are Hausdorff. In fact, the definition of a manifold explicitly requires this property, which we shall see if I ever manage to get that far.</p>
<p>There are a few more properties of Hausdorff spaces which I'd like to prove before moving on, just because they're interesting. The first is the fact that singleton sets in Hausdorff spaces are closed. Its proof is quite straightforward.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ be a nonempty Hausdorff space. Then for every point $x\in X$, the set $\{x\}$ is closed.</p>
<p><strong>Proof.</strong> Since $X$ is Hausdorff, for every $y\in X$ with $y\ne x$ there exist disjoint neighborhoods $U_y$ of $x$ and $V_y$ of $y$. It follows from the union lemma that</p>
<p>$$\bigcup\limits_{y\ne x}V_y = X-\{x\},$$</p>
<p>and this set is open because it is the union of open sets. Thus, $\{x\}$ is closed because its complement is open.</p>
</blockquote>
<p>The next property is a little bit more interesting</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ and $Y$ denote topological spaces and suppose $Y$ is Hausdorff. Then the graph of any continuous function $f:X\to Y$, given by</p>
<p>$$G=\left\{\big(x,f(x)\big)\mid x\in X\right\}$$</p>
<p>is closed in the product space $X\times Y$.</p>
<p><strong>Proof.</strong> It suffices to show that $(X\times Y)-G$ is open in $X\times Y$. Choose $(x,y)\in (X\times Y)-G$. Clearly $y\ne f(x)$, so because $Y$ is Hausdorff there exist disjoint neighborhoods $U$ of $y$ and $V$ of $f(x)$. Furthermore, because $f$ is continuous we have that $f^{-1}[V]$ is open in $X$. Notice that $x\in f^{-1}[V]$ by definition.</p>
<p>Next, choose any point $\big(g,f(g)\big)\in G$, and let us consider separately the cases where $g\in f^{-1}[V]$ and $g\notin f^{-1}[V]$. If $g\in f^{-1}[V]$ then by definition $f(g)\in V$. Thus, $f(g)\notin U$ because $U$ and $V$ are disjoint. It follows that $\big(g,f(g)\big)\notin f^{-1}[V]\times U$. If, on the other hand, $g\notin f^{-1}[V]$ then it follows immediately that $\big(g,f(g)\big)\notin f^{-1}[V]\times U$ from the definition of the Cartesian product.</p>
<p>Either way, $\big(g,f(g)\big)\notin f^{-1}[V]\times U$ and so we have that $(f^{-1}[V]\times U)\cap G=\varnothing$. Clearly $f^{-1}\times U$ is open as it is the product of open sets. Thus every point $(x,y)\in (X\times Y)-G$ is contained in the open set $f^{-1}[V]\times U$, which is itself contained in $(X\times Y)-G$. It follows that $(X\times Y)-G$ is open in $X\times Y$, so $G$ is closed.</p>
</blockquote>
<p>This is a pretty nice result, although it isn't too useful to us right now. At the very least, it tells us that continuous real-valued functions have closed graphs because $\mathbb{R}$ is Hausdorff. The next two theorems should immediately seem useful to you.</p>
<blockquote>
<p><strong>Theorem.</strong> Any subspace of a Hausdorff space is Hausdorff.</p>
<p><strong>Proof.</strong> Let $A$ be a subspace of a Hausdorff space $X$ and choose points $x,y\in A$. Then there exist disjoint neighborhoods in $X$, $U$ of $x$ and $V$ of $y$. It follows that $A\cap U$ is a neighborhood of $x$ in $A$ and $A\cap V$ is a neighborhood of $y$ in $A$. Furthermore,</p>
<p>$$\begin{aligned}<br>
(A\cap U)\cap (A\cap V) &amp;= A\cap (U\cap V) \\<br>
&amp;= A\cap\varnothing \\<br>
&amp;= \varnothing,<br>
\end{aligned}$$</p>
<p>so $A$ is Hausdorff.</p>
</blockquote>
<blockquote>
<p><strong>Theorem.</strong> The product of two Hausdorff spaces is Hausdorff.</p>
<p><strong>Proof.</strong> Let $X$ and $Y$ denote Hausdorff spaces and choose distinct points $(x_1,y_1)$ and $(x_2,y_2)$ in $X\times Y$. Without loss of generality (the other case is so similar) suppose $x_1\ne x_2$. Then because $X$ is Hausdorff, there exist disjoint neighborhoods $U_1$ of $x_1$ and $U_2$ of $x_2$ in $X$. Note that $U_1\times Y$ and $U_2\times Y$ are both open in $X\times Y$, and that $(x_1,y_1)\in U_1\times Y$ while $(x_2,y_2)\in U_2\times Y$. Furthermore,</p>
<p>$$\begin{aligned}<br>
(U_1\times Y)\cap (U_2\times Y) &amp;= (U_1\cap U_2)\times Y \\<br>
&amp;= \varnothing\times Y \\<br>
&amp;= \varnothing,<br>
\end{aligned}$$</p>
<p>so $X\times Y$ is Hausdorff.</p>
</blockquote>
<p>It can be shown by induction that the product of any finite number of Hausdorff spaces is Hausdorff. It is also possible to show, in fact, that the product of <em>any</em> collection of Hausdorff spaces is Hausdorff, but I try to avoid talking about infinite Cartesian products unless I have no other choice.</p>
<p>Given that products and subspaces of Hausdorff spaces inherit Hausdorffness from their parents, you might be tempted to guess that quotients of Hausdorff spaces are Hausdorff. This is wrong in general, although I won't provide a counterexample because this post is already very long and I haven't even started discussing nets yet.</p>
<p>Unfortunately, before I get to nets I have a few more things about sequences that I would like to talk about. In particular, it would be a shame for me not to prove the following beautiful theorem for you.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ and $Y$ denote topological spaces and let $(x_n)_{n\in\mathbb{N}}$ be a sequence which converges to the point $x\in X$. Then for any continuous function $f:X\to Y$, the sequence $\big(f(x_n)\big)_{n\in\mathbb{N}}$ converges to the point $f(x)\in Y$.</p>
<p><strong>Proof.</strong> Choose any neighborhood $U\subseteq Y$ of $f(x)$. Since $f$ is continuous, $f^{-1}[U]\subseteq X$ is open and clearly $x\in f^{-1}[U]$, so $f^{-1}[U]$ is a neighborhood of $x$. Since $(x_n)_{n\in\mathbb{N}}$ converges to $x$, there exists $N\in\mathbb{N}$ for which $x_n\in f^{-1}[U]$ whenever $n&gt;N$. It follows that $f(x_n)\in U$ whenever $n&gt;N$. Thus, $\big(f(x_n)\big)_{n\in\mathbb{N}}$ converges to $f(x)$.</p>
</blockquote>
<p>This theorem is great because it tells us that continuous functions preserve convergent sequences! It would be even better if the converse was true, because that would give us yet another alternative characterization of continuous functions. Unfortunately, this is not the case without additionally assuming that both spaces are first-countable (a property that I haven't mentioned yet, but that every metric space has). For general spaces, it is also possible for function which aren't continuous to preserve convergent sequences.</p>
<p>This hints that sequences might not be exactly the right tool to study continuity. The problem is that they are too specific a concept. Let's next look at a generalization of sequences that will solve all of our problems.</p>
<h3 id="netsanamenets">Nets<a name="nets"></a></h3>
<p>Before I start trying to explain nets to you, let me state the main theorem we eventually want to prove about them.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ and $Y$ denote topological spaces. A function $f:X\to Y$ is continuous if and only if for every net $(x_a)_{a\in A}$ that converges to $x$, the net $\big(f(x_a)\big)_{a\in A}$ converges to $f(x)$.</p>
</blockquote>
<p>In stating this theorem of things to come, I've already given away a fair amount of information about the nature of nets. Namely, the fact that nets look almost exactly like sequences, except perhaps that their entries are indexed over sets other than $\mathbb{N}$. However, nets aren't indexed over just any kind of set â€” after all, we would still like the entries of a net to progress in some order. Thus, we will define them over sets with a specific type of relation:</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>preorder</strong> on a set $X$ is a reflexive and transitive relation.</p>
</blockquote>
<p>That is, a preorder on $X$ is a relation $\le$ such that $x\le x$ for every $x\in X$, and $x\le z$ whenever $x\le y$ and $y\le z$.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>directed set</strong> is a nonempty set $X$ together with a preorder $\le$ which satisfies the additional property that for any $x,y\in X$, there exists $z\in X$ such that $x\le z$ and $y\le z$.</p>
</blockquote>
<p>A shorter way of describing this final property of directed sets might be to say that every pair of elements has an upper bound. This ensures that, although some pairs of elements may not be related to each other, they are at least related to some third element. In turn, this guarantees that strange behavior, as in the following example, does not occur.</p>
<blockquote>
<p><strong>Example.</strong> Just to make sure there's no confusion, this will be an example of a set with a preorder that is <em>not</em> a directed set, because pairs of elements will not necessarily have upper bounds.</p>
<p>We will define preorders $\le_1$ on the set $\mathbb{N}\times\{1\}$ and $\le_2$ on the set $\mathbb{N}\times\{2\}$ that act similarly to the standard &quot;less than or equal to&quot; relation on $\mathbb{N}$. Recall that we previously defined $\le$ on $\mathbb{N}$ so that $n\le m$ if and only if $m=n+k$ for some $k\in\mathbb{N}$.</p>
<p>Notice that every element of $\mathbb{N}\times\{1\}$ is of the form $(n,1)$ for some $n\in\mathbb{N}$. Thus it makes sense to define $\le_1$ using the rule that $(n,1)\le_1 (m,1)$ if and only if $n\le m$. Similarly, we define $\le_2$ using the rule that $(n,2)\le_2 (m,2)$ if and only if $n\le m$.</p>
<p>It is obvious that both $\le_1$ and $\le_2$ are preorders on their respective sets because they both inherit their reflexivity and transitivity from $\le$.</p>
<p>Let's use these to define a preorder on $(\mathbb{N}\times\{1\})\cup(\mathbb{N}\times\{2\})$. We can define $\le_3$ on this union using the rule that $n\le_3 m$ if and only if either $n\le_1 m$ or $n\le_2 m$. Using the rigorous set-theoretic definition of relations, we could alternatively define this by $\le_3=\le_1\cup\le_2$. Again, it's easy to see that $\le_3$ is a preorder because it inherits its reflexivity and transitivity from $\le_1$ and $\le_2$.</p>
<p>Basically what we have is two disjoint copies of things that act identically to $\mathbb{N}$, which have been glued together, but are related to each other in absolutely no way. In particular, if we choose $n_1\in\mathbb{N}\times\{1\}$ and $n_2\in\mathbb{N}\times\{2\}$, there is certainly no element of $(\mathbb{N}\times\{1\})\cup (\mathbb{N}\times\{2\})$ which serves as an upper bound for both $n_1$ and $n_2$. Thus, this example does not constitute a directed set.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> On the other hand, the set $\mathbb{N}$ of natural numbers equipped with $\le$, the standard &quot;less than or equal to&quot; relation, is a directed set. I proved in my post on quotient sets that this relation is reflexive and transitive, so it is certainly a preorder. The fact that all pairs of natural numbers have an upper bound is easy to show. For any $x,y\in\mathbb{N}$, choose $x=\max\{x,y\}$. Then clearly $x\le z$ and $y\le z$. This is a particularly easy example because every natural number is either less than or greater than every other natural number.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> Another interesting directed set can be formed as follows. Let $X$ denote any nonempty topological space and pick a point $x\in X$. The set $N_x$ of all neighborhoods of $x$ forms a directed set when equipped with the preorder $\le$ defined by $U\le V$ is and only if $V\subseteq U$.</p>
<p>This relation is reflexive because for any neighborhood $U$ of $x$, it is clear that $U\subseteq U$ and so $U\le U$.</p>
<p>It is only a tad more difficult to see that $\le$ is transitive. Suppose we have neighborhoods $U, V$ and $W$ of $x$ for which $U\le V$ and $V\le W$. Then $W\subseteq V\subseteq U$, so certainly $W\subseteq U$. Thus, $U\le Q$.</p>
<p>Lastly, we need to show that any pair of neighborhoods of $x$ has an upper bound, which in this case simply means they both contain a common neighborhood of $x$. Again, this is easy to show. Choose any two neighborhoods $U$ and $V$ of $x$. Clearly $x\in U\cap V$, and by the definition of a topology $U\cap V$ is open. Thus it is a neighborhood of $x$. It is obvious that $U\cap V\subseteq U$ and $U\cap V\subseteq V$, so $U\le U\cap V$ and $V\le U\cap V$.</p>
</blockquote>
<p>Now that we have some examples of directed sets in our arsenal, it's finally time to define nets. You've likely already guessed how we'll proceed.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>net</strong> in a topological space $X$ is a function $x:A\to X$, where $A$ is any directed set.</p>
</blockquote>
<p>Again, we generally write $x_a$ rather than $x(a)$, and we denote a net itself by $(x_a)_{x\in A}$. Since we've already established that $\mathbb{N}$ is a directed set, it should be clear that sequences are a special type of net.</p>
<p>Convergence of nets is extremely similar to convergence of sequences.</p>
<blockquote>
<p><strong>Definition.</strong> A net $(x_a)_{x\in A}$ in a topological space $X$ <strong>converges</strong> to a point $x\in X$ if for every neighborhood $U$ of $x$, there exists $b\in A$ for which $x_a\in U$ whenever $a\ge b$.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> If a net $(x_a)_{x\in A}$ in a topological space converges to a point $x$, we say that $x$ is a <strong>limit</strong> of that net and we write $\lim x_a=x$.</p>
</blockquote>
<p>It's fairly easy to come up with a convergent net that is not a sequence, using an example I've already given.</p>
<blockquote>
<p><strong>Example.</strong> Given a topological space $X$ and a point $x\in X$, let $N_x$ denote the directed set of neighborhoods of $x$ as detailed above. We can construct a net $(x_U)_{U\in N_x}$ by choosing a point $x_U\in U$ for each neighborhood $U$ of $x$. (Notice that this action requires the Axiom of Choice). Intuition tells us that this net should converge to $x$ because the neighborhoods of $x$ get &quot;smaller&quot; the further out we go in our directed set $N_x$. This claim is super easy to verify, so let's just do it.</p>
<p>Choose any neighborhood $U$ of $x$. From our construction of the net $(x_U)_{U\in N_x}$, it is clear that $x_U\in U$. Furthermore, for any neighborhood $V$ of $x$ with $V\ge U$, we have that $V\subseteq U$ and thus $x_V\in X\subseteq U$. It follows that $(x_U)_{U\in N_x}$ converges to $x$.</p>
</blockquote>
<p>This post is already so ridiculously long that I'm just going to prove the theorem that I promised you and then be done. Unfortunately, the proof is a little bit on the longer side.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ and $Y$ denote topological spaces. Then a function $f:X\to Y$ is continuous if and only if for every net $(x_a)_{a\in A}$ that converges to $x$, the net $\big(f(x_a)\big)_{a\in A}$ converges to $f(x)$.</p>
<p><strong>Proof.</strong> The forward direction is practically identical for the analogous result for series. Suppose $f$ is continuous and that the net $(x_a)_{a\in A}$ converges to the point $x\in X$. Choose any neighborhood $U$ of $f(x)$. Since $f$ is continuous, $f^{-1}[U]\subseteq X$ is open and clearly $x\in f^{-1}[U]$, so $f^{-1}[U]$ is a neighborhood of $x$. Thus, there exists $b\in A$ for which $x_a\in f^{-1}[U]$ whenever $a\ge b$. It follows that $f(x_a)\in U$ whenever $a\ge b$, so the net $\big(f(x_a)\big)_{a\in A}$ converges to $f(x)$.</p>
<p>I will prove the reverse direction by contradiction. Suppose that for every net $(p_a)_{a\in A}$ that converges to $p$, the net $\big(f(p_a)\big)_{a\in A}$ converges to $f(p)$, but that $f$ is not continuous. Then there exists a point $x\in X$ and a neighborhood $V$ of $f(x)$ for which $f^{-1}[V]$ is not a neighborhood of $x$. Thus, we can construct a net $(x_U)_{U\in N_x}$ for which each $x_U\notin f^{-1}[V]$. Clearly each $f(x_U)\notin V$. Choose any neighborhood $W$ of $x$. Then for any neighborhood $T\ge W$, i.e., $T\subseteq W$, and so $x_T\in W$. It follows that $(x_U)_{U\in N_x}$ converges to $x$, and thus $\big(f(x_U)\big)_{U\in N_x}$ converges to $f(x)$. However, the interior of $V$ is a neighborhood of $f(x)$ and thus $f(x_U)$ is eventually in this interior and therefore also in $V$, but this is a contradiction.</p>
</blockquote>
<p>So continuity is equivalent to the preservation of convergent nets, which is pretty cool. It's also true that being Hausdorff is equivalent to the existence of unique limits for nets, but I'm going to end this post here because it's really just getting ridiculous at this point.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Recall that in the trivial topology the only open sets are $\varnothing$ and $X$. <a href="#fnref1" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Or <strong>separated</strong>, or $\mathbf{T}_2$. <a href="#fnref2" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
</ol>
</section>
]]></content:encoded></item><item><title><![CDATA[Quotient Spaces]]></title><description><![CDATA[The notion of a quotient space will effectively allow us to glue pieces of topological spaces together. This corresponds to the collapsing of equivalent subsets to points which occurs in quotient sets, as I mentioned in my last post.]]></description><link>http://localhost:2368/quotient-spaces/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae221</guid><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Sun, 09 Apr 2017 14:16:40 GMT</pubDate><content:encoded><![CDATA[<p>Now that we've defined quotient sets, let's talk about quotient sets of topological spaces. The notion of a quotient space will effectively allow us to glue pieces of topological spaces together. This corresponds to the collapsing of equivalent subsets to points which occurs in quotient sets, as I mentioned in my last post. These are useful tools, so I'm going to jump right into it. It may be a bit difficult to see where I'm going with this at first, but bear with me and hopefully it'll become clear.</p>
<blockquote>
<p><strong>Definition.</strong> Let $X$ denote a topological space, let $A$ be a set and let $f:X\to A$ be a surjective function. The <strong>quotient topology</strong> induced by $f$ has as its open sets all sets $U$ such that $f^{-1}[U]$ is open in $X$. We call $f$ a <strong>quotient map</strong> and $A$ a <strong>quotient space</strong> when equipped with this topology.</p>
</blockquote>
<p>Supposing the quotient topology is truly a topology, we get for free that quotient maps are always continuous, simple from the way they're defined.  However, we still need to verify that quotient topologies satisfy the requirements of a topology. We need to show that $\varnothing$ and $A$ are open, and that unions and finite intersections of open sets are open.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ denote a topological space, let $A$ be a set and let $f:X\to A$ be a surjective function. Then the quotient topology defined above is a topology on $A$.</p>
<p><strong>Proof.</strong> It is clear that $f^{-1}[\varnothing]=\varnothing$ and so the empty set is open in $A$. Furthermore, since $f$ is surjective, we know that $f^{-1}[A]=X$ is open in $X$ and so $A$ is also open in $A$.</p>
<p>Next, suppose that $I$ is an indexing set and $U_i\subseteq A$ are such that $f^{-1}[U_i]$ is open in $X$ for each $i\in I$. Then certainly</p>
<p>$$f^{-1}\left[\bigcup\limits_{i\in I}U_i\right]=\bigcup\limits_{i\in I}f^{-1}[U_i]$$</p>
<p>is open in $X$ since it is the union of open sets. Thus, $\bigcup\limits_{i\in I}U_i$ is open in A.</p>
<p>Finally, suppose that $n\in\mathbb{Z}^+$ and that $U_i\subseteq A$ are such that $f^{-1}[U_i]$ is open in $X$ for each $1\le i\le n$. Then</p>
<p>$$f^{-1}\left[\bigcap\limits_{i=1}^n U_i\right]=\bigcap\limits_{i=1}^n f^{-1}[U_i]$$</p>
<p>is open in $X$ since it is the intersection of a finite number of open sets. Thus, $\bigcap\limits_{i=1}^n U_i$ is open in $A$, completing the proof.</p>
</blockquote>
<p>There is also an equivalent, more intuitive way to define quotient spaces which will probably look more familiar to you after our discussion on quotient sets last post.</p>
<blockquote>
<p><strong>Definition.</strong> Let $X$ denote a topological space and let $\sim$ be an equivalence relation on $X$. The <strong>quotient space</strong> $X/\negthickspace\sim$ is this quotient set where the open sets are sets of equivalence classes whose unions are open in $X$.</p>
</blockquote>
<p>We've established that quotient maps induce a topology in this way, so let's take a look at what they can do for us.</p>
<p>While it's true that any set $A$ can be turned into a quotient space by defining a suitable surjection as our quotient map, we generally restrict our interest to partitions of $A$. Let's first visualize the construction of a simple quotient space, which you may have seen before. We will then figure out how to formally document this process.</p>
<blockquote>
<p><strong>Example.</strong> Consider the square $[0,1]^2$ in the standard topology. We can first create a cylinder from the square by gluing two opposite edges together.</p>
<p><img src="http://localhost:2368/content/images/2017/04/square_to_cylinder.svg" alt="square to cylinder"></p>
<p>Next, we can glue the two open circular ends of the cylinder together to form a torus.</p>
<p><img src="http://localhost:2368/content/images/2017/04/cylinder_to_torus.svg" alt="cylinder to torus"></p>
<p>What we've really done here is glue the opposite edges of the square together, one pair at a time. A much more concise diagram representing this act simply identifies opposite edges of the square with each other, and the gluing is implied.</p>
<p><img src="http://localhost:2368/content/images/2017/04/square_to_torus.svg" alt="square to torus"></p>
<p>In such diagrams, it is understood that the two edges with one arrow get glued to each other and the two edges with two arrows get glued to each other. In this case everything is symmetrical so it doesn't matter which pair gets glued first, but there are probably cases in which it does matter. It is conventional to first glue together sides with one arrow, then two arrows and so on.</p>
<p>It is this last diagram which provides us with something we can really use. This <em>identification</em> of sides is crucial to defining the torus as a quotient space of the square. What we are really doing here is partitioning the square in such a way that certain pairs of points on the boundary get grouped into the same equivalence class. More precisely, we partition the square into many sets, namely every point in the interior of the square and each pair of opposite points on the square's boundary:</p>
<p>$$\begin{align}<br>
B_{x,y} &amp;= \{(x,y)\}\text{ for } (x,y)\in (0,1)^2, \\<br>
C_x     &amp;= \{(x,0),(x,1)\}\text{ for } x\in (0,1), \\<br>
D_y     &amp;= \{(0,y),(1,y)\}\text{ for } y\in (0,1), \\<br>
E        &amp;= \{(0,0),(0,1),(1,0),(1,1)\}.<br>
\end{align}$$</p>
<p>Allow me to explain these choices a little bit. We have defined one set $B_{x,y}$ in the partition for every point $(x,y)$ in the interior of the square. This is because in our quotient space, we do not want the interior of the square to collapse at all, and so every point should be in its own equivalence class. There is one set $C_x$ for every pair of points along the bottom and top of the square, and one set $C_y$ for every pair along the left and right edges. Putting these pairs into the same equivalence classes ensures that they will become one thing, i.e. 'glued together,' in the quotient space. Lastly, $E$ includes the corners of the square separately to avoid double-counting them.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> This also ensures explicitly that the four corners of the square will all end up being glued together.</p>
<p>Going back to our original definitions for quotient maps and spaces, we define the set $A$ as the collection of all sets in this partition and the quotient map $f$ as the surjective function mapping each point in the square to the set in $A$ which contains it. Then it is easy to show that the resulting quotient space is (homeomorphic to) the torus $S^1\times S^1$, so we have accomplished what we set out to.</p>
</blockquote>
<p>Many, many more examples of quotient spaces can be generated easily in much the same manner. We can define the sphere as a quotient space of pretty much any polygon, for instance. However, it is tedious and difficult to draw diagrams and they can be found all over the internet anyway. This post is also nice and short for a change, so I'm going to stop here. The good news is that I've now introduced most of the common methods for constructing new topological spaces!</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Remember that the sets in a partition must be pairwise disjoint. <a href="#fnref1" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
</ol>
</section>
]]></content:encoded></item><item><title><![CDATA[Equivalence Relations and Quotient Sets]]></title><description><![CDATA[Quotient sets of $A$ are comprised not of elements of $A$, but of the equivalence classes they fall into. This gives us a powerful method to collapse a set into a smaller set that is in some way still representative of the original set.]]></description><link>http://localhost:2368/equivalence-relations-and-quotient-sets/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae220</guid><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Sun, 09 Apr 2017 00:41:51 GMT</pubDate><content:encoded><![CDATA[<p>The next topological construction I'm going to talk about is the quotient space, for which we will certainly need the notion of quotient sets. However, equivalence relations and quotient sets show up all over the place in mathematics and are worth studying on their own because of their tremendous importance and ubiquitousness.</p>
<p>The first concept I should introduce is that of a relation. A special type of relation, called an equivalence relation, will be vital to all of the content in this post.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>relation</strong> on a set $A$ is a subset of the Cartesian product $A\times A$.</p>
</blockquote>
<p>So technically any subset of $A\times A$ is a relation on $A$, but most of them are usually boring and have little meaning. Relations are generally used to compare two elements in some way. That is, we use them to determine whether two elements are 'related' in the manner specified. We should take a look at some characteristics that relations may possess which make them more interesting, but first I'd like to give some examples of familiar relations.</p>
<blockquote>
<p><strong>Example.</strong> Take $A=\mathbb{N}$, the set of natural numbers. The &quot;less than or equal to&quot; relation on $\mathbb{N}$, usually written $\le$, is defined so that $(a,b)\in\le$ if and only if $b=a+x$ for some $x\in\mathbb{N}$. Clearly</p>
<p>$$\le \; := \{(a,b)\in\mathbb{N}^2\mid b=a+x\text{ for some }x\in\mathbb{N}\}$$</p>
<p>fits our definition of a relation because it is a subset of $\mathbb{N}\times\mathbb{N}$. However, no one ever writes things this way. Normal people use infix notation. That is, they write $a\le b$ rather than $(a,b)\in\le$. I will pretty much use infix notation for the rest of time since it tends to simplify things a great deal, and it is probably what you're used to seeing.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> Take $A=\mathbb{Z}$, the set of integers, and choose some $n\in\mathbb{Z}$. We can define a relation $=_n$ on $\mathbb{Z}$ so that for any $a,b\in\mathbb{Z}$, we have that $a=_n b$ if and only if $b-a=kn$ for some $k\in\mathbb{Z}$. We can write this more formally as</p>
<p>$$=_n \; := \{(a,b)\in\mathbb{Z}^2\mid b-a=kn\text{ for some }k\in\mathbb{Z}\}$$</p>
<p>but there isn't usually any benefit in doing things that way, and I think it's even a little bit confusing to look at. By the way, this relation is called <strong>congruence modulo $n$</strong> and it is of tremendous importance to many fields of mathematics. You can bet we'll be seeing this again at some point soon.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> For any set $A$, both $\varnothing$ and $A\times A$ are relations on $A$. The $\varnothing$ relation doesn't relate elements of $A$ to anything, not even themselves. On the other hand, the relation $A\times A$ relates every element to every element of $A$. These are two extreme sorts of relations, but neither is particularly interesting or important.</p>
</blockquote>
<p>Now that I've given you a few examples, hopefully the definition of a relation has had time to sink in and begun to make a bit of sense. As promised I'll now discuss some important qualities that a relation may or may not have.</p>
<blockquote>
<p><strong>Definition.</strong> A relation $\sim$ on a set $A$ is <strong>reflexive</strong> if $a\sim a$ for every $a\in A$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> The relation $\le$ on $\mathbb{N}$ is reflexive because every natural number is less than or equal to itself, i.e. $n\le n$ for every $n\in\mathbb{N}$</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> Another example of a reflexive relation is that of congruence modulo $n$. This is because $a-a=0n$ for every $a\in\mathbb{Z}$.</p>
</blockquote>
<p>The reflexive property holds for many important relations, and is in general quite easy to verify. This is because in most relations of interest to mathematicians, elements tend to be related to themselves. In fact, the only relation we discussed above that is not reflexive is the empty relation.</p>
<blockquote>
<p><strong>Definition.</strong> A relation $\sim$ on a set $A$ is <strong>symmetric</strong> if $b\sim a$ whenever $a\sim b$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> The relation $=_n$ on $\mathbb{Z}$ of congruence modulo some $n\in\mathbb{Z}$ is symmetric. To see this, suppose $a=_n b$ for some $a,b\in\mathbb{Z}$. Then by definition, $b-a=kn$ for some integer $k$. Negating each side, we see that $a-b=-kn$ and since $-k$ is also an integer it follows that $b=_n a$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> The relation $\le$ on $\mathbb{N}$, on the other hand, is not symmetric. We can see this by examining a single counterexample. Clearly $3\le 5$ but it is not true that $5\le 3$.</p>
</blockquote>
<p>The final property I'm going to talk about is generally the most difficult to demonstrate holds for a particular relation. It sort of resembles the triangle inequality.</p>
<blockquote>
<p><strong>Definition.</strong> A relation $\sim$ on a set $A$ is <strong>transitive</strong> if $a\sim c$ whenever both $a\sim b$ and $b\sim c$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> It's not too hard to show that the relation $\le$ on $\mathbb{N}$ is transitive. Consider $a,b,c\in\mathbb{N}$ such that $a\le b$ and $b\le c$. Then by definition, $b=a+x$ and $c=b+y$ for some $x,y\in\mathbb{N}$. Substituting the first equation into the second, we see that $c=a+x+y$. And since $x+y\in\mathbb{N}$, it follows that $a\le c$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> Next, consider again the relation $=_n$ on $\mathbb{Z}$ of congruence modulo some integer $n$. Suppose $a=_n b$ and $b=_n c$. Then $b-a=k_1 n$ and $c-b=k_2 n$ for some integers $k_1$ and $k_2$. Thus,</p>
<p>$$\begin{align}<br>
c-a &amp;= (c-b) + (b-a) \\<br>
&amp;= k_2 n - k_1 n \\<br>
&amp;= (k_2-k_1)n.<br>
\end{align}$$</p>
<p>Since $k_2-k_1\in\mathbb{Z}$, it follows that $a=_n c$.</p>
</blockquote>
<p>Now that we have established these three types of relation, it is a piece of cake to define an equivalence relation:</p>
<blockquote>
<p><strong>Definition.</strong> An <strong>equivalence relation</strong> is a relation which is reflexive, symmetric and transitive.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> We've established above that congruence modulo $n$ satisfies each of these properties, which automatically makes it an equivalence relation on the integers.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> The relation &quot;is the same age as&quot; on the set of all people is an equivalence relation. Every person is the same age as him/herself. If person $A$ is the same age as person $B$, then certainly person $B$ is the same age as person $A$. And transitivity also holds, but I'm too lazy to type that one out right now.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> The relation &quot;has shaken hands with&quot; on the set of all people is <em>not</em> an equivalence relation because it is not transitive. For instance, it is entirely possible that Bob has shaken Fred's hand and Fred has shaken hands with the president, yet this does not necessarily mean that Bob has shaken the president's hand.</p>
</blockquote>
<p>As you will learn, equivalence relations pop up constantly in every area of mathematics. This is because they give sets a very nice kind of structure. To explain this further, we first need the following concepts:</p>
<blockquote>
<p><strong>Definition.</strong> Given an equivalence relation $\sim$ on a set $A$ and an element $a\in A$, the <strong>equivalence class</strong> of $A$ is the set $[a]=\{x\in A\mid a\sim x\}$.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> Given an equivalence relation $\sim$ on a set $A$, the set of equivalence classes corresponding to $\sim$ is called a <strong>quotient set</strong><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> and is written $A/\negthickspace\sim$.</p>
</blockquote>
<p>So quotient sets of $A$ are comprised not of elements of $A$, but of the equivalence classes they fall into. This gives us a powerful method to collapse a set into a smaller set that is in some way still representative of the original set. Hopefully the following example will help make some sense of this.</p>
<blockquote>
<p><strong>Example.</strong> Let's take another look at the set $\mathbb{Z}$ and the relation $=_3$ of congruence modulo $3$.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> Under this relation, two integers $a$ and $b$ are related if $b-a=3k$ for some integer $k$. Put more plainly, two integers are congruent if their difference is a multiple of $3$.</p>
<p>How many equivalence classes does this relation create? It isn't too tough to see that there are three: $[0], [1]$ and $[2]$. This is because all multiples of three, i.e. elements of the form $3k$ for some integer $k$, are congruent. Similarly, all elements of the form $3k+1$ are congruent and all elements of the form $3k+2$ are congruent. And these are all the possible options, really. I have chosen $0, 1$ and $2$ as the <strong>representatives</strong> of these equivalence classes, but this choice was arbitrary (albeit standard). I could just as easily have named them $[3000], [16]$ and $[-1]$.</p>
<p>Lastly, we see that the quotient set $\mathbb{Z}/\negthickspace=_3$ is just the set $\big\{[0],[1],[2]\big\}$. That is, all congruent elements are essentially collapsed to a single point in the quotient set.</p>
</blockquote>
<p>There is one final topic I need to talk about here, which is the fact that equivalence classes form partitions of sets. This is called the Fundamental Theorem of Equivalence Relations, but I'm getting ahead of myself. Before I prove it, I need to tell you what a partition is.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>partition</strong> $\cal P$ of a set $A$ is a collection of subsets of $A$ which satisfies the following properties:</p>
<ol>
<li>The empty set is not in $\cal P$.</li>
<li>For any two sets $X,Y$ in $\cal P$, either $X$ and $Y$ are disjoint or $X=Y$.</li>
<li>The union $\bigcup\limits_{X\in\cal P}X$ of all subsets in the partition is equal to $A$ itself.</li>
</ol>
</blockquote>
<p>This is all basically a fancy way of saying that a partition is a method of breaking a set into non-overlapping subsets. Now it's time to prove that Fundamental Theorem thingy I mentioned a moment ago.</p>
<blockquote>
<p><strong>Fundamental Theorem of Equivalence Relations.</strong> Let $\sim$ be an equivalence relation on a set $A$. Then the quotient set $A/\negthickspace\sim$ is a partition of $A$.</p>
<p><strong>Proof.</strong> All we need to do is show that the three properties of partitions hold. Clearly the empty set $\varnothing$ is not in the quotient set $A/\negthickspace\sim$ because the reflexive property of equivalence relations tells us that $x\sim x$, and thus $x\in [x]$ for every $x\in A$.</p>
<p>Next, suppose that $[x]$ and $[y]$ are equivalences classes in $A/\negthickspace\sim$ and that they are disjoint, i.e. $[x]\cap [y]\ne\varnothing$. Then there exists some $a\in A$ such that $a\in [x]\cap [y]$. That is, $a\in [x]$ and $a\in [y]$, so $a\sim x$ and $a\sim y$. By the symmetric property of equivalence relations, $x\sim a$. And by the transitive property, $x\sim y$. Thus, it follows that $[x]=[y]$.</p>
<p>Finally, it is clear from the union lemma that the union of all equivalence classes, $\bigcup\limits_{[x]\in A/\sim}[x]$, is the entire set $A$.</p>
</blockquote>
<p>Notice that we used all three properties of equivalence relations (reflexivity, symmetry and transitivity) to prove this result. This indicates that equivalence relations are the only relations which partition sets in this manner. It turns out that this is true, and it's very easy to prove. I won't do that here because this post is already longer than I intended, but I will at least state the theorem.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $\cal P$ be a partition of a set $A$ and define a relation $\sim$ by $x\sim y$ if and only if $x,y\in X$ for some $X\in\cal P$. Then $\sim$ is an equivalence relation.</p>
</blockquote>
<p>What this theorem really tells us is that every partition is the quotient set of some equivalence relation, and that's a really cool idea.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>More formally, it is called the <strong>quotient set of $A$ modulo $\sim$.</strong> <a href="#fnref1" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
<li id="fn2" class="footnote-item"><p>It's easy to extend this example to integers other than $3$, but I feel like a more concrete example is useful here. <a href="#fnref2" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
</ol>
</section>
]]></content:encoded></item><item><title><![CDATA[Product Spaces]]></title><description><![CDATA[Next let's talk about an intuitive way to combine topological spaces to create new spaces which inherit certain characteristics from their parents. We've talked about Cartesian products before in the context of set theory, but what happens if we take the Cartesian product of topological spaces?]]></description><link>http://localhost:2368/product-spaces/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae21f</guid><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Sat, 08 Apr 2017 20:54:38 GMT</pubDate><content:encoded><![CDATA[<h3 id="contents">Contents</h3>
<ol>
<li><a href="#definition">Definition</a></li>
<li><a href="#examples">Examples</a></li>
</ol>
<hr>
<h3 id="definitionanamedefinition">Definition<a name="definition"></a></h3>
<p>Next let's talk about an intuitive way to combine topological spaces to create new spaces which inherit certain characteristics from their parents. We've talked about Cartesian products before in the context of set theory, but what happens if we take the Cartesian product of topological spaces? What should the topology look like?</p>
<p>Obviously there are many options (to name a few, the discrete or trivial topologies can be defined on any set), but we would like to choose a topology that is as natural as possible and inherits its properties from the spaces from which it is built. This decision actually has its roots in category theory, but I hope that the choice will make some sense to you nonetheless.</p>
<p>Let's say we are given topological spaces $X$ and $Y$ and we want to construct a &quot;natural&quot; topology on $X\times Y$. Our first instinct might be to choose as the open sets all products $U\times V$ where $U$ is open in $X$ and $V$ is open in $Y$. But even in $\mathbb{R}\times\mathbb{R}$ we can see that this doesn't result in a topology, since the union of products of open sets isn't necessarily itself a product of open sets, as illustrated below.</p>
<p><img src="http://localhost:2368/content/images/2017/04/nonono.svg" alt="no no no"></p>
<p>So clearly the products of open sets aren't going to form a topology by themselves, since they are not closed under unions. We don't throw out this idea entirely, though. It just so happens that the products of open sets do form a <em>basis</em> for a topology on $X\times Y$.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $(X,{\cal T}_X)$ and $(Y,{\cal T}_Y)$ denote topological spaces. Then ${\cal B} = \{U\times V\mid U\in {\cal T}_X, V\in {\cal T}_V\}$ is a basis for a topology on $X\times Y$.</p>
<p><strong>Proof.</strong> We argue first that for every $(x,y)\in X\times Y$ there exists a basis element $B\in {\cal B}$ with $(x,y)\in B$. Take $B=X\times Y$. Since $x\in X$ and $y\in Y$, clearly $(x,y)\in B$.</p>
<p>Next, suppose we are given basis elements $B_1, B_2\in{\cal B}$ for which $B_1\cap B_2\ne\varnothing$. Then by definition $B_1=U_1\times V_1$ and $B_2=U_2\times V_2$ for some open sets $U_1, U_2\in{\cal T}_X$ and $V_1, V_2\in{\cal T}_Y$. Note that $U=U_1\cap U_2\in{\cal T}_X$ and $V=V_1\cap V_2\in{\cal T}_Y$. Thus,</p>
<p>$$\begin{align}<br>
B_1\cap B_2 &amp;= (U_1\times V_1)\cap (U_2\times V_2) \\<br>
&amp;= (U_1\cap U_2)\times (V_1\cap V_2) \\<br>
&amp;= U\times V \\<br>
&amp;\in {\cal B}.<br>
\end{align}$$</p>
<p>It follows that $B_1\cap B_2$ is a basis element contained in itself, completing the proof.</p>
</blockquote>
<p>Now that we have a natural basis for a topology on the product of two spaces, defining the product topology is a piece of cake.</p>
<blockquote>
<p><strong>Definition.</strong> Let Let $(X,{\cal T}_X)$ and $(Y,{\cal T}_Y)$ denote topological spaces. The <strong>product topology</strong> on $X\times Y$ is the topology generated by the basis ${\cal B} = \{U\times V\mid U\in {\cal T}_X, V\in {\cal T}_V\}$. We call $X\times Y$ a <strong>product space</strong> when equipped with this topology.</p>
</blockquote>
<p>Just to refresh your memory, the open sets in the topology generated by a basis are the empty set and all unions of basis elements. This also guarantees that the entire space is open as a result of the union lemma, as we saw several posts ago.</p>
<p>The product topology can easily be extended in the obvious way to the Cartesian product of a finite numbers of sets. This basis even generates a topology for an infinite number of sets, but in that case it is actually not the topology we generally use. For an infinite number of sets, the product topology has a few extra restrictions. The basis we just gave extends to what is called the <strong>box topology</strong> for an infinite product, and it has some undesirable properties. However, I'm fairly confident that I will never need to talk about infinite products on this blog, so I'm going to leave the discussion at that for now.</p>
<h3 id="examplesanameexamples">Examples<a name="examples"></a></h3>
<blockquote>
<p><strong>Example.</strong> For our first example, consider $\mathbb{R}$ with the standard topology. What is the product topology on $\mathbb{R}\times\mathbb{R}=\mathbb{R}^2$? Well we know that a basis for this topology is all products of open intervals. If $(a,b)$ and $(c,d)$ are open intervals in $\mathbb{R}$ then $(a,b)\times(c,d)$ can be viewed as an open rectangle in $\mathbb{R}^2$. But open rectangles, just like open balls, generate the standard topology on $\mathbb{R}^2$. So the product topology on $\mathbb{R}^2$ is actually the standard topology, and the same holds for any finite product of $\mathbb{R}$.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> For our next example, consider the closed interval $[0,1]$ as a subspace of $\mathbb{R}$. The product $[0,1]\times[0,1]$ is just the unit square in $\mathbb{R}^2$. Open sets in the square are unions of products of open sets in $[0,1]$. That is, they are unions of open rectangles.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> Consider the unit circle $S^1=\{(x,y)\in\mathbb{R}^2\mid x^2+y^2=1\}$ as a subspace of $\mathbb{R}^2$. Let's begin by visualizing $S^1\times S^1$. Recalling the definition of the Cartesian product, we can think loosely of each point on the first circle $S^1$ as corresponding to an entire circle. We thus obtain the <strong>torus</strong>, which is a donut-shaped subset of $\mathbb{R}^3$.</p>
<p><img src="http://localhost:2368/content/images/2017/04/torus-1.svg" alt="torus"></p>
<p>Notice that the torus is hollow. If we wanted a solid torus, we would take $S^1\times D^2$, where $D^2=\{(x,y)\in\mathbb{R}^2\mid x^2+y^2\le 1\}$ is the closed unit ball.</p>
<p>Before we can think about the topology on the torus $S^1\times S^1$, we should first consider the topology on the circle $S^1$. Since it's a subspace of $\mathbb{R}^2$, open sets in the circle are intersections of $S^1$ with open sets in $\mathbb{R}^2$. These open sets basically look like unions of &quot;open intervals&quot; wrapped around the circle. In fact, they are all homeomorphic to open intervals, except for $S^1$ itself (which I will prove when I talk about connectedness).</p>
<p><img src="http://localhost:2368/content/images/2017/04/circle_open_sets.svg" alt="circle open sets"></p>
<p>Products of these open sets somewhat resemble open rectangles wrapped around the surface of the torus. We'll call them open patches, and the unions of these open patches form the open sets on the torus.</p>
<p><img src="http://localhost:2368/content/images/2017/04/torus_open_sets.svg" alt="torus open sets"></p>
</blockquote>
<p>Since $S^1\times S^1$ can also be viewed as a subspace of $\mathbb{R}^3$, we could also view the open sets on the torus as intersections of open sets in $\mathbb{R}^3$ with $S^1\times S^1$. This statement may seen obvious, but I haven't proved it yet.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ and $Y$ denote topological spaces with $A\subseteq X$ and $B\subseteq Y$. The topology on $A\times B$ as a subspace of $X\times Y$ is the same as the product topology where $A$ is a subspace of $X$ and $B$ is a subspace of $Y$.</p>
<p><strong>Proof.</strong> We argue that any open set in either topology is also open in the other. Choose an open set $U$ in the subspace topology on $A\times B$. By definition, there exists some open set $V$ in $X\times Y$ such that $U=(A\times B)\cap B$. Since it is open in the product topology on $X\times Y$, we have that $V$ must be a union of products of open sets. That is,</p>
<p>$$\begin{align}<br>
V &amp;= \bigcup\limits_{i\in I}(S_i\times T_i) \\<br>
&amp;= \bigcup\limits_{i\in I}S_i\times\bigcup\limits_{i\in I}T_i,<br>
\end{align}$$</p>
<p>where $I$ is an indexing set such that $S_i$ is open in $X$ and $T_i$ is open in $Y$ for every $i\in I$. But this means that $U$ is open in the product topology on $A\times B$.</p>
<p>The proof of the reverse direction is completely symmetrical.</p>
</blockquote>
<p>So any open set on the torus can also be expressed as the intersection of open balls in $\mathbb{R}^3$ with $S^1\times S^1$. This may or may not be a simpler way of viewing the topology on the torus, depending on the application.</p>
<p>I would like to conclude with the proof I promised you in my last post, which greatly simplified the task of showing that the $x$-axis as a subspace of $\mathbb{R}^2$ is homeomorphic to $\mathbb{R}$. This proof closely mimics the corresponding proof in my last post, although I have defined the homeomorphism in the opposite direction just to spice things up a bit. Notice first that the $x$-axis may be written as $\mathbb{R}\times\{0\}$.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $A$ and $B$ denote topological spaces with $b\in B$ and consider $A\times\{b\}$ as a subspace of $A\times B$ with the product topology. Then $A$ is homeomorphic to $A\times\{b\}$.</p>
<p><strong>Proof.</strong> We will argue that $f:A\to A\times\{b\}$ defined by $f(a)=(a,b)$ is a homeomorphism. Certainly $f$ is bijective and its inverse function $f^{-1}:A\times\{b\}\to A$ is given by $f^{-1}\big((a,b)\big)=a$.</p>
<p>First we'll show that $f$ is continuous. Let $U$ denote an open set in $A\times\{b\}$. We can write $U$ as the intersection of $A\times\{b\}$ with some union of basis elements of $A\times B$, which are themselves products of open sets. That is, for some indexing set $I$,</p>
<p>$$U=(A\times B)\cap\bigcup\limits_{i\in I}(A_i\times B_i),$$</p>
<p>where $A_i\subseteq A$ and $B_i\subseteq B$ are open for every $i\in I$. Thus,</p>
<p>$$\begin{align}<br>
f^{-1}[U] &amp;= f^{-1}\left[(A\times B)\cap\bigcup_{i\in I}(A_i\times B_i)\right]\\<br>
&amp;= f^{-1}[A\times B]\cap f^{-1}\left[\bigcup_{i\in I}(A_i\times B_i)\right]\\<br>
&amp;= f^{-1}[A\times B]\cap \bigcup_{i\in I}f^{-1}[A_i\times B_i]\\<br>
&amp;= A\cap\bigcup_{i\in I}A_i\\<br>
&amp;= \bigcup_{i\in I}A_i,<br>
\end{align}$$</p>
<p>which is certainly open in $A$ since it is the union of open sets.</p>
<p>It is easier to show that $f^{-1}$ is continuous. Let $V$ denote an open set in $A$. Note that $V\times B$ is a basis element for $A\times B$ and is thus open in $A\times B$. Therefore,</p>
<p>$$\begin{align}<br>
(f^{-1})^{-1}[V] &amp;= f[V] \\<br>
&amp;= V\times\{b\} \\<br>
&amp;= (A\times\{b\})\cap(V\times B).<br>
\end{align}$$</p>
<p>This is open in $A\times\{b\}$ because it is the intersection of $A\times\{b\}$ with an open set in $A\times B$, completing the proof.</p>
</blockquote>
<p>Using this result, we can immediately construct homeomorphisms</p>
<p>$$\begin{align}<br>
\mathbb{R}&amp;\to\mathbb{R}\times\{b\}\subset\mathbb{R}^2 \\<br>
\mathbb{R}^2&amp;\to\mathbb{R}^2\times\{b\}\subset\mathbb{R}^3 \\<br>
\mathbb{R}^3&amp;\to\mathbb{R}^3\times\{b\}\subset\mathbb{R}^4 \\<br>
&amp;\;\; \vdots \\<br>
\mathbb{R}^n&amp;\to\mathbb{R}^n\times\{b\}\subset\mathbb{R}^{n+1} \\<br>
&amp;\;\; \vdots<br>
\end{align}$$</p>
<p>between $\mathbb{R}^i$ and 'horizontal' hyperplanes in $\mathbb{R}^{i+1}$. It is also possible to show that arbitrary hyperplanes (formally $n$-dimensional subspaces in the linear algebraic sense) of $\mathbb{R}^{i+1}$ are homeomorphic to $\mathbb{R}^i$, but the proof would require a change of basis and that isn't something we have the machinery to get into right now.</p>
<p>Anyway, that's all the time I have right now and I think I've done enough to introduce product spaces. May this knowledge aid you in your quest and be your savior in many battles.</p>
]]></content:encoded></item><item><title><![CDATA[Subspaces]]></title><description><![CDATA[A topological space is, at its core, just a set with some additional structure. So what if we want to keep the structure, but change the underlying set? There's an easy and somewhat obvious way to do this.]]></description><link>http://localhost:2368/subspaces/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae21e</guid><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Sat, 08 Apr 2017 16:47:05 GMT</pubDate><content:encoded><![CDATA[<h3 id="contents">Contents</h3>
<ol>
<li><a href="#definition">Definition</a></li>
<li><a href="#examples">Examples</a></li>
</ol>
<hr>
<h3 id="definitionanamedefinition">Definition<a name="definition"></a></h3>
<p>In this post I'm going to introduce a classic method which allows us to construct new topological spaces from existing ones in a natural way.</p>
<p>A topological space is, at its core, just a set with some additional structure. So what if we want to keep the structure, but change the underlying set? There's an easy and somewhat obvious way to do this.</p>
<blockquote>
<p><strong>Definition.</strong> Let $X$ denote a topological space with topology $\cal T$ and suppose $A\subseteq X$. Then ${\cal T}_A=\{A\cap U\mid U\in{\cal T}\}$ is called the <strong>subspace topology</strong> on $A$. Equipped with this topology, we call $A$ a <strong>subspace</strong> of $X$.</p>
</blockquote>
<p>In standard English, this says that in the subspace topology on $A$, a set is open if it is the intersection of $A$ with some open set in $X$.</p>
<p>Now we have to prove that we are justified in calling this thing a topology. We need to show that the empty set and the subspace itself are open, that unions of opens sets are open, and that finite intersections of open sets are open.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ denote a topological space with topology $\cal T$ and suppose $A\subseteq X$. Then ${\cal T}_A=\{A\cap U\mid U\in{\cal T}\}$ is a topology on $A$.</p>
<p><strong>Proof.</strong> Notice first that $\varnothing, X\in\cal T$ by the definition a topology. Certainly $\varnothing=A\cap\varnothing\in{\cal T}_A$, so the empty set is open in the subspace. Similarly, $A=A\cap X\in{\cal T}_A$ because $A\subseteq X$, so $A$ itself is open in the subspace.</p>
<p>Next, suppose that $I$ is an indexing set such that $V_i\in{\cal T}_A$ is open in the subspace for each $i\in I$. Then for every $i\in I$ we have that $V_i=A\cap U_i$ for some open set $U_i\in{\cal T}$, by the definition of the subspace topology. Since $\bigcup\limits_{i\in I}U_i\in\cal T$ it is clear that</p>
<p>$$\begin{align}<br>
\bigcup\limits_{i\in I}V_i &amp;= \bigcup\limits_{i\in I}(A\cap U_i) \\<br>
&amp;= A\cap \bigcup\limits_{i\in I}U_i \\<br>
&amp;\in{\cal T}_A,<br>
\end{align}$$</p>
<p>so we have shown that the union of any collection of open sets in the subspace is also open in the subspace.</p>
<p>Finally, suppose that $V_i\in{\cal T}_A$ is open in the subspace for $1\leq i\leq n$ for some $n\in\mathbb{Z}^+$. Then for $1\leq i\leq n$, we again have that $V_i=A\cap U_i$ for some open set $U_i\in{\cal T}$, by the definition of the subspace topology. Since $\bigcap\limits_{i=1}^n U_i\in\cal T$, it is clear that</p>
<p>$$\begin{align}<br>
\bigcap\limits_{i=1}^n V_i &amp;= \bigcap\limits_{i=1}^n (A\cap U_i) \\<br>
&amp;= A\cap \bigcap\limits_{i=1}^n U_i \\<br>
&amp;\in {\cal T}_A,<br>
\end{align}$$</p>
<p>so we have shown that the intersection of a finite collection of open sets in the subspace is also open in the subspace. It follows that the subspace topology on $A$ is a topology, as desired.</p>
</blockquote>
<p>I have noticed that people often find the following observation confusing: If $A$ is a subset of a topological space $X$, then the set $A$ is both open and closed in the subspace topology on $A$. This is automatically true because of the definition of a topology. However, this says nothing at all about whether $A$ is open or closed as a <em>subset</em> of $X$. Similarly, there are plenty of sets that may be open or closed in the subspace topology that may not have been that way in the original topology. Make sure you pay attention to the distinction between subspaces and subsets!</p>
<p>Next, there's a way to figure out what sets in a subspace are closed which is sometimes more direct than trying to make sure that their complements are open.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $A$ be a subspace of a topological space $X$. Then $U\subseteq A$ is closed in $A$ if and only if $U=A\cap V$ for some closed set $V\subseteq X$.</p>
<p><strong>Proof.</strong> First, suppose that $U$ is closed in $A$. Then $A-U$ is open by definition, so $A-U=A\cap W$ for some open set $W$ in $X$. Certainly $V=X-W$ is closed in $X$, so</p>
<p>$$\begin{align}<br>
U &amp;= A-(A-U) \\<br>
&amp;= A-(A\cap W) \\<br>
&amp;= A-W \\<br>
&amp;= A\cap (X-W) \\<br>
&amp;= A\cap V.<br>
\end{align}$$</p>
<p>Conversely, suppose that $U=A\cap V$ for some closed set $V$ in $X$. Then $X-V$ is open in $X$, so</p>
<p>$$\begin{align}<br>
A-U &amp;= A-(A\cap V) \\<br>
&amp;= A\cap (X-V)<br>
\end{align}$$</p>
<p>is open in $A$. Thus, $U$ is closed in $A$.</p>
</blockquote>
<p>I may not have explicitly proved all those set equalities in the past, but if they aren't immediately obvious to you then this might be a good time to go back and get some more practice with set-theoretic proofs.</p>
<h3 id="examplesanameexamples">Examples<a name="examples"></a></h3>
<p>Let's look at a few examples of subspaces of topological spaces we are already familiar with.</p>
<blockquote>
<p><strong>Example.</strong> Consider $\mathbb{R}$ with the standard topology. Obviously the half-open interval $[0,1)$ is a subset of $\mathbb{R}$. So what is the subspace topology on $[0,1)$?</p>
<p>Well, we already know it. The open sets are just intersections of $[0,1)$ with any open set in $\mathbb{R}$. Of course, the subspace $[0,1)$ is itself both open and closed.</p>
<p>Is $(0,1)$ open in the subspace topology? Of course it is. Certainly $(0,1)$ is open in $\mathbb{R}$, and $(0,1)=[0,1)\cap (0,1)$, so it is open in the subspace as well.</p>
<p>Play around with this a little bit more and you'll notice that the open sets in $[0,1)$ actually look a lot like the open sets in $\mathbb{R}$ which happen to be subsets of $[0,1)$. This is just a testament to the fact that the subspace topology is a very natural object. In fact, whenever I talk about an interval, I'll generally be assuming that it is equipped with the subspace topology induced by the standard topology on $\mathbb{R}$. Even though I won't always explicitly mention this, it will be especially important to remember when I talk about paths and homotopy in the future.</p>
</blockquote>
<p>Let's look at another example, shall we?</p>
<blockquote>
<p><strong>Example</strong>. Consider $\mathbb{R}^2$ in the standard topology, and the set $A=\{(x,0)\in\mathbb{R}^2\mid x\in\mathbb{R}\}$.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> The open sets in the subspace topology on $A$ are, of course, any sets which can be expressed as the intersection of $A$ with unions of open balls in $\mathbb{R}^2$.</p>
<p>This looks an awful lot like the standard topology on $\mathbb{R}$, but technically it isn't because $A\ne\mathbb{R}$. In reality, $A$ is just the $x$-axis in the two-dimensional Euclidean plane. So it's basically $\mathbb{R}$, and behaves exactly like it. What we have, then, is that $A$ is homeomorphic to $\mathbb{R}$ with the standard topology.</p>
</blockquote>
<p>Let's prove the assertion I just made. It's going to be a bit of work, but I think it's worth proving things just for fun once in a while. The homeomorphism we will choose in our proof is the obvious choice, but there are actually others that could work just as well.</p>
<blockquote>
<p><strong>Proposition.</strong> Consider $\mathbb{R}$ with the standard topology and $A=\{(x,0)\in\mathbb{R}^2\mid x\in\mathbb{R}\}$ as a subspace of $\mathbb{R}^2$ with the standard topology. The spaces $\mathbb{R}$ and $A$ are homeomorphic.</p>
<p><strong>Proof.</strong> We argue that the function $f:A\to\mathbb{R}$ defined by $f\big((x,0)\big)=x$ is a homeomorphism. It is trivial to check that $f$ is bijective and that its inverse function $f^{-1}:\mathbb{R}\to A$ is given by $f^{-1}(x)=(x,0)$.</p>
<p>We will show first that $f^{-1}$ is continuous. Choose an open set $U$ in $A$. By definition, we can write $U$ as the intersection of $A$ with some union of open balls in $\mathbb{R}^2$. That is,</p>
<p>$$U=A\cap\bigcup\limits_{i\in I} B\big((x_i,y_i),r_i\big)$$</p>
<p>for some indexing set $I$, where $(x_i,y_i)\in\mathbb{R}^2$ and $r_i\in\mathbb{R}^+$ for each $i\in I$. It follows that</p>
<p>$$\begin{align}<br>
(f^{-1})^{-1}[U] &amp;= f[U] \\<br>
&amp;= f\left[A\cap\bigcup\limits_{i\in I}B\big((x_i,y_i),r_i\big)\right] \\<br>
&amp;= f\left[\bigcup\limits_{i\in I}A\cap B\big((x_i,y_i),r_i\big)\right] \\<br>
&amp;= \bigcup\limits_{i\in I}f\Big[A\cap B\big((x_i,y_i),r_i\big)\Big].<br>
\end{align}$$</p>
<p>A small amount of geometry and the Pythagorean Theorem gets us that for each $i\in I$, the set $f\Big[A\cap B\big((x_i,y_i),r_i\big)\Big]$ is equal to the open interval</p>
<p>$$\left(x_i-\sqrt{r_i^2-y_i^2}, x_i+\sqrt{r_i^2-y_i^2}\right)$$</p>
<p>if $r_i&gt;y_i$, and it is empty if $r_i\leq y_i$. The union of open intervals is open in $\mathbb{R}$, so $(f^{-1})^{-1}[U]$ is open in $\mathbb{R}$ and thus $f^{-1}$ is continuous.</p>
<p>Next, we will show that $f$ is continuous. Choose an open set $U$ in $\mathbb{R}$. By definition, $U$ is a union of open intervals in $\mathbb{R}$ (possibly $\mathbb{R}$ itself, in which case $f^{-1}[U]=f^{-1}[\mathbb{R}]=A$ is certainly open). More precisely, we can write</p>
<p>$$U=\bigcup\limits_{i\in I}(x_i-r_i,x_i+r_i),$$</p>
<p>where $I$ is some indexing set such that $x_i\in\mathbb{R}$ and $r_i\in\mathbb{R}^+$ for every $i\in I$. It follows that</p>
<p>$$\begin{align}<br>
f^{-1}[U] &amp;= f^{-1}\left[\bigcup\limits_{i\in I}(x_i-r_i,x_i+r_i)\right] \\<br>
&amp;= f^{-1}\left[\bigcup\limits_{i\in I}\left\{x\in\mathbb{R}\bigg\vert \vert x-x_i\vert&lt; r_i\right\}\right] \\<br>
&amp;= \bigcup\limits_{i\in I}f^{-1}\left[\left\{x\in\mathbb{R}\bigg\vert \vert x-x_i\vert&lt; r_i\right\}\right] \\<br>
&amp;= \bigcup\limits_{i\in I}\left\{(x,0)\in\mathbb{R}^2\bigg\vert\vert x-x_i\vert&lt; r_i\right\} \\<br>
&amp;= \bigcup\limits_{i\in I}\bigg(A\cap B\big((x_i,0),r_i\big)\bigg) \\<br>
&amp;= A\cap \bigcup\limits_{i\in I}B\big((x_i,0),r_i\big).<br>
\end{align}$$</p>
<p>The union of open balls is open, so $f^{-1}[U]$ is the intersection of $A$ with an open set in $\mathbb{R}$ and is thus open in $A$. It follows that $f$ is continuous, completing the proof.</p>
</blockquote>
<p>It ended up being a bit long, but not too difficult. And I think this example really gets the point across that even though homeomorphic spaces may not be identical, they behave in essentially the same way. Most people aren't as careful as I just was, and frequently write that $\mathbb{R}$ is a subspace of $\mathbb{R}^2$. This isn't strictly true, but at least you now know how to interpret it.</p>
<p>Just so you know, we will see another proof of the above proposition when I talk about product spaces. I just really wanted to prove it this way once to give you some geometric intuition behind why it's true.</p>
<p>The last thing I'm going to do in this post is show that an open interval in $\mathbb{R}$ is homeomorphic to $\mathbb{R}$ itself. This will reinforce the notion that stretching a space does not alter its topological properties.</p>
<blockquote>
<p><strong>Proposition.</strong> Let $a,b\in\mathbb{R}$ with $a&lt; b$. Then $\mathbb{R}$ and the interval $(a,b)$ are homeomorphic.</p>
<p><strong>&quot;Proof.&quot;</strong> Define $f:(a,b)\to\mathbb{R}$ by</p>
<p>$$f(x)=\frac{1}{x-a}+\frac{1}{x-b}.$$</p>
<p>We will argue that $f$ is a homeomorphism. I am not going to check that $f$ is a bijection, although it is obvious just from looking at its graph. This could be done using the quadratic formula to find an explicit inverse function, being careful to restrict $f^{-1}$ so that it is surjective. It could also be done using calculus to show that this function is injective because it is monotonically decreasing and surjective by the intermediate value theorem because it approaches negative infinity at its left endpoint and positive infinity at its right endpoint.</p>
<p>The function $f$ is clearly continuous because it is a rational function whose denominator is nonzero for every $x\in(a,b)$. Its inverse, $f^{-1}$ is also continuous for the same reason, but since I haven't explicitly computed it I will not show that this is the case.</p>
</blockquote>
<p>Okay, so that really wasn't a proof at all. But again, a quick glance at the graph of $f$ should really be enough to convince you that it's a homeomorphism. I could be more precise in giving an argument that any two open intervals are homeomorphic, but I think this post has gone on long enough already, and hopefully this fact is obvious to you. On the other hand, closed intervals are <em>not</em> homeomorphic to open intervals. We don't have enough tools to prove that at this point, but it will become trivial when I talk about compactness in a later post.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>$(x,0)$ is not an open interval here, but a point in $\mathbb{R}^2$. That is, it's an ordered pair of real numbers. The fact that open intervals and ordered pairs have the same notation is unfortunate, but if we are careful it should always be clear which we are talking about. <a href="#fnref1" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
</ol>
</section>
]]></content:encoded></item><item><title><![CDATA[Continuity and Homeomorphisms]]></title><description><![CDATA[The concept of continuity is central to the study of topology. So much so, in fact, that whenever anyone talks about a map between topological spaces, they generally expect you to know that they're talking about a continuous map.]]></description><link>http://localhost:2368/continuity-and-homeomorphisms/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae21d</guid><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Sat, 08 Apr 2017 03:08:10 GMT</pubDate><content:encoded><![CDATA[<h3 id="contents">Contents</h3>
<ol>
<li><a href="#continuity">Continuity</a></li>
<li><a href="#homeomorphisms">Homeomorphisms</a></li>
</ol>
<hr>
<h3 id="continuityanamecontinuity">Continuity<a name="continuity"></a></h3>
<p>The concept of continuity is central to the study of topology. So much so, in fact, that whenever anyone talks about a map between topological spaces, they generally expect you to know that they're talking about a continuous map.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p>
<p>I've heard it said that a function is continuous if you can draw its graph without lifting your pencil. This is an awful, imprecise definition, but it does give us a hint of intuition regarding the nature of continuity.</p>
<p>So then what is continuity? Let's take a look at pieces of two functions, $f,g:\mathbb{R}\to\mathbb{R}$.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p>
<p><img src="http://localhost:2368/content/images/2017/04/i_can_haz_continuous.svg" alt="I can haz continuous?"></p>
<p>Your gut instinct should tell you that $f$ is continuous on the interval depicted but $g$ is not. So what does your gut know that you don't? Here's a (very) rough description:</p>
<p>If you zoom in far enough on any point in the graph of $f$, the whole graph behaves similarly in that region. On the other hand, there are pieces of the graph of $g$ which look nothing like nearby pieces.</p>
<p>This is just an informal way of saying that if a function is continuous, points that are close together get mapped to points that are close together. This phrasing is not only much more revealing, it also allows us to extend the definition of continuity away from real-valued functions.</p>
<p>Let's make this notion more precise, in the context of metric spaces. I'll give you the definition first, and then try to make sense of it for you.</p>
<blockquote>
<p><strong>Definition.</strong> Let $X$ and $Y$ denote metric spaces. A function $f:X\to Y$ is <strong>continuous at the point</strong> $x\in X$ if for every real number $\epsilon&gt;0$, there exists a real number $\delta&gt;0$ such that $f\big[B(x,\delta)\big]\subseteq B\big(f(x),\epsilon\big)$.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> A function $f:X\to Y$ is <strong>continuous</strong> if it is continuous at every point $x\in X$.</p>
</blockquote>
<p>All the definition really says is that if a function is continuous at $x$, no matter how closely you look at $f(x)$ there will always be points around $x$ which get mapped within that distance of $f(x)$. So like I said earlier, points that are close together get mapped to points that are close together.</p>
<p>If that makes perfect sense to you, skip the next few paragraphs. Otherwise, I'm going to explain this definition to you very slowly through a story.</p>
<hr>
<p>Think of the person you loathe the most in this world. I'll refer to this person as &quot;the enemy&quot; and use the pronoun &quot;it.&quot;</p>
<p>Say you have two metric spaces $X$ and $Y$, and a continuous map $f:X\to Y$ between them:</p>
<p><img src="http://localhost:2368/content/images/2017/04/some_spaces.svg" alt="some spaces"></p>
<p>You are omnipotent, as in real life, and you know everything there is to know about $X,Y$ and $f$. However, the enemy, being a foolish fool, mistakenly thinks that $f$ is not continuous. And so it tries to trick you. It plots a point $x\in X$ and its image $f(x)\in Y$. Then it draws an open ball $B$ of radius $\epsilon$ around $f(x)$.</p>
<p><img src="http://localhost:2368/content/images/2017/04/turn_1.svg" alt="turn 1"></p>
<p>The enemy challenges you to find an open ball $B'$ around $x$ whose image is contained in $B$. So you think for a while and you use your information about $f$ and you quickly come up with such an open ball, with radius $\delta$.</p>
<p><img src="http://localhost:2368/content/images/2017/04/turn_2.svg" alt="turn 2"></p>
<p>The enemy is angry. It chooses a new smaller ball $B$ and you draw a new, smaller $B'$ to compensate. You do it faster this time, because you've noticed a pattern. No matter what radius $\epsilon$ it chooses to make $B$, you've come up with a method to calculate the radius $\delta$ that $B'$ needs to be so that $f[B']$ will fit inside $B$.</p>
<p>You tell the enemy your method, and it has no choice but to conclude that $f$ is continuous at $x$. Next, you demonstrate how to do this for any point $x\in X$, and reason that $f$ is continuous. The enemy's head promptly explodes.</p>
<p>Let's look at a specific example. Let $X$ be a metric space and consider the identity function $i:X\to X$ defined by $i(x)=x$ for every $x\in X$. The enemy first chooses a point $x_0\in X$ and draws a ball of radius $\epsilon$ around $x_0=i(x_0)$. You realize immediately that</p>
<p>$$\begin{align}<br>
i\big[B(x_0,\epsilon)\big] &amp;= B(x_0,\epsilon) \\<br>
&amp;= B\big(i(x_0),\epsilon\big)<br>
\end{align}$$</p>
<p>because $i$ is the identity function. So no matter what point $x_0$ or radius $\epsilon$ the enemy chooses, picking $\delta=\epsilon$ will always work. This argument shows that the identity function on any metric space is continuous!</p>
<hr>
<p>Hopefully the definition of continuity makes sense to you now, because I'm about move on to an equivalent and simpler definition.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ and $Y$ denote metric spaces with $f: X\to Y$. Then $f$ is continuous if and only if the preimage $f^{-1}[U]$ of any open set $U\subseteq Y$ is open in $X$.</p>
<p><strong>Proof.</strong> Suppose first that $f:X\to Y$ is continuous. If $f^{-1}[U]=\varnothing$ then we are done because this is open in $X$. Suppose then that $f^{-1}[U]\ne\varnothing$. Choose $x\in f^{-1}[U]$, so that $f(x)\in U$. Since $U$ is open, there exists a real number $\epsilon&gt;0$ such that $B\big(f(x),\epsilon\big)\subseteq U$. Since $f$ is continuous, there exists a real number $\delta&gt;0$ such that $f\big[B(x,\delta)\big]\subseteq B\big(f(x),\epsilon\big)$. That is, $B(x,\delta)\subseteq f^{-1}\big[B\big(f(x),\epsilon\big)\big]\subseteq f^{-1}[U]$. Thus, there is an open ball centered at $x$ which is contained in $f^{-1}[U]$, so clearly $f^{-1}[U]$ is open in $X$.</p>
<p>Suppose conversely that $f^{-1}[U]$ is open in $X$ whenever $U$ is open in $Y$. Choose $x\in X$ and a real number $\epsilon&gt;0$. Since $B\big(f(x),\epsilon\big)$ is open in $Y$, we have that $f^{-1}\big[B\big(f(x),\epsilon\big)\big]$ is open in $X$. Since $x\in f^{-1}\big[B\big(f(x),\epsilon\big)\big]$, there exists a real number $\delta&gt;0$ such that $B(x,\delta)\subseteq f^{-1}\big[B\big(f(x),\epsilon\big)\big]$. Thus, $f$ is continuous.</p>
</blockquote>
<p>This is most excellent. We've arrived at a &quot;distance-free&quot; definition of continuity, phrased entirely in terms of open sets. You know what that means! We're going to adopt this as the definition of continuity in a general topological setting.</p>
<blockquote>
<p><strong>Definition.</strong> Let $X$ and $Y$ denote topological spaces. A function $f:X\to Y$ is <strong>continuous</strong> if $f^{-1}[U]$ is open in $X$ whenever $U$ is open in $Y$.</p>
</blockquote>
<p>There is also a corresponding definition for continuity at a point, but we will never need it.</p>
<p>As is oft desired, we would like a way to determine whether a function between topological spaces is continuous, given only a basis for each space. This is done easily and naturally.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ and $Y$ denote topological spaces, let ${\cal B}_X$ be a basis for the topology on $X$ and ${\cal B}_Y$ a basis for the topology on $Y$. A function $f:X\to Y$ is continuous if and only if $f^{-1}[B]$ is open in $X$ whenever $B\in{\cal B}_Y$.</p>
<p><strong>Proof.</strong> Suppose first that $f$ is continuous. Since $B$ is a basis element, it is open in $Y$. It follows immediately that $f^{-1}[B]$ is open in $X$.</p>
<p>Suppose next that the preimage of any basis element in $Y$ is open in $X$. It has been established that for any open set  $U\subseteq Y$, there exists an indexing set $I$ and basis elements $B_i\in{\cal{B}}_Y$ such that  $U=\bigcup\limits_{i\in I}B_i$. Thus,</p>
<p>$$f^{-1}[U]=f^{-1}\left[\bigcup\limits_{i\in I}B_i\right]=\bigcup\limits_{i\in I}f^{-1}[B_i],$$</p>
<p>which is open since it is the union of open sets.</p>
</blockquote>
<p>Yay! Now let's use our beautiful definition of continuity to prove an elementary property of continuous maps: that the composition of two continuous functions</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X,Y$ and $Z$ denote topological spaces with continuous functions $f:X\to Y$ and $g:Y\to Z$. Then their composition $(g\circ f):X\to Z$, defined by $(g\circ f)(x)=g\big(f(x)\big)$ for every $x\in X$, is continuous.</p>
<p><strong>Proof.</strong> Suppose $U\subseteq Z$ is open. Then $g^{-1}[U]\subseteq Y$ is open, so $f^{-1}\big[g^{-1}[U]\big]\subseteq X$ is open. Thus, it suffices to show that $f^{-1}\big[g^{-1}[U]\big]=(g\circ f)^{-1}[U]$.</p>
<p>Suppose $x\in f^{-1}\big[g^{-1}[U]\big]$. Then $f(x)\in g^{-1}[U]$, so $g\big(f(x)\big)=(g\circ f)(x)\in U$. Thus, $x\in (g\circ f)^{-1}[U]$.</p>
<p>Suppose next that $x\in (g\circ f)^{-1}[U]$. Then $(g\circ f)(x)=g\big(f(x)\big)\in U$, so $f(x)\in g^{-1}[U]$ and thus $x\in f^{-1}\big[g^{-1}[U]\big]$.</p>
</blockquote>
<h3 id="homeomorphismsanamehomeomorphisms">Homeomorphisms<a name="homeomorphisms"></a></h3>
<p>Ages ago, when I first started talking about topological spaces, I mentioned the idea of topological equivalence. I believe I said something like &quot;two spaces are topologically equivalent if they can be continuously deformed into each other,&quot; and that &quot;continuous deformations are ways we can bend, stretch and move spaces without tearing, cutting or gluing them.&quot; We now have the machinery to formally define this notion.</p>
<blockquote>
<p><strong>Definition.</strong> Let $X$ and $Y$ denote topological spaces. A bijective function $f:X\to Y$ is a <strong>homeomorphism</strong> if both $f$ and $f^{-1}:X\to Y$ are continuous. We say that the spaces are <strong>homeomorphic</strong>.</p>
</blockquote>
<p>It is particularly important that $f$ is bijective, since otherwise $f^{-1}$ would not be well defined.</p>
<p>There is no consistent symbol for &quot;is homeomorphic to,&quot; although I've seen several used by different authors. For instance, $X\simeq Y$, $Y\approx Y$ or $X\sim Y$ could all be used. And I've seen them all used. I won't use any such notation, however, because I find it terribly confusing.</p>
<p>It should go without saying that if a function is a homeomorphism then so is its inverse. The next result is perhaps a tad less obvious, although its proof is almost trivial.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X, Y$ and $Z$ denote topological spaces with homeomorphisms $f:X\to Y$ and $g:Y\to Z$. Then $X$ is homeomorphic to $Z$ via $(g\circ f):X\to Z$.</p>
<p><strong>Proof.</strong> Since $f,g$ are bijective and continuous, $g\circ f$ is bijective and continuous. Furthermore, since $f^{-1},g^{-1}$ are bijective and continuous, $(g\circ f)^{-1}=f^{-1}\circ g^{-1}$ is bijective and continuous. Thus $g\circ f$ is a homeomorphism.</p>
</blockquote>
<p>Unfortunately, it is difficult to give simple examples of homeomorphic spaces without first discussing the constructions of new spaces, so more examples of homeomorphisms won't come for a few posts. We can, however, talk about what homeomorphism really means.</p>
<p>Given a homeomorphism $f$, we know that $f$ and its inverse $f^{-1}$ are continuous. This means that open sets get mapped to open sets in both directions. So essentially, homeomorphisms preserve open sets. Why is this nice? <strong>Because topological spaces are defined in terms of open sets.</strong> This indicates that homeomorphic spaces are really the same, just with their open sets renamed in some way.</p>
<p>There is a lot more to say about homeomorphisms. In fact, much of what topologists do is in an attempt to identify which spaces are homeomorphic.<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> And it isn't always obvious directly from the definition. All the tools I bring up from now on will be useful on this quest.</p>
<p>We will mainly be interested in things called <strong>topological invariants</strong>, which are characteristics of spaces that are preserved under homeomorphisms. If two spaces have different invariants, this tells us that these spaces are <em>not</em> homeomorphic. As we shall see, connectedness, compactness, countability, separation conditions, homology and homotopy groups are all topological invariants. These are all properties of the open sets of spaces, so this makes sense. Properties like boundedness and completeness, on the other hand, which rely on the notion of distance, are not topological invariants.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>If I haven't mentioned this before, &quot;map&quot; is just another word for &quot;function&quot; and the two are often used interchangeably. <a href="#fnref1" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
<li id="fn2" class="footnote-item"><p>I'm not sure why the arrows in my diagram are on sideways, but I'm too lazy to fix it. <a href="#fnref2" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Although we are often willing to settle for homotopy equivalence. <a href="#fnref3" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
</ol>
</section>
]]></content:encoded></item><item><title><![CDATA[Limit Points, Closure, Boundary and Interior]]></title><description><![CDATA[It's fairly common to think of open sets as sets which do not contain their boundary, and closed sets as sets which do contain their boundary. The trouble here lies in defining the word 'boundary.']]></description><link>http://localhost:2368/limit-points-closure-boundary-and-interior/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae21c</guid><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Sat, 08 Apr 2017 00:34:07 GMT</pubDate><content:encoded><![CDATA[<h3 id="contents">Contents</h3>
<ol>
<li><a href="#limit-points">Limit Points</a></li>
<li><a href="#closure">Closure</a></li>
<li><a href="#boundary">Boundary</a></li>
<li><a href="#interior">Interior</a></li>
</ol>
<hr>
<p>We are nearly ready to begin making some distinctions between different topological spaces. Distinguishing between fundamentally different spaces lies at the heart of the subject of topology, and it will occupy much of our time. However, before we can really dig in, we're going to need some additional tools.</p>
<p>On a different note, and I've been meaning to mention this, I'd be willing to bet that you probably had a different sense of what open and closed sets were before learning any topology. It's fairly common to think of open sets as sets which do not contain their boundary, and closed sets as sets which do contain their boundary. The trouble here lies in defining the word 'boundary.' This is finally about to be addressed, first in the context of metric spaces because it is easier to see why the definitions are natural there. As in prior posts, these concepts generalize easily to topological space.</p>
<h3 id="limitpointsanamelimitpoints">Limit Points<a name="limit-points"></a></h3>
<p>If you've had any calculus, you've probably had some experience with limits. Whether or not the formal $\epsilon$-$\delta$ definition of a limit is familiar to you is of no real consequence. Even though this definition is extremely insightful, it isn't really necessary for our purposes. In fact, if we aren't working in a metric space then this definition doesn't even apply. The good news it that many definitions in topology have a sort of <em>too-good-to-be-true</em> feel to them, since they're often deceptively simple.</p>
<p>Limit points are not the same type of limit that you encounter in a calculus or analysis class, but the underlying idea is similar. Informally, a point in a metric space is a limit point of some subset if it is arbitrarily close to other points in that subset. What exactly does this mean? It means that no matter how closely we zoom in on a limit point, there will always be another point in its immediate vicinity which belongs to the subset in question. Thus, we can in some sense approximate limit points to arbitrary accuracy using other points in the subset. This might be a little confusing or overwhelming, so I'll give you the definition now and you'll see that it isn't too bad. Its simplicity may even be a bit misleading.</p>
<blockquote>
<p><strong>Definition.</strong> Let $A$ denote a subset of a metric space $X$. A point $p\in X$ is a <strong>limit point</strong> of $A$ if every open ball centered at $p$ contains a point $x\in A$ with $x\neq p$. We write $L(A)$ to denote the set of limit points of $A$.</p>
</blockquote>
<p>The first thing that I will emphasize is that a limit point of a set does not need to belong to that set! All that is necessary is that there are points in the set <em>as close as we like</em> to the limit point. This can be made more obvious by rephrasing the definition slightly. By &quot;every open ball,&quot; what we mean is that for every real number $\epsilon&gt;0$ there exists some point $x\in A$ distinct from $p$ such that $x\in B(p,\epsilon)$. Take a moment to appreciate and understand that this definition ensures what I claim it does.</p>
<p>There's a sort of dual notion as well, which is called an isolated point. The following definition makes it clear that any point in a subset of a metric space is either a limit point or an isolated point.</p>
<blockquote>
<p><strong>Definition.</strong> Let $A$ denote a subset of a metric space $X$. A point $p\in X$ is an <strong>isolated point</strong> of $A$ if there exists an open ball centered at $p$ which contains no other points of $A$.</p>
</blockquote>
<p>Let's look at an example, to make sure we're fully on board with these concepts before we press onward. I'm going to cordon off the entire example so that we don't accidentally make sweeping generalizations about every metric space based on this one scenario.</p>
<blockquote>
<p><strong>Example.</strong> Take $X=\mathbb{R}^2$ equipped, of course, with the standard metric $d:X\times X\to\mathbb{R}$. Let $A=B\big((0,0),1\big)$, the open unit ball centered at the origin. Consider the following points in $X$:</p>
<p>$$\begin{align}<br>
x &amp;= (0,0), \\<br>
y &amp;= (1,0), \\<br>
z &amp;= (1,1). \\<br>
\end{align}$$</p>
<p>I'll even throw in a diagram to make things crystal clear:</p>
<p><img src="http://localhost:2368/content/images/2017/04/limit_point_example.svg" alt="limit point example"></p>
<p>Clearly $x\in A$ because $d(x,x)=0&lt; r$. However, $y,z\notin A$ because $d(x,y)=r$ and $d(x,z)=\sqrt{2}&gt;r$. Which, if any, of the points $x,y,z$ are limit points of $A$?</p>
<p>Let's consider $x$ first. Does every open ball centered at $x$ contain a point in $A$ distinct from $x$? The simple answer is &quot;well DUH,&quot; but let's be a tad more rigorous.</p>
<p>Choose a real number $\epsilon&gt;0$. We consider separately the cases where $\epsilon\geq 1$ and $0&lt;\epsilon&lt;1$. If $\epsilon\geq 1$ then $A\subseteq B(x,\epsilon)$ and we are done because there are certainly points in $A$ distinct from $x$. If $0&lt;\epsilon&lt;1$, it is clear that the point $x_0=(0,\epsilon/2)\in B(x,\epsilon)$. Furthermore, $x_0\neq x$ because $d(x,x_0)&gt;0$.</p>
<p>So $x$ is a limit point of $A$, which was sort of obvious from the start. A similar argument could be made that any point $a\in A$ is a limit point of $A$ by adding a few more technical details to the above argument, but I won't bother because it feels like overkill.</p>
<p>Next, let's take a look at $y$. You should have a sense that $y$ is a limit point of $A$, because even though it is not in $A$ it is <em>superextraclose</em> to lots and lots of points in $A$. Let's show that our intuition is correct.</p>
<p>Again, choose a real number $\epsilon&gt;0$ and consider the open ball $B(y,\epsilon)$. The point $y_0=(0,1-\epsilon/2)$ is clearly in $B(y,\epsilon)$ because $d(y,y_0)=\epsilon/2&lt;\epsilon$ and thus it is also distinct from $y$. Furthermore, $y_0\in A$ since $d(x,y_0)=1-\epsilon/2&lt;1$.</p>
<p>We've shown then that $y$ is a limit point of $A$, which is somewhat more interesting. A similar argument would show that any point $a\in X$ with $d(a,x)=1$ is a limit point of $A$. Sooo... even though we haven't defined the concept of &quot;boundary&quot; yet, it's looking like any point on what we intuitively perceive to be the boundary of $A$ is a limit point of $A$. This is an important realization.</p>
<p>Finally, let's show that $z$ is not a limit point of $A$. To show that the necessary property is not true for every open ball centered at $z$, all we need to do is demonstrate a single open ball centered at $z$ which contains no points of $A$. This is easy! Just take as the radius any positive $\epsilon&lt;\sqrt{2}-1$ and that's essentially all there is to it.</p>
<p>More formally, choose $\epsilon&lt;\sqrt{2}-1$ and consider $z_0\in B(z,\epsilon)$. By the definition of this open ball $d(z,z_0)&lt;\epsilon$. By the triangle inequality,</p>
<p>$$\begin{align}<br>
d(x,z) &amp;= \sqrt{2} \\<br>
&amp;\leq d(x,z_0) + d(z_0,z) \\<br>
&amp;&lt; d(x,z_0) + \epsilon \\<br>
&amp;&lt; d(x,z_0)+\sqrt{2}-1.<br>
\end{align}$$</p>
<p>Rearranging the above inequality, we have that $d(x,z_0)&gt;1$ and so $z_0\notin A$. Thus, $z$ is not a limit point of $A$. Once again, a similar argument would show that, in general, any point $p\in X$ with $d(p,x)&gt;1$ is not a limit point of $A$.</p>
<p>What can we conclude about this particular set $A$? The limit points of $A$ are every point in $A$ as well as every point on the unit circle $S^1$. That is, $L(A)=A\cup S^1=\overline{B}(x,r)$. This is the closed ball with the same center and radius as $A$.</p>
</blockquote>
<p>We shall see soon enough that this is no accident. For any subset $A$ of a metric space $X$, it happens that the set of limit points $L(A)$ is closed. Let's prove something even better.</p>
<blockquote>
<p><strong>Theorem.</strong> A subset of a metric space is closed if and only if it contains all of its limit points.</p>
<p><strong>Proof.</strong> We argue first that if $L(A)\subseteq A$ then $A$ is closed. It suffices to show that $X-A$ is open. Choose a point $x\in X-A$. Clearly $x$ is not a limit point of $A$ since $x\notin A$ and thus $x\notin L(A)\subseteq A$. Thus there exists some open ball $B$ centered at $x$ which does not contain any points in $A$. It follows that $B\subseteq X-A$ and so $X-A$ is open.</p>
<p>We argue next that if $A$ is closed then $L(A)\subseteq A$. Since $A$ is closed, we know that $X-A$ is open. Thus for any $x\in X-A$ there exists some open ball centered at $x$ which is strictly contained in $X-A$ and therefore contains no points of $A$. It follows that $x$ is not a limit point of $A$, so any limit points of $A$ are contained in $A$.</p>
</blockquote>
<p>Hooray! We now have an alternative, and often easier to use, definition of a closed set in a metric space! What's awesome is that nothing we've done (other than our example) depends on the metric, so we can immediately abstract everything to the setting of topological spaces!</p>
<blockquote>
<p><strong>Definition.</strong> Let $A$ denote a subset of a topological space $X$. A point $p\in X$ is a <strong>limit point</strong> of $A$ if every neighborhood of $p$ contains a point $x\in A$ with $x\neq p$.</p>
</blockquote>
<p>The theorem we just proved translates as well, if we simply replace open balls with neighborhoods each time the appear in the proof.</p>
<p>As always, there's nothing contradictory about the fact that the empty set is closed. Certainly the empty set contains all of its limit points, since it contains no points at all.</p>
<h3 id="closureanameclosure">Closure<a name="closure"></a></h3>
<p>We will now define the closure of a subset of a topological space. We will see later that taking the closure of a set is equivalent to include the set's boundary.</p>
<blockquote>
<p><strong>Definition.</strong> Let $A$ denote a subset of a topological space $X$. The <strong>closure</strong> of $A$ is the intersection of all closed set in $X$ which contain $A$. We denote the closure of a $A$ by $\overline A$.</p>
</blockquote>
<p>The closure of a set is always closed, because it is the intersection of closed sets. Furthermore, it is obvious that any closed set must equal its own closure. Intuitively, $\overline A$ is the smallest closed set which contains $A$. This is because, by definition, any closed set containing $A$ must also contain $\overline A$. Even more intuitively, the closure of $A$ is the union of $A$ with all of its limit points. Let's prove that this is true.</p>
<blockquote>
<p><strong>Theorem.</strong> If $A$ is a subset of a topological space, then $\overline A=A\cup L(A)$.</p>
<p><strong>Proof.</strong> We argue first that $A\cup L(A)\subseteq\overline A$. Clearly $A\subseteq\overline A$ since $\overline A$ is the intersection of all closed sets containing $A$, and thus itself contains $A$. It remains to show then that $L(A)\subseteq\overline A$, which we do by contraposition. Suppose $x\notin\overline A$ so that $x\in X-\overline A$. Since $\overline A$ is closed, $X-\overline A$ is open and is thus a neighborhood of $x$ which contains no points in $A$. Thus $x\notin L(A)$.</p>
<p>We argue next that $\overline A\subseteq A\cup L(A)$. If $x\in A$ then the proof is immediate. If $x\in\overline A -A$, then by definition $x$ is in every closed set containing $A$. It follows that for any open set $U\subseteq X-A$, we have $x\notin U$. Thus any neighborhood of $x$ intersects $A$ in some point $p$. Since $x\notin A$, clearly $x\neq p$ and thus $x$ is a limit point of $A$.</p>
</blockquote>
<p>It's a somewhat gross proof, but a nice and useful result, and one that we will use often.</p>
<h3 id="boundaryanameboundary">Boundary<a name="boundary"></a></h3>
<p>Next let's formalize the concept of boundary. It's easy and intuitive to think about the boundary of a ball, so let's start there. Let's say we have an open and closed ball of the same center and radius in some metric space. They obviously have the same boundary â€” the circle with the same radius as these balls. Even though these points don't belong to the open ball, they are <em>just</em> touching its outer edge.</p>
<p>Now what about this set, as a subset of $\mathbb{R}^2$ in the standard metric?</p>
<p><img src="http://localhost:2368/content/images/2017/04/vomitous_mass.svg" alt="vomitous mass"></p>
<p>Trying to calculate the boundary of this set is a bit more difficult than just drawing a circle. Does that loop at the top right count as boundary? What about the points sitting by themselves? Do those inner circles count as well, or does the boundary have to enclose the set? It turns out that, the way we define boundary, the answer to all of these questions is yes.</p>
<blockquote>
<p><strong>Definition.</strong> Let $A$ be a subset of a metric space $X$. A point $p\in X$ is a <strong>boundary point</strong> of $A$ if every open ball centered at $p$ contains at least one point in $A$ and one point in $X-A$.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> The <strong>boundary</strong> of $A$ is the set of all boundary points of $A$. We denote it by $\partial A$.</p>
</blockquote>
<p>This makes a lot of sense! No matter how tiny an open ball we choose around a boundary point, it will always intersect both $A$ and its complement. That is, it will always contain points that are in $A$ and points that are not in $A$.</p>
<p>These definitions are identical in a topological space if we again replace open balls with neighborhoods. I won't even bother restating them.</p>
<p>It turns out we could already have defined the boundary of a set without the notion of boundary points!</p>
<blockquote>
<p><strong>Theorem.</strong> If $A$ is a subset of a topological space $X$, then $\partial A=\overline A\cap\overline{X-A}$.</p>
<p><strong>Proof.</strong> Choose $x\in\partial A$. By definition, every neighborhood of $x$ contains a point in $A$ and a point in $X-A$, so $x\in\overline A$ and $x\in\overline{X-A}$. Thus $x\in\overline A\cap\overline{X-A}$ and so $\partial A\subseteq \overline A\cap\overline{X-A}$.</p>
<p>The proof that $\overline A\cap\overline{X-A}\subseteq\partial A$ is precisely the same as above, with the steps reversed.</p>
</blockquote>
<p>This means that the boundary of $A$ is the intersection of the smallest closed set containing $A$ and the smallest closed set containing its complement, which hopefully seems reasonable to you.</p>
<h3 id="interioranameinterior">Interior<a name="interior"></a></h3>
<p>I'm not going to bother with metric spaces for this part, since I think you get the idea. You can easily translate all the results about topological spaces back into metric spaces if you'd like. Intuitively, we can think of the interior of a set as everything in the set which does not belong to its boundary. This is actually not the definition we'll initially give, although we shall soon see that they are equivalent.</p>
<blockquote>
<p><strong>Definition.</strong> Let $A$ denote a subset of a topological space $X$. The <strong>interior</strong> of $A$ is the union of all open subsets of $A$. We write $\mathring A$ to denote the interior of $A$.</p>
</blockquote>
<p>Clearly the interior of a set is always open because it is the union of open sets. We can think of the interior of a set as the largest open set contained in that set. Clearly every point in $\mathring A$ has a neighborhood contained in $A$. We call each such point an <strong>interior point</strong> of $A$.</p>
<p>Now that we have defined the interior and closure of any set $A$, we have a sort of set sandwich $\mathring A\subseteq A\subseteq\overline A$. Here, $\mathring A$ contains none of its boundary points, $\overline A$ contains them all, and $A$ can contain some, all or none.</p>
<p>Let's finish up with the proof I promised you a minute ago.</p>
<blockquote>
<p><strong>Theorem.</strong> If $A$ is a subset of a topological space $X$, then $\mathring A=\overline A-\partial A$.</p>
<p><strong>Proof.</strong> Choose $x\in\mathring A$. Clearly $x\in\overline A$ because $\mathring A\subseteq\overline A$. Since $x$ is an interior point of $A$, there exists some neighborhood of $x$ which is contained in $A$. Thus $x$ is not a boundary point of $A$ and so $x\in\overline A-\partial A$.</p>
<p>Next, choose $x\in\overline A-\partial A$. Then $x\in A$ or $x$ is a limit point of $A$, but $x$ is not a boundary point of $A$. Thus any neighborhood of $x$ contains a point in $A$ but no points in $X-A$, so $x\in\mathring A$.</p>
</blockquote>
<p>I'm probably boring you. I'm boring myself, actually, because I don't find any of this particularly interest. It's kind of necessary though, so I'm glad I went through it. I'm going to end the post here, and next time I'll finally talk about continuity, which is actually interesting and incredibly important.</p>
]]></content:encoded></item><item><title><![CDATA[Groups and their Basic Properties]]></title><description><![CDATA[Essentially, a group is a set endowed with a very basic structure. This structure is enforced by an operation which governs how the elements in the group interact with each other.]]></description><link>http://localhost:2368/groups-and-their-basic-properties/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae21b</guid><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Fri, 07 Apr 2017 23:05:25 GMT</pubDate><content:encoded><![CDATA[<h3 id="contents">Contents</h3>
<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#the-definition">The Definition</a></li>
<li><a href="#examples">Examples</a></li>
<li><a href="#basic-properties">Basic Properties</a></li>
<li><a href="#subgroups">Subgroups</a></li>
</ol>
<hr>
<h3 id="introductionanameintroduction">Introduction<a name="introduction"></a></h3>
<p>I think it's probably time I wrote a post about something other than topology, so today I'm going to break way into the field of algebra.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> It's difficult to explain exactly what algebra is until you've been exposed to some of it, but I like to think of algebra as the study of structure. Using algebraic techniques, seemingly different objects can be shown to be not so different after all, and many difficult problems can be solved relatively easy using such observations.</p>
<p>The first algebraic object I'd like to introduce, as you may have guessed, is a group. This will not be my only post on groups, since there is a lot to say about them. Essentially, a group is a set endowed with a very basic structure. This structure is enforced by an operation which governs how the elements in the group interact with each other. Before I define a group, I need to talk a little bit about binary operations.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>binary operation</strong> $\circ$ on a set $X$ is a function $\circ:X\times X\to X$.</p>
</blockquote>
<p>The definition nicely captures several important ideas. First, it guarantees that if $a,b\in X$ then $\circ(a,b)\in X$ as well. This is called <strong>closure</strong> under the binary operation â€” when we combine two elements in $X$, we always get back something which is also in $X$. Next, because we have defined $\circ$ as a function, it is guaranteed that every pair of elements in $X$ can be combined to yield a new element.</p>
<p>You're probably more used to <strong>infix notation</strong> $a\circ b$, rather than the equivalent function notation $\circ(a,b)$, when it comes to denoting binary operations. That's good, since infix notation is more convenient and we will use it more often.</p>
<p>As an example, the usual operations of addition and multiplication on $\mathbb{N},\mathbb{Z},\mathbb{Q},\mathbb{R}$ and $\mathbb{C}$ are binary operations on each of these sets. Let $X$ denote any of the aforementioned sets. That addition and multiplication on $X$ actually constitute binary operations is shown easily by noting that for any two elements $a,b\in X$, their sum $a+b$ and their product $a\cdot b$ are well defined and are also in $X$.</p>
<h3 id="thedefinitionanamethedefinition">The Definition<a name="the-definition"></a></h3>
<p>I'll jump straight into it, shall I?</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>group</strong> is a set $G$ together with a binary operation $\circ:G\times G\to G$ with the following properties:</p>
<ol>
<li><strong>Associativity.</strong> For any $a,b,c\in G$, we have that $(a\circ b)\circ c=a\circ (b\circ c)$.</li>
<li><strong>Identity.</strong> There exists $e\in G$ such that $e\circ x=x\circ e=x$.</li>
<li><strong>Inverses.</strong> For every $x\in G$ there exists $y\in G$ such that $x\circ y=e$.</li>
</ol>
</blockquote>
<p>Stated in plain English, groups must have an identity element and inverses for every element, and the group operation must be associative. These all seem like reasonable requirements.</p>
<p>In the future, we will occasionally be even more lax and denote $a\circ b$ as simply $ab$, when a group's binary operation is implicitly understood and no confusion can arise. When an element is 'multiplied' by itself numerous times, we will use exponential notation. For instance, $aa=a^2$ and $aaa=a^3$. It is consistent and convenient in this notation to denote the identity element $e=a^0$ for any element $a$ in a group.</p>
<p>Furthermore, I will frequently refer to $G$ itself as a group, as this very rarely results in any confusion. Just remember that whenever I mention a group, there is always some binary operation lurking behind the curtain.</p>
<h3 id="examplesanameexamples">Examples<a name="examples"></a></h3>
<p>Now that groups have been defined, I'd like to walk you through a few simple examples of groups.</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>trivial group</strong> is the group containing only one element.</p>
</blockquote>
<p>Technically there are infinitely many 'trivial groups' since that one element could be anything, but all trivial groups are really the same. Let's call the element $e$. To see that the trivial group is in fact a group, first note that $e$ must be the group's identity element. Thus, $(ee)e=ee=e(ee)$ and so we have associativity. Furthermore, since $ee=ee=e$, clearly $e$ is its own inverse.</p>
<p>The set of integers under addition also forms a group. Associativity is a property of addition itself, and you probably use it every day without even thinking about it. The identity element is zero, since $x+0=0+x=x$ for any $x\in\mathbb{Z}$. Lastly, the inverse of any integer $x$ is $-x$, since $x+(-x)=0$.</p>
<p>Similarly, the set $\mathbb{R}^+$ of positive real numbers under multiplication forms a group. Again, associativity is obvious.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> The identity element is $1$ since $1\cdot x=x\cdot 1=x$ for any $x\in\mathbb{R}^+$. Lastly, the inverse of any $x\in\mathbb{R}^+$ is $\frac{1}{x}$. Notice that the set of all real numbers does not form a group under multiplication because zero has no multiplicative inverse!</p>
<p>The last example I'd like to talk about is considerably more abstract, and probably not something you would ever have considered might be a group. It is called the <strong>dihedral group</strong> (or <strong>group of symmetries</strong>) of a regular polygon. We write $D_n$ to denote the dihedral group of the $n$-sided regular polygon. How is $D_n$ defined? I'm not quite ready to do it rigorously, since we haven't talked about permutations yet, but I'll explain it intuitively.</p>
<p>Basically, $D_n$ is the set of all rotations and reflections which preserve the locations of the vertices. The group operation is composition of these rotations and reflections, which essentially amounts to performing them one after the other. For instance, consider the equilateral triangle (the $3$-sided regular polygon)</p>
<p><img src="http://localhost:2368/content/images/2017/04/equilateral-triangle-1.svg" alt="equilateral triangle"></p>
<p>where I've numbered the vertices to avoid the tremendous confusion that would otherwise ensue. Here's one of the vertex location preserving rotations we can perform, $r_1$:</p>
<p><img src="http://localhost:2368/content/images/2017/04/r_1.svg" alt="r_1"></p>
<p>Here's another one, $r_2$:</p>
<p><img src="http://localhost:2368/content/images/2017/04/r_2-1.svg" alt="r_2"></p>
<p>It's not too difficult to see that $r_2$ is really just $r_1$ applied twice! That is, $r_2=r_1\circ r_1$. Remember that this is the notation of function composition, so the first rotation applied is written on the right.</p>
<p>Now that we have a better grasp of what's going on, let's think about the identity element of $D_3$. It should hopefully make sense that the identity element is the rotation by zero degrees. That is, the identity element is the act of doing nothing to the triangle. Call this element $e$. It should be clear that $e\circ r_1=r_1\circ e=r_1$ and likewise for $r_2$, so things are looking good so far.</p>
<p>What are the inverses of $r_1$ and $r_2$? We just need to figure out how to get the vertices back to their original positions. It looks like $r_1\circ r_2=r_2\circ r_1=e$, so $r_1$ and $r_2$ are actually inverses!</p>
<p>We've exhausted all the rotations, so now we need to look at the reflections. There are three of them, and each preserves the location of one vertex while swapping the other two. Let's call them $f_1, f_2$ and $f_3$, where the subscript denotes the vertex preserved under each. For instance, here's $f_1$:</p>
<p><img src="http://localhost:2368/content/images/2017/04/f_1.svg" alt="f_1"></p>
<p>What is the inverse of each reflection? If you reflect something twice along the same axis, it goes back to its original position. That is, $f_1, f_2$ and $f_3$ are each their own inverse.</p>
<p>It's important to remember that the elements of the group are these reflections and rotations, rather than the triangles they act upon. I'll make all this more precise later when I talk about permutations, but for now I think this visual explanation should suffice.</p>
<p>Next, $D_4$ is the dihedral group of the square, with four distinct rotations (where I have counted the identity among the rotations) and four reflections.</p>
<p>Likewise, $D_5$ is the dihedral group of the regular pentagon, with five rotations and five reflections. In general, $D_n$ consists of $n$ rotations and $n$ reflections, totaling $2n$ elements.</p>
<p>I'll talk about dihedral groups again in the future, but for now let me conclude by noting that the rotations in $D_n$ form a group of their own, since they cannot be composed in such a way as to produce anything other than a rotation. The reflections are a different story, though. They do not form a group since, for example, in $D_3$ we have that $f_2\circ f_1=r_2$. That is, the composition of two reflections can be a rotation.</p>
<h3 id="basicpropertiesanamebasicproperties">Basic Properties<a name="basic-properties"></a></h3>
<p>Now that I've gotten a few examples of groups out of the way, I'd like to talk about some immediate consequences of the group axioms. Let's begin by showing that we are allowed to cancel terms from equations, as we are so used to doing in the familiar number systems.</p>
<blockquote>
<p><strong>Left Cancellation Law.</strong> Let $G$ denote a group with $a,b,x\in G$. If $xa=xb$, then $a=b$.</p>
<p><strong>Proof.</strong> Since $x\in G$, there must exist an inverse $y\in G$ for $x$ and an identity element $e\in G$ such that $yx=e$. Thus,</p>
<p>$$\begin{align}<br>
a &amp;= e a &amp; \scriptstyle\textit{identity}\\<br>
&amp;=(y x) a &amp; \scriptstyle\textit{inverses}\\<br>
&amp;=y(x a) &amp; \scriptstyle\textit{associativity}\\<br>
&amp;=y(x b) &amp;\scriptstyle{x a=x b}\\<br>
&amp;=(y x) b &amp;\scriptstyle\textit{associativity}\\<br>
&amp;=e b &amp;\scriptstyle\textit{inverses}\\<br>
&amp;=b. &amp;\scriptstyle{\textit{identity}}<br>
\end{align}$$</p>
<p>This completes the proof.</p>
</blockquote>
<p>I have included my reasoning for each step to the right, because such proofs can be difficult to parse when you first encounter them. Do not expect me to continue being this nice in the future. The <strong>right cancellation law</strong> and its proof are completely symmetric, so I will not even bother to state them. Now that these cancellation laws have been established, we will be using them frequently. For instance, let's use one to prove the following proposition about inverses.</p>
<blockquote>
<p><strong>Theorem.</strong> Each element in a group has a unique inverse.</p>
<p><strong>Proof.</strong> Let $G$ denote a group with $x\in G$ and suppose that $y_1, y_2\in G$ are both inverses for $x$. Then $y_1x=y_2x=e$ by the definition of inverses, so by the right cancellation law it follows that $y_1=y_2$, completing the proof.</p>
</blockquote>
<p>How does this proof establish that each element's inverse is unique? We are guaranteed the existence of at least one inverse by the group axioms. Furthermore, we just demonstrated that if an element has two inverses, then they must really be the same element!</p>
<p>Next, let's prove a similar statement that there is only one identity element in any group. This is probably the simplest proof ever, but it's rather informative.</p>
<blockquote>
<p><strong>Theorem.</strong> The identity element in a group is unique.</p>
<p><strong>Proof.</strong> Let $G$ denote a group and suppose that $e_1, e_2\in G$ are both identity elements. Then $e_1=e_1e_2=e_2$ by the definition of the identity, completing the proof.</p>
</blockquote>
<p>I've already been saying 'the inverse' of an element and 'the identity' a lot prior to this, but now I'm actually justified in doing so. Furthermore, since each element $x$ only has one inverse, we can denote it unambiguously as $x^{-1}$. This plays well with the exponential notation I introduced earlier, i.e. $(x^{-1})^n=x^{-n}$ and $x^mx^{-m}=e$.</p>
<p>Before moving on I'd like to mention a very nice property that certain groups may exhibit, but they do not necessarily have to.</p>
<blockquote>
<p><strong>Definition.</strong> A group $G$ is <strong>abelian</strong> (or <strong>commutative</strong>) if $xy=yx$ for every $x,y\in G$.</p>
</blockquote>
<p>Examples of abelian groups that we've already seen are the integers under addition and the positive real numbers under multiplication. On the other hand, the dihedral groups of order three and above are nonabelian.</p>
<h3 id="subgroupsanamesubgroups">Subgroups<a name="subgroups"></a></h3>
<p>I'm almost done now, and I know this has been a pretty long post. I just need to introduce one more important concept and then I promise I'll stop.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>subgroup</strong> $H$ of a group $G$ is a subset of $G$ which is itself a group under the group operation on $G$.</p>
</blockquote>
<p>If $H$ is a subgroup of $G$, I will sometimes write $H\leq G$. This cannot really be confused with the &quot;less than or equal to&quot; relation because groups are not numbers and thus have no such concept.</p>
<p>As an example, the even integers (denoted $2\mathbb{Z}$) are a subgroup of $\mathbb{Z}$ under addition. This is because the sum of two even integers is always even. However, the odd integers do not form a subgroup of $\mathbb{Z}$ under addition because the sum of two odd integers is not odd.<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></p>
<p>I pointed out earlier that the rotations in $D_3$ themselves formed a group under function composition, and this means that they are a subgroup of $D_3$.</p>
<p>In general, a group does not necessarily have any subgroups other than itself and the trivial group. We shall see later that if a subgroup (of a finite group) is to exist, it must contain a very predictable number of elements.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Although my true motivation is perhaps more sinister than you could possibly imagine. ðŸ˜ˆ <a href="#fnref1" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
<li id="fn2" class="footnote-item"><p>It's really not. When I inevitably define the real numbers from scratch as equivalence classes of Cauchy sequences of rational numbers in a future post, showing associativity will actually be something of a chore. As will everything else. <a href="#fnref2" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
<li id="fn3" class="footnote-item"><p>The odd integers form what is called a coset, $2\mathbb{Z}+1$ of the subgroup $2\mathbb{Z}$, but I will talk about this in a later post. <a href="#fnref3" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
</ol>
</section>
]]></content:encoded></item><item><title><![CDATA[Bases for Topologies]]></title><description><![CDATA[Essentially, a basis is a 'small' collection of open sets from which every open set can be easily generated. It is often useful to talk about the topology generated by a specific basis, since many facts about a topology can be gleaned by studying one of its bases.]]></description><link>http://localhost:2368/bases-for-topologies/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae21a</guid><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Mon, 03 Apr 2017 04:07:41 GMT</pubDate><content:encoded><![CDATA[<h3 id="contents">Contents</h3>
<ol>
<li><a href="#recap">Recap</a></li>
<li><a href="#bases">Bases</a></li>
</ol>
<hr>
<h3 id="recapanamerecap">Recap<a name="recap"></a></h3>
<p>It occurred to me after I finished my last post on topological spaces that there is actually quite a lot I can already tell you about them. First I'm going to show a sort of stronger correspondence between the ideas of open sets in metric and topological spaces. Then I'm going to talk about bases for topologies. I believe I've mentioned these before, but now I'm finally ready to define them.</p>
<p>Recall that in a metric space, a set $U$ is open if every point $x\in U$ is at the center of some open ball which is itself a subset of $U$.</p>
<p>Recall also that in a topological space, the open sets are required to include the empty set and the space itself, the union of any number of open sets, and the intersection of any finite number of open sets.</p>
<p>Let's see if we can get the definition of open sets in topological spaces to look a little bit more like the definition in metric spaces. First, we'll need the following lemma. It's a bit obvious, but useful enough to have a name.</p>
<blockquote>
<p><strong>Union Lemma.</strong> Suppose $X$ is a set and $\cal T$ is a collection of subsets of $X$. If every $x\in X$ is contained in some set $U_x\in\cal T$ then $X=\bigcup\limits_{x\in X}U_x$.</p>
<p><strong>Proof.</strong> To show that these two sets are equal, it suffices to show that each is a subset of the other.</p>
<p>Since every set $U_x$ is a subset of $X$, it must be that their union, $\bigcup\limits_{x\in X}U_x$ is a subset of $X$.</p>
<p>Next, suppose $y\in X$. Then by hypothesis there exists $U_y\in\cal T$ for which $y\in U_y$. Thus, $y\in U_y\subseteq\bigcup\limits_{x\in X}U_x$. It follows that $X$ is a subset of $\bigcup\limits_{x\in X}U_x$, completing the proof.</p>
</blockquote>
<p>Just from the letters I chose to represent the sets in the statement of this lemma, it should be somewhat obvious where I'm going with this. Think of $X$ as a topological space, $\cal T$ as a topology on $X$, and each $U_x\in\cal T$ as a neighborhood of each point $x\in X$. (Recall that a neighborhood of a point is any open set containing that point, and that I've likened them to open balls in the past.)</p>
<p>Now let's do what I promised earlier<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</p>
<blockquote>
<p><strong>Theorem.</strong> A subset $V$ of a topological space $X$ is open if and only if every point $x\in V$ has a neighborhood $U_x$ such that $U_x\subseteq V$.</p>
<p><strong>Proof.</strong> Suppose first that $V$ is open. Then the set $V$ itself is a neighborhood of every $x\in V$, so choosing each $U_x=V$ will suffice.</p>
<p>Suppose next that every point $x\in V$ has a neighborhood $U_x$ such that $U_x\subseteq V$. Then by the union lemma, $v=\bigcup\limits_{x\in V}U_x$, which is open because it is the union of open sets.</p>
</blockquote>
<p>That looks a lot like the definition of openness in a metric space, doesn't it? This analogous definition of open sets in topological spaces says that a set $V$ is open if every point in the set is contained in an open set which is itself a subset of $V$.</p>
<p>Since metric spaces <em>are</em> topological spaces (when equipped with the proper topology, of course (the one that's induced by the metric (I wonder how many nested parenthetical statements I can get away with?))) we can use this new concept of openness in metric spaces too! That means we are no longer restricted to open balls centered at a point.</p>
<h3 id="basesanamebases">Bases<a name="bases"></a></h3>
<p>Now I'm going to talk about the idea of a basis for a topological space. Essentially, a basis is a 'small' collection of open sets from which every open set can be easily generated. It is often useful to talk about the topology generated by a specific basis, since many facts about a topology can be gleaned by studying one of its bases.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>basis</strong> $\cal B$ for a topology on a set $X$ is a collection of open sets (called <strong>basis elements</strong> with the following two properties:</p>
<ol>
<li>Every point in $X$ is contained in some basis element $B\in\cal B$.</li>
<li>If two basis elements $B_1, B_2\in\cal B$ are not disjoint, then for each $x\in B_1\cap B_2$ there is another basis element $B_x\subseteq B_1\cap B_2$ for which $x\in B_x$.</li>
</ol>
</blockquote>
<p>The first condition is self-evident. The second condition just means that if two basis elements intersect, then for every point in their intersection there is another basis element containing that point which is itself contained in their intersection.</p>
<p>It isn't too hard to see how such a collection generates a topology:</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>topology generated by the basis</strong> $\cal B$ is defined as follows:</p>
<ol>
<li>The empty set is, of course, open.</li>
<li>The union of any collection of sets in $\cal B$ is open.</li>
</ol>
</blockquote>
<p>We need to verify that the topology generated by a basis is, in fact, a topology. Otherwise we'd be in a world of trouble, and we'd have some nerve calling it that. To do so, we're going to need another lemma:</p>
<blockquote>
<p><strong>Basis Intersection Lemma.</strong> Let $\cal B$ be a basis for a topology on $X$, let $B_1, B_2, \dotsc, B_n\in\cal B$ and suppose that $x\in\bigcap\limits_{i=1}^n B_i$. Then there also exists $B_x\in\cal B$ such that $x\in B_x\subseteq\bigcap\limits_{i=1}^n B_i$.</p>
<p><strong>Proof.</strong> We proceed by induction on $n$. The base case, $n=2$, holds by the definition of a basis.</p>
<p>Suppose then that the lemma holds for $n-1$, where $n&gt;2$. That is, if $B_1, B_2, \dotsc, B_n\in\cal B$ then there exists $B_x\in\cal B$ such that $x\in B_x\subseteq\bigcap\limits_{i=1}^{n-1}B_i$. Suppose that $x\in\bigcap\limits_{i=1}^n=\bigcap\limits_{i=1}^{n-1}\cap B_n$. Then $x\in B_x$ and $x\in B_n$, so from the definition of a basis it it follows that there exists $B'_x\in\cal B$ for which $x\in B'_x\subseteq\bigcap\limits_{i=1}^n$, as desired. This completes the proof.</p>
</blockquote>
<p>We now have all the tools we need to prove that a basis actually generates a topology!</p>
<blockquote>
<p><strong>Theorem.</strong> Given a set $X$, the topology generated by a basis $\cal B$ is a topology on $X$.</p>
<p><strong>Proof.</strong> First notice that the empty set is open by definition, and the set $X$ is open by the union lemma since every point in $X$ is required to be in some basis element.</p>
<p>Next, let $I$ be an indexing set such that $A_i\subseteq X$ is open for each $i\in I$. Define $U=\bigcup\limits_{i\in I}A_i$. Each set $A_i$ is either empty or the union of some collection of basis elements, so it is open. Thus, $U$ is open since it is the union of open sets.</p>
<p>Finally, let $A_1, A_2, \dotsc, A_n$ be open sets for some $n\in\mathbb{N}$ and define $I=\bigcap\limits_{i=1}^n A_i$. If any $A_i$ is empty then so is $I$, and thus it is open. Suppose then that $I$ is nonempty, and let $x\in I$ so that $x\in A_i$ for each $i$. Since each $A_i$ is the union of basis elements, there exists $B_i\in\cal B$ such that $x\in B_i\subseteq A_i$ for each $i$. Thus, $x\in\bigcap\limits_{i=1}^n B_i$, so by the basis intersection lemma there exists $B_x\in\cal B$ such that $x\in B_x\subseteq\bigcap\limits_{i=1}^n B_i\subseteq V$. By the union lemma, $V=\bigcup\limits_{x\in I} B_x$. This is open since it is the union of basis elements, completing the proof.</p>
</blockquote>
<p>Woah. That proof was long and boring. What was even the point of all that again? Well, now we can be verify that a collection of subsets is a basis for a topology and immediately talk about the topology it generates without having to prove that it really is a topology each and every time. Looking at a space in terms of a basis for that space can also simplify our talk of open sets quite a bit, as in the following theorem.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $X$ denote a topological space generated by a basis $\cal B$. A set $U\subseteq X$ is open if and only if for each $x\in U$ there is some basis element $B_x\in\cal B$ for which $x\in B_x\subseteq U$.</p>
<p><strong>Proof.</strong> Suppose first that $U\subseteq X$ is open. Then $U$ is the union of some collection of basis elements, so for each $x\in U$ there exists at least one $B_x\in\cal B$ for which $x\in B_x\subseteq U$.</p>
<p>Suppose next that for each $x\in U$ there is some $B_x\in\cal B$ for which $x\in B_x\subseteq U$. Then by the union lemma we have that $U=\bigcup\limits_{x\in U} B_x$, which is open since it is the union of basis elements.</p>
</blockquote>
<p>This is the ultimate relationship between open sets in metric and topological spaces. In fact, if we take open balls in the metric as the basis elements of a topology, we have precisely the same definition for each. Let's actually make sure that the open balls in a metric space form a basis.</p>
<blockquote>
<p><strong>Theorem.</strong> In a metric space $X$ with metric $d:X\times X\to\mathbb{R}$, the set of open balls $\{B(x,r)\subseteq X\mid x\in X,r&gt;0\}$ is a basis for a topology on $X$.</p>
<p><strong>Proof.</strong> The first condition for a basis is clearly satisfied, since for any $x\in X$ we have that $x\in B(x,r)$ for any $r&gt;0$. For the next part, here's a diagram to help you visualize my argument, because I know how much you love my diagrams:</p>
<p><img src="http://localhost:2368/content/images/2017/04/basis-intersection-balls-1.svg" alt=""></p>
<p>I've practically already proved that second condition already in my first post on metric spaces. Suppose that $a,b\in X$ and $B(a,r_a)\cap B(b,r_b)\neq\varnothing$ for some real numbers $r_a, r_b &gt;0$. If $x\in B(a,r_a)\cap B(b,r_b)$ then $x\in B(a,r_a)$ and $x\in B(b,r_b)$ by the definition of set intersection. We already know that there exist real numbers $r_1, r_2&gt;0$ for which $B(x,r_1)\subseteq B(a,r_a)$ and $B(x,r_2)\subseteq B(b,r_b)$ Simply choosing the smaller of these open balls, $B(x,r)$ with $r=\min\{r_1,r_2\}$, yields a basis element containing $x$ which is contained entirely in the intersection.</p>
</blockquote>
<p>It's important to realize that this is a proof that the open balls in <em>any</em> metric space are the basis for a topology on that space. We call this topology the <strong>topology induced by a metric</strong>. It is clear that this topology on $\mathbb{R}^n$ is the standard topology, and any open set in $\mathbb{R}^n$ is thus the union of some collection of open balls. I talked about this a few posts ago, but now I'm actually justified in claiming it.</p>
<p>Can you show that in $\mathbb{R}^2$, the set of open rectangles of the form</p>
<p>$$\{(x,y)\in\mathbb{R}^2\mid x\in (a,b), y\in (c,d)\},$$</p>
<p>where $a,b,c,d\in\mathbb{R}$, is the basis for a topology on $\mathbb{R}^2$? It's really not terribly difficult, and drawing a picture of the situation is practically a proof in itself. It can be shown that this basis also generates the standard topology on $\mathbb{R}^2$, although this is a bit more work.</p>
<p>In closing, several metrics can induce the same topology on a space. However, not every topology is induced by some metric! We say that a topology on $X$ is <strong>metrizable</strong> if there exists a metric on $X$ which induces that topology. I may talk about this in the future, but I don't personally find this topic too interesting.</p>
<p>In my next post, I'm going to do what I promised to do in my last post. That is, I'll introduce some more important definitions in metric spaces, and hopefully it won't be as long and boring as this post was.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>I don't believe I've done an 'if and only if' type proof on here before. The basic idea is that '$A$ if and only if $B$' is the same as saying '$A$ implies $B$' and '$B$ implies $A$. That is, all we have to do in order to prove such a statement is show that each half implies the other half. <a href="#fnref1" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
</ol>
</section>
]]></content:encoded></item><item><title><![CDATA[Mathematical Induction]]></title><description><![CDATA[Before my next post on bases for topologies, I need to introduce a proof technique that I haven't used so far.]]></description><link>http://localhost:2368/mathematical-induction/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae219</guid><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Sun, 02 Apr 2017 22:53:45 GMT</pubDate><content:encoded><![CDATA[<h3 id="contents">Contents</h3>
<ol>
<li><a href="#the-axiom-of-choice-and-well-ordering-principle">The axiom of choice and well-ordering principle</a></li>
<li><a href="#the-principle-of-mathematical-induction">The principle of mathematical induction</a></li>
<li><a href="#examples">Examples</a></li>
</ol>
<hr>
<p>This post is a much needed break from my steady stream of posts concerning topology. Before my next post on bases for topologies, I need to introduce a proof technique that I haven't used so far.</p>
<p>In mathematics, we come across patterns all the time. For instance, here's a pattern you may have noticed before:</p>
<p>$$\begin{align}<br>
1       = 1\phantom{0}  &amp;= 1^2 \\<br>
1+3     = 4\phantom{0}  &amp;= 2^2 \\<br>
1+3+5   = 9\phantom{0}  &amp;= 3^2 \\<br>
1+3+5+7 = 16 &amp;= 4^2 \\<br>
&amp;\phantom{0}\vdots \\<br>
1+3+5+7+\dotsc +(2n+1) &amp;= n^2<br>
\end{align}$$</p>
<p>It turns out that for any natural number $n$, if you add the first $n$ odd numbers the result is always $n^2$. (Remember that the $n$th odd number is $2n-1$.) Here's a visual depiction of the pattern:</p>
<p><img src="http://localhost:2368/content/images/2017/04/sum_of_odds-2.svg" alt=""></p>
<p>From this image, it is evident that adding together the first $n$ odd numbers literally results in a square of side length $n$. But how can we come up with a convincing argument that this pattern always holds? In answering this question, we will discuss a method that can also be applied to many other similar problems.</p>
<h3 id="theaxiomofchoiceandwellorderingprincipleanametheaxiomofchoiceandwellorderingprinciple">The axiom of choice and well-ordering principle<a name="the-axiom-of-choice-and-well-ordering-principle"></a></h3>
<p>Before we start, I should mention that you cannot do a proof by induction unless you accept the <strong>axiom of choice</strong>. The axiom of choice is an extra axiom of set theory which is somewhat controversial (among mathematicians). This is because at first glance it seems obviously true, but it actually leads to some bizarre and unexpected results.</p>
<blockquote>
<p><strong>Axiom of Choice.</strong> Suppose $I$ is an indexing set and $C$ is a collection of nonempty sets $X_i$ for each $i\in I$. Then there exists a function $f:C\to\bigcup\limits_{i\in I} X_i$ such that $f(X_i)\in X_i$ for every $i$.</p>
</blockquote>
<p>In simpler words, the axiom of choice ensures that, given any number of nonempty sets, it is possible to choose precisely one element from each set. This is probably something you never would have dreamt was controversial, and it certainly seems like a natural assumption, but as I said before it allows for some strange results.</p>
<p>For instance, it leads to the <strong>Banach-Tarski Paradox</strong>, that is, the fact that given a solid ball in three dimensions ($\mathbb{R}^3$), it is possible to break the ball up into a finite number of disjoint pieces and rearrange these pieces using rigid transformations into two solid balls which are each identical to the original. This is perhaps at least slightly suspicious.</p>
<p>I'm always going to assume the axiom of choice, mostly because I like it and it makes a lot of proofs considerably easier. Sometimes it is the only thing that makes proofs possible at all, like the proof of the principle of mathematical induction. Which I promise I'm getting to.</p>
<p>For this proof, we're going to need the axiom of choice in a slightly different form. This form is actually so commonly referred to that it even has a special name. The axiom of choice is equivalent to the statement that any set can be well-ordered. I'm not going to go into orderings right now, but I will state what this means for the natural numbers because we'll need to use it in our proof.</p>
<blockquote>
<p><strong>Well-Ordering Principle.</strong> Any nonempty set of natural numbers contains a least element.</p>
</blockquote>
<p>This assumes the ordering you're familiar with for the natural numbers. You know, the one where $0&lt;1&lt;2&lt;3$ and so on. This only holds because the natural numbers are bounded below. That is, there is a smallest natural number: $0$. If we wanted to extend this principle to the integers, we'd have to add the condition that our nonempty set of integers has a lower bound.</p>
<p>The well-ordering principle should hopefully seem very obvious to you as well, and I'm not going to show how it implies the principle of mathematical induction.</p>
<h3 id="theprincipleofmathematicalinductionanametheprincipleofmathematicalinduction">The principle of mathematical induction<a name="the-principle-of-mathematical-induction"></a></h3>
<p>I'll stop beating around the bush and just do the thing already.</p>
<blockquote>
<p><strong>Principle of Mathematical Induction.</strong> Suppose P(n) is a (true/false) statement about a natural number $n$, and that the following two criteria hold:</p>
<ol>
<li><strong>Base Case:</strong> $P(0)$ is true.</li>
<li><strong>Inductive Step:</strong> If $P(n)$ is true for some $n\in\mathbb{N}$ then so is $P(n+1)$.</li>
</ol>
<p>Then $P(n)$ is true for every $n\in\mathbb{N}$.</p>
<p><strong>Proof.</strong> We give a proof by contradiction. That is, we're going to assume that the statement is false and show that this leads to an absurd result. We will then be forced to accept the principle of mathematical induction because we will have shown that it cannot be false.</p>
<p>Suppose then that both criteria hold, but that $P$ is not true for at least one natural number. Let $S$ denote the set of natural numbers for which $P$ is false. By assumption, $S$ is not empty, so the well-ordering theorem tells us that $S$ contains a least element. Call this element $x$. Then $x-1\notin S$ since $x$ is the smallest element of $S$. That is, $P(x)$ is false but $P(x-1)$ is true. However, the second criterion tells us that $P\big((x-1)+1\big)=P(x)$ must be true. This is a contradiction, since $P(x)$ cannot be simultaneously true and false. It follows that $P(n)$ is true for every natural number $n$.</p>
</blockquote>
<p>The proof is not terribly important, so don't spend too much time dissecting it. What really matters to us is the statement itself, which might seem daunting at first, so let's break it down piece by piece.</p>
<p>We start off with a statement which is either true or false depending on what number we feed it. The following are examples of such statements<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>:</p>
<ul>
<li>$P(n):=n$ is a prime number.</li>
<li>$Q(n):=n$ sandwiches are on fire.</li>
<li>$R(n):=1+3+5+\dotsc+(2n-1)=n^2$.</li>
<li>$S(n):=$ the $n$th domino will fall over.</li>
</ul>
<p>If we want to show that such a statement is true for all natural numbers, we must first verify the base case, which means showing that if we set $n=1$ then the resulting statement is true. This is usually the easy part of a proof by induction, but it is still necessary.</p>
<p>The last thing we have to do is show that if the statement holds for an arbitrary natural number $n$, then the statement also holds for $n+1$. This is the inductive step, which is usually where the most work and insight are needed.</p>
<p>If we can do these two things, then we have shown that the statement is true for every natural number! Just remember: base case, then inductive step. Now let's look more closely at the example statements from above:</p>
<p>By definition, $P(n)$ is true only when $n$ is prime. For instance, $P(1)$ is false because $1$ is not prime, but $P(2)$ is true because $2$ is prime.</p>
<p>On the other hand, $Q(n)$ is presumably false for all $n&gt;0$. I, for one, have never seen an exploding sandwich.</p>
<p>$R(n)$ is the same statement we discussed earlier: that the sum of the first $n$ odd numbers is $n^2$. We will come back to this in a few moments and show it to be true for any $n$ using an inductive argument.</p>
<p>For now, let's focus on $S(n)$ because it is a helpful and illustrative example of why induction works. Suppose we have a line of dominoes extending infinitely to the right, all standing up and close enough together that if one falls over it will hit the next domino in line:</p>
<p><img src="http://localhost:2368/content/images/2017/04/dominoes-1.svg" alt="line of dominoes"></p>
<p>What are the base case and inductive step for this example? The base case would be $S(1)$, which corresponds to whether or not the first domino will be knocked over. The inductive step is the fact that, if the $n$th domino falls, then the $(n+1)$th domino will also fall. The inductive step is clearly true because each domino knocks over the next one when it falls. However, if the first domino is never knocked over, none of the other ones will be either. This illustrates the importance of the base case! If the base case does not hold, then a proof by induction is invalid even if the inductive step does hold.</p>
<p>So in other words, if we know for certain that the first domino will fall, this means that <em>every</em> domino will fall because we have verified both the base case and the inductive step.</p>
<p><img src="http://localhost:2368/content/images/2017/04/dominoes_falling.svg" alt="dominoes falling"></p>
<p>The argument we have just given shows an infinite number of things to be true! This is part of the power of inductive arguments.</p>
<h3 id="examplesanameexamples">Examples<a name="examples"></a></h3>
<p>Let's now prove that the pattern I introduced at beginning of this post always holds.</p>
<blockquote>
<p><strong>Theorem.</strong> For any $n\geq 1$,</p>
<p>$$1+3+5+\dotsc+(2n-1)=n^2.$$</p>
<p><strong>Proof.</strong> We give a proof by induction on $n$.</p>
<p>For our base case, we need to show that the statement is true when we let $n=1$. This is simple enough, because $1$ is certainly the sum of the first $1$ odd numbers, and it is equal to its square. That is, $1=1^2$, so we have shown that our base case is true.</p>
<p>Now for the inductive step. We will assume that</p>
<p>$$1+3+5+\dotsc+(2n-1)=n^2,$$</p>
<p>and use this to show that</p>
<p>$$1+3+5+\dotsc+(2n-1)+(2n+1)=(n+1)^2.$$</p>
<p>This requires a tiny bit of algebraic manipulation, but isn't at all difficult:</p>
<p>$$\begin{align}<br>
1+3+5+\dotsc+(2n-1)+(2n+1) &amp;= n^2+(2n+1) \\<br>
&amp;= (n+1)^2.<br>
\end{align}$$</p>
<p>In the first step above, we used our assumption that the sum of the first $n$ odd numbers is $n^2$ and substituted this result into the equation. In the second step, we factored our resulting quadratic, and subsequently showed that our inductive step holds.</p>
<p>It follows from the principle of mathematical induction that our assertion is true.</p>
</blockquote>
<p>This proof is a good indicator of a trend that such proofs tend to follow. If we can show that the base case is true, we can frequently break down the statement for higher numbers into smaller pieces and then use our base case to glue everything back together.</p>
<p>In my first post on set theory, I stated without proof that De Morgan's Laws extend to arbitrary collections of sets. I still won't prove that in full, but I'll prove something that's nearly as powerful.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $A_1,A_2,\dotsc,A_n\subseteq X$. Then</p>
<p>$$X-\bigcup\limits_{i=1}^n A_i=\bigcap\limits_{i=1}^n (X-A_i).$$</p>
<p><strong>Proof.</strong> We proceed by induction on $n$. For our base case, we choose $n=2$. That is, we need to show that</p>
<p>$$X-(A_1\cup A_2)=(X-A_1)\cap (X-A_2).$$</p>
<p>But this is just one of De Morgan's Laws, which we already know to be true! So we don't actually have to do any work to establish that the first criterion holds.</p>
<p>Next, let $n\geq 2$ be a natural number and suppose that</p>
<p>$$X-\bigcup\limits_{i=1}^n A_i=\bigcap\limits_{i=1}^n (X-A_i).$$</p>
<p>It is easy to see that</p>
<p>$$\begin{align}<br>
X-\bigcup\limits_{i=1}^{n+1}A_i &amp;= X-\left(\bigcup\limits_{i=1}^nA_i\cup A_{n+1}\right) \\<br>
&amp;= \left(X-\bigcup\limits_{i=1}^nA_i\right)\cap (X-A_{n+1}) \\<br>
&amp;= \bigcap\limits_{i=1}^n(X-A_i)\cap(X-A_{n+1}) \\<br>
&amp;= \bigcap\limits_{i=1}^{n+1}(X-A_i).<br>
\end{align}$$</p>
<p>This completes the proof.</p>
</blockquote>
<p>Let's look at another example. This is one of my favorites, and it lends itself nicely to a visual representation.</p>
<blockquote>
<p><strong>Theorem.</strong> A $2^n\times 2^n$ checkerboard can be covered by L-shaped tiles, with the exception of one arbitrary square.</p>
<p><strong>Proof.</strong> Let's break down this statement a little bit first. When we talk about a $2^n\times 2^n$ checkerboard, we are talking about a square board with sides of length $2^n$. So</p>
<ul>
<li>$n=1$ corresponds to the $2\times 2$ board,</li>
<li>$n=2$ corresponds to the $4\times 4$ board,</li>
<li>$n=3$ corresponds to the $8\times 8$ board, etc.</li>
</ul>
<p>For instance, here is the $2^3\times 2^3$ checkerboard:</p>
<p><img src="http://localhost:2368/content/images/2017/04/2-3_x_2-3_board.svg" alt="2^3 by 2^3 board"></p>
<p>By 'L-shaped tile,' we mean a tile comprised of three squares which is shaped sort of like the letter L:</p>
<p><img src="http://localhost:2368/content/images/2017/04/L_shaped_tile-1.svg" alt="L-shaped tile"></p>
<p>Lastly, when we say that a board can be covered by these tiles with the exception of one arbitrary square, we mean something like this:</p>
<p><img src="http://localhost:2368/content/images/2017/04/tiled_4x4_board.svg" alt="tiled 4 by 4 board"></p>
<p>It's important to point out that we could have left out <em>any</em> one square on the board above and still been able to tile the rest of the board. It does not matter which square we choose to exclude; we should still be able to cover the rest of the board. Try playing around with boards of different sizes, leaving one random square empty, and trying to come up with such a tiling. For larger boards, this quickly becomes no easy task.</p>
<p>Now that we have a better understanding of the problem, it is our goal to show that we can cover any $2^n\times 2^n$ board in this manner, and we will do so by induction on $n$.</p>
<p>For our base case ($n=1$), we must show that the $2\times 2$ board can be tiled by L-shaped tiles, no matter which square we choose to omit. Since there are only four squares on this board, we can 'brute-force' the base case by demonstrating that with any square ommitted, the rest of the board can be tiled quite trivially using a single L-shaped tile:</p>
<p><img src="http://localhost:2368/content/images/2017/04/base_case.svg" alt="base case"></p>
<p>We have shown that any $2\times 2$ board can be covered in the desired manner, so the base case has been verified.</p>
<p>Next we argue the inductive step. We begin with the assumption that for an arbitrary $n\geq 1$, we can cover the $2^n\times 2^n$ checkerboard (with any one arbitrary square omitted) as desired. We must use this information to show that we can then tile the $2^{n+1}\times 2^{n+1}$ board in the same manner.</p>
<p>Notice that if we are given a $2^{n+1}\times 2^{n+1}$ board, the missing tile will necessarily lie in precisely one of its four quadrants:</p>
<p><img src="http://localhost:2368/content/images/2017/04/quadrants.svg" alt="quadrants"></p>
<p>But each of these quadrants is actually a $2^n\times 2^n$ board, which we already know can be covered! So one of our quadrants has a missing tile already chosen for us, and the other three quadrants we can cover however we choose. So we do so in the following crafty way:</p>
<p><img src="http://localhost:2368/content/images/2017/04/quadrants_tiled.svg" alt="quadrants tiled"></p>
<p>We've now covered the entire $2^{n+1}\times 2^{n+1}$ board with the exception of our arbitrary missing square and a nice L-shaped hole in the middle, which we can fill in with a single L-shaped tile, leaving only the chosen square uncovered. We have thus shown that the inductive step holds, completing the proof.</p>
</blockquote>
<p>I think that's more than enough for one post, so I'm going to leave it at that. We will be using induction for many of our future proofs, though, so this won't be the last time you'll see it!</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>The symbol $:=$ means 'is defined as.' <a href="#fnref1" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
</ol>
</section>
]]></content:encoded></item><item><title><![CDATA[A First Look at Topological Spaces]]></title><description><![CDATA[There are also many circumstances in which we care about the shape of a space but couldn't care less about distances. For instance, a famous puzzle that influenced the development of the entire field of topology is the problem of the **Seven Bridges of KÃ¶nigsberg**.]]></description><link>http://localhost:2368/a-first-look-at-topological-spaces/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae218</guid><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Fri, 31 Mar 2017 17:01:23 GMT</pubDate><content:encoded><![CDATA[<h3 id="contents">Contents</h3>
<ol>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#the-definition">The definition</a></li>
<li><a href="#examples">Examples</a></li>
</ol>
<hr>
<h3 id="motivationanamemotivation">Motivation<a name="motivation"></a></h3>
<p>So far we've put a lot of effort into defining notions of distance, even giving these ideas fancy names. Distance is an important property in many situations, and you probably don't need me telling you this because you think about distances every day.</p>
<p>However, there are also many circumstances in which we care about the shape of a space but couldn't care less about distances. For instance, a famous puzzle that influenced the development of the entire field of topology is the problem of the <strong>Seven Bridges of KÃ¶nigsberg</strong>. Basically, the city of KÃ¶nigsberg had two major islands at its center, completely disconnected from the mainland by a river except for seven bridges.</p>
<p><img src="http://localhost:2368/content/images/2017/04/bridges.svg" alt="bridges of KÃ¶nigsberg"></p>
<p>Here is my <em>beautiful</em> depiction of the scenario. It should be obvious from my exceptional artistic ability, but the blue crud is the river and the grey smudges are the bridges. For a far inferior â€” though perhaps more descriptive â€” picture of the problem, I suggest a Google Image search.</p>
<p>The puzzle is as follows: <strong>Is it possible to walk across every bridge precisely once without getting wet?</strong></p>
<p>Try drawing a couple paths and you'll quickly begin to suspect that the answer is no. This is correct, but proving it is tricky. The solution to a problem like this has nothing whatsoever to do with the distance between bridges. Stretch the river, move the bridges slightly, and the solution remains the same. This indicates that there may be more fundamental properties intrinsic to the space in this puzzle than distance.</p>
<p>The branch of mathematics called <strong>topology</strong> is, loosely speaking, the study of the properties of space which remain unaltered by continuous deformations. This probably sounds kind of like poo at this point, but bear with me. Continuous deformations are, intuitively, ways in which we can stretch, bend and move objects but not tear or cut them. As an example, an egg and a pancake are in some sense topologically equivalent, since you can squish an egg down until it becomes sufficiently cake-shaped. You won't even see this idea again for a while, but I find it helps to have some idea of where we're headed in the long run.</p>
<h3 id="thedefinitionanamethedefinition">The definition<a name="the-definition"></a></h3>
<p>Before we proceed, let's recall a few of the nice properties I've previously showed are true of open sets in metric spaces:</p>
<ul>
<li>The empty set and the space itself are open.</li>
<li>The union of any collection of open sets is open.</li>
<li>The intersection of any finite collection of open sets is open.</li>
</ul>
<p>As I mentioned before, we need to trash any definitions we have that mention metrics or distances, so the old definition of open sets in a metric space won't work for the more general topological spaces. What we do instead is kind of neat.</p>
<p>We define a topology in terms of which sets are open, and these open sets must obey certain properties. Actually, they must obey the properties I just listed! These properties talk about open sets only in terms of set theory, and never mention distance, so we simply demand that all of these properties hold.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>topology</strong> $\cal T$ on a set $X$ is a collection of subsets of $X$ called <strong>open sets</strong> which satisfy the following properties.</p>
<ol>
<li>The empty set $\varnothing$ and the set $X$ are in $\cal T$.</li>
<li>The union of any collection of sets in $\cal T$ is in $\cal T$.</li>
<li>The intersection of any finite collection of sets in $\cal T$ is in $\cal T$.</li>
</ol>
<p>A <strong>topological space</strong> is a set $X$ together with a topology $\cal T$ on $X$.</p>
</blockquote>
<p>As a slight abuse of notation, the set $X$ will usually be referred to as a topological space, and it will go without saying that we're talking about some topology on $X$. This is much the same as when we talk about a set as a metric space and it is implicitly understood to have some distance function.</p>
<p>This probably all feels a bit anticlimactic. I've been building up to this for a while, and it probably seems like this isn't anything new at all. At first, it might seem odd to define a topological space as a collection of sets that are open, but it's actually fairly natural. Hopefully at the very least you understand why the three properties in the definition are a natural choice. I spent the last two posts trying to get it to feel that way.</p>
<p>There is a sort of topological analogue to the concept in metric spaces of an open ball centered at a point:</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>neighborhood</strong> of a point $x$ in a topological space is an open set containing $x$.</p>
</blockquote>
<p>And of course, whenever there are open sets there are also closed sets:</p>
<blockquote>
<p><strong>Definition.</strong> A subset $U$ of a topological space $X$ is <strong>closed</strong> in $X$ if its complement, $X-U$, is open.</p>
</blockquote>
<p>Notice again that sets in a topological space can be open, closed, both or neither. In every topological space $X$, we have that $\varnothing$ and $X$ are both open and closed. This information is now part of the very definition of open and closed sets!</p>
<h3 id="examplesanameexamples">Examples<a name="examples"></a></h3>
<p>I haven't been including terribly many examples so far, so I should probably try to fix that. Most people don't learn too well being relentlessly bombarded by definitions and theorems and not ever having an opportunity to step back and apply what they've just learned.</p>
<blockquote>
<p><strong>Example.</strong> Consider the set $X=\{a,b,c\}$ with just three elements. Let's determine whether the following is a valid topology on $X$:</p>
<p>$${\cal T} = \big\{\varnothing,\{a\}, \{a,b\}, \{a,b,c\}\big\}$$</p>
<p>Let's look at this topology visually:</p>
<p><img src="http://localhost:2368/content/images/2017/04/top-abc-1.svg" alt="example topology"></p>
<p>The ellipses in this diagram depict the open sets in the topology. Each ellipse is a subset of all the ellipses that encompass it. The empty set is a subset of every set and is not depicted.</p>
<p>Even without the picture, it's easy to verify that all the required properties hold and that this is a topology on $X$. If you take the union of any collection of these open sets, you wind up with another open set in the topology. The same is true of &gt; intersections. (We could list out all such unions and intersections, but that would take up too much space.) The empty set and $X$ itself are explicitly listed and so clearly they are open, and thus $\cal T$ is indeed a topology on $X$.</p>
<p>One last thing: the set $\{b\}$ in this topological space is neither open nor closed, as an example of what I was saying before.</p>
</blockquote>
<p>There are two topologies that we can define right off the bat on any set $X$.</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>trivial topology</strong> on $X$ is the set $\{\varnothing, X\}$.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> The <strong>discrete topology</strong> on $X$ is the set $2^X$ of all subsets of $X$.</p>
</blockquote>
<p>The trivial topology only has two<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> open sets, and it is in a sense the smallest topology we can define because those sets are required to be open by the definition of a topology. On the other hand, every set is open in the discrete topology, so it is in some sense the largest. Let's make rigorous these notions by defining the concepts of coarseness and fineness.</p>
<blockquote>
<p><strong>Definition.</strong> Given two topologies ${\cal T}_1, {\cal T}_2$ on a set $X$, we say that ${\cal T}_1$ is <strong>coarser</strong> than ${\cal T}_2$ if ${\cal T}_1\subseteq {\cal T}_2$. That is, every open set in ${\cal T}_1$ is also open in ${\cal T}_2$. Equivalently, we also sometimes say that ${\cal T}_2$ is <strong>finer</strong> than ${\cal T}_1$.</p>
</blockquote>
<p>Notice that the trivial topology is always the coarsest topology we can define, and the discrete topology is always the finest. Also notice that there can exist topologies on a set that cannot be compared with each other in this way because it is possible for each topology to contain open sets that are not contained in the other.</p>
<p>Here's just one more example for now, and this should hopefully feel a lot like the &quot;correct&quot; definition to you. We can still talk about open balls in $\mathbb{R}^n$ purely as the sets $B(x_0,r)=\{x\in\mathbb{R}^n\mid d(x,x_0)&lt; r\}$. We can define a topology using these that is very similar to the standard metric on $\mathbb{R}^n$:</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>standard topology</strong> on $\mathbb{R}^n$ is the topology whose nonempty open sets are precisely the unions of open balls in $\mathbb{R}^n$.</p>
</blockquote>
<p>The last thing I want to mention right now is that our definition of a topological space is broad. So broad, in fact, that many such spaces often behave in ways that are not at all desirable. Very frequently, we will talk about certain &quot;tame&quot; types of topological spaces. For instance, soon I'll introduce you to Hausdorff spaces, which have a number of nice properties that we'd generally expect &quot;space&quot; to have.</p>
<p>Next time we'll look again at some more properties of metric spaces. Fair warning: the next post is where things will start getting fairly technical, so make sure you're really comfortable with everything I've discussed so far!</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Unless $X=\varnothing$, in which case it only has one and is particularly uninteresting. <a href="#fnref1" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
</ol>
</section>
]]></content:encoded></item><item><title><![CDATA[Metric Spaces (2)]]></title><description><![CDATA[Looking back through my first post about metric spaces, it occurred to me that I should probably have emphasized a few things that could be a bit confusing, so let me address those first before pressing forward. ]]></description><link>http://localhost:2368/metric-spaces-2/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae217</guid><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Fri, 31 Mar 2017 14:42:15 GMT</pubDate><content:encoded><![CDATA[<h3 id="contents">Contents</h3>
<ol>
<li><a href="#review-from-previous-post">Review from previous post</a></li>
<li><a href="#open-sets">Open sets</a></li>
<li><a href="#closed-sets">Closed sets</a></li>
<li><a href="#now-what">Now what?</a></li>
</ol>
<hr>
<h3 id="reviewfrompreviouspostanamereviewfrompreviouspost">Review from previous post<a name="review-from-previous-post"></a></h3>
<p>Looking back through my first post about metric spaces, it occurred to me that I should probably have emphasized a few things that could be a bit confusing, so let me address those first before pressing forward.</p>
<p>I briefly mentioned the <strong>standard metric</strong> on $\mathbb{R}^n$ ($n$-dimensional Euclidean space), but I didn't relate it back to anything concrete. I defined this metric by</p>
<p>$$d({\bf x},{\bf y})=\sqrt{\sum\limits_{i=1}^n(x_i-y_i)^2},$$</p>
<p>where ${\bf x},{\bf y}\in\mathbb{R}^n$. For $n=1$, this whole thing collapses down to</p>
<p>$$d(x,y)=\sqrt{(x-y)^2}=\vert x-y\vert,$$</p>
<p>where $x,y\in\mathbb{R}$. Thus, the standard metric in one-dimensional Euclidean space is precisely the distance function that motivated the entirety of my last post. Moreover, the open balls $B(x,r)$ in this metric are open intervals of the form $(x-r,x+r)$. I'm sure you can figure out for yourself what the closed balls are.</p>
<p>For ${\bf x},{\bf y}\in\mathbb{R}^2$, where ${\bf x}=(x_1,x_2)$ and ${\bf y}=(y_1,y_2)$, the standard metric becomes</p>
<p>$$d({\bf x},{\bf y})=\sqrt{(x_1-y_1)^2+(x_2-y_2)^2}.$$</p>
<p>This is the distance function we all saw in high school, and it's easy enough to verify that it is, in fact, a metric. However, it's somewhat challenging to show that the standard metric satisfied the properties of a metric for Euclidean spaces of arbitrary dimension, so I won't do that here.</p>
<p>I also want to be clear that the standard metric isn't even close to the only metric we can define. In fact, we can define the following metric on any set at all:</p>
<blockquote>
<p><strong>Definition.</strong> Let $X$ be a metric space. The <strong>discrete metric</strong> on $X$ is specified by</p>
<p>$$d(a,b)=<br>
\begin{cases}<br>
0 &amp; \text{if } a=b,\\<br>
1 &amp; \text{if } a\neq b<br>
\end{cases}$$</p>
<p>for $a,b\in X$.</p>
</blockquote>
<p>It's easy to show that the discrete metric is in fact a metric. Furthermore, we can actually use the discrete metric as a basis to generate an infinite number of metrics on any set! (How?)</p>
<p>There are less trivial metrics, too. For instance, if $X$ is the set of real-valued functions which are continuous on the closed interval $[a,b]$, then</p>
<p>$$d(f,g)=\int\limits_0^1\vert f(x)-g(x)\vert\mathrm{d}x,$$</p>
<p>where $f,g\in X$, is a metric on $X$. If this example is gibberish to you, don't worry. I haven't defined integration, or even continuity, yet. I just wanted to show that there are even meaningful concepts of distance between functions, which is an enticing concept.</p>
<p>I think that's about all I wanted to clarify from last time. Now we can move on to some more exciting new stuff!</p>
<h3 id="opensetsanameopensets">Open sets<a name="open-sets"></a></h3>
<p>We already defined open sets in the last post, but let's restate that definition here so you don't have to go looking it up:</p>
<blockquote>
<p><strong>Definition.</strong> A subset $U$ of a metric space $X$ is <strong>open</strong> in $X$ if for every point $x\in U$ there exists a real number $r&gt;0$ for which the open ball $B(x,r)\subseteq U$.</p>
</blockquote>
<p>I didn't explicitly say this last time, but this definition for open sets applies only for metric spaces. When we talk about the more general topological spaces, we'll have to throw out this definition (although it will serve to motivate the general definition).</p>
<p>Now, armed only with this definition and a few notions from set theory, we can already say quite a bit about open sets.</p>
<blockquote>
<p><strong>Theorem.</strong> The union of any collection of open sets in a metric space is open.</p>
<p><strong>Proof.</strong> Let $X$ denote a metric space and let $I$ be an indexing set such that $A_i\subseteq X$ is an open set for each $i\in I$. Define $U=\bigcup\limits_{i\in I}A_i$.</p>
<p>If every $A_i$ is empty then $U=\varnothing$, which is open. So if this is the case, then we're done.</p>
<p>Suppose then that $U\neq\varnothing$. We need to show that for any $x\in U$, there exists a real number $r&gt;0$ for which $B(x,r)\subseteq U$. But this is extremely easy!</p>
<p>Since $x\in U$, we know from the definition of the union operation that $x\in A_j$ for some $j\in I$. Since $A_j$ is open by assumption, there exists a real number $r_j&gt;0$ such that $B(x,r_j)\subseteq A_j$. Since every point in $A_j$ is also in $U$, choosing $r=r_j$ gives us that $B(x,r)\subseteq U$.</p>
</blockquote>
<p>This is an important property, so never forget it. Also notice that essentially all we had to do in the above proof was notice that, for each point in the union, we already had an open ball that served our purposes.</p>
<p>Next, let's prove a similar result:</p>
<blockquote>
<p><strong>Theorem.</strong> The intersection of a finite collection of open sets in a metric space is open.</p>
<p><strong>Proof.</strong> Let $X$ denote a metric space and let $A_1,A_2,\dotsc,A_n\subseteq X$ be open sets for some $n\in\mathbb{N}$. Define $I=\bigcap\limits_{i=1}^n A_i$.</p>
<p>First, consider the case where the intersection $I$ is empty. If this is true, then we're done since the empty set is open.</p>
<p>Suppose then that $I\neq\varnothing$. We need to show that for every $x\in I$ there exists some real number $r&gt;0$ for which $B(x,r)\subseteq I$. This time the choice of $r$ isn't quite so obvious, but hopefully my reasoning is clear. (If it isn't, try drawing a picture!)</p>
<p>Since $x\in I$, the point $x$ is in every set $A_1,A_2,\dotsc,A_n$. Since each of these sets is open, there exist real numbers $r_1,r_2,\dotsc,r_n&gt;0$ such that</p>
<p>$$B(x,r_1)\subseteq A_1, \\<br>
B(x,r_2)\subseteq A_2, \\<br>
\vdots                  \\<br>
B(x,r_n)\subseteq A_n.$$</p>
<p>Since there are only a finite number of sets, simply picking the smallest radius, that is, $r=\min\limits_{1\leq i\leq n} r_i$, will ensure that $B(x,r)\subseteq A_i$ for $1\leq i\leq n$. It follows immediately that $B(x,r)\subseteq I$.</p>
</blockquote>
<p>Hopefully you're wondering why I only argued that a <em>finite</em> intersection of open sets is open. After all, I showed that an infinite union work, and unions and intersections are pretty similar, right? Well, not really. In fact, I can easily come up with an example of an infinite collection of open sets whose intersection isn't open:</p>
<p>Consider $\mathbb{R}$ equipped with the standard metric, and the infinite collection of open intervals defined by $A_n=\left(-\frac{1}{n},\frac{1}{n}\right)$ for $n\in\mathbb{N}$. It's easy enough to see that $\bigcap\limits_{n=1}^\infty \left(-\frac{1}{n},\frac{1}{n}\right)=\{0\}$. For any $x&gt;0$, the quantity $\frac{1}{n}$ eventually becomes smaller than $x$ as $n$ grows larger, so the only point common to all these sets is $0$ because $\frac{1}{n}\neq 0$ for any $n\in\mathbb{N}$, no matter how large.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> The set $\{0\}$ is not open because no open ball of positive radius is contained within it.</p>
<h3 id="closedsetsanameclosedsets">Closed sets<a name="closed-sets"></a></h3>
<p>Again, we defined closed sets last time, but we'll restate their definition here as well for convenience:</p>
<blockquote>
<p><strong>Definition.</strong> A subset $U$ of a metric space $X$ is <strong>closed</strong> in $X$ if its complement, $X-U$, is open in $X$.</p>
</blockquote>
<p>Let's pause for a second to think about what this means in terms of open balls. This definition tells us that $U$ is closed when any point that it not in $U$ is at the center of some open ball which is disjoint from $U$. This is the concept we used in the last post to prove that a closed ball was closed.</p>
<p>Now we're going to prove two theorems about closed sets that closely mirror the theorems about open sets that we just proved above. Rather than go through a similar process of trying to find a radius which works, we'll make use of the previous results and apply De Morgan's Laws.</p>
<blockquote>
<p><strong>Theorem.</strong> The union of a finite collection of closed sets in a metric space is closed.</p>
<p><strong>Proof.</strong> Let $X$ denote a metric space and let $A_1,A_2,\dotsc,A_n\subseteq X$ be closed sets for some $n\in\mathbb{N}$. From the definition of a closed set we see immediately that their complements, $X-A_1$, $X-A_2$, $\dotsc$, $X-A_n$ are each open. Since the intersection of a finite number of open sets is open,</p>
<p>$$\bigcap\limits_{i=1}^n (X-A_i)=X-\bigcup\limits_{i=1}^n A_i$$</p>
<p>is open by De Morgan's Laws. Since the complement of an open set is closed, we have that $\bigcup\limits_{i=1}^n A_i$ is closed, and we are done.</p>
</blockquote>
<p>Hopefully by now you can see the next theorem coming from a mile away, simply from the symmetry of things. I'll prove it anyway for completeness.</p>
<blockquote>
<p><strong>Theorem.</strong> The intersection of any collection of closed sets in a metric space is closed.</p>
<p><strong>Proof.</strong> Let $X$ denote a metric space and let $I$ be an indexing set such that $A_i\subseteq X$ is a closed set for each $i\in I$. Then the complement of each set, $X-A_i$, is open for every $i\in I$. Since the union of an arbitrary collection of open sets is open,</p>
<p>$$\bigcup\limits_{i\in I} (X-A_i)=X-\bigcap\limits_{i\in I}A_i$$</p>
<p>is open by De Morgan's Laws. Therefore its complement, $\bigcap\limits_{i\in I}A_i$, is closed, completing the proof.</p>
</blockquote>
<h3 id="nowwhatanamenowwhat">Now what?<a name="now-what"></a></h3>
<p>What's fairly awesome is that we now actually know everything we need to know about open and closed sets. Well, not really. But we <em>do</em> know enough now to take another leap forward and define a <strong>topology</strong> on a set! I'm not going to do that right now, though. Instead I'm going to show you that everything we've done so far is really perfectly natural, in that it gives us the results we'd expect when talking about familiar sets.</p>
<p>Let's first look at the set of real numbers. To be formal, I'm talking about $\mathbb{R}$ equipped with the standard metric $d(x,y)=\vert x-y\vert$. I think I said this earlier, but unless I say otherwise you should always assume that I'm talking about the standard metric whenever I say words about the real numbers. I asserted above that the open balls in this metric space are open intervals. That is,</p>
<p>$$\begin{align}<br>
B(x,r) &amp;= \{x\in\mathbb{R}\mid d(x,y)&lt; r\} \\<br>
&amp;= (x-r,x+r).<br>
\end{align}$$</p>
<p>What are the simplest open sets in this metric space? We'll talk about this in a more formal sense when we discuss bases for topologies, but it turns out that in a very real sense, these open intervals are the fundamental building blocks for all open sets on the real number line. That is, any nonempty open set in $\mathbb{R}$ can actually be written as the union of some collection of open intervals!</p>
<p>This may seem obvious, but it's actually a very nice property. I'll give you a hand-wavy reason why it's true. The definition of an open set is that every point in the set is at the center of some open ball which is, in turn, contained entirely in the set. So if we take any open set and union together the largest such open balls centered at each point, we really ought to get our entire open set!</p>
<p>In the plane, $\mathbb{R}^2$ or $\mathbb{C}$, the same is true. However, we're now talking about the standard metric in two dimensions, so open balls look like open disks instead of open line segments. Nonempty open sets in the plane can all be formed by joining together these open balls. Even open <em>rectangles</em> can be expressed as unions of open balls.</p>
<p>An <strong>open rectangle</strong> is the Cartesian product of two open intervals, say $(a,b)\times(c,d)$. The proof that they are open is similar to the proof that open balls are open, and it is not particularly informative so I won't include it . here. But I think it's pretty neat that even the <em>corners</em> of these rectangles can be expressed as the union of a bunch of sufficiently small open balls. What might be even more interesting is that open rectangles can alternatively be thought as the primitive open sets in the plane, and you can get any open set by unioning rectangles together. That includes open balls!</p>
<p>In my next post I'll finally define topological spaces. It will likely be fairly short, since we don't have much to say about them yet, but it will at least give you a sense of what we'll be working with from here on out. From then on, I'll probably alternate between metric spaces and topological spaces. It's usually easiest to introduce topological concepts such as convergence, continuity, connectedness and compactness in terms of metric spaces first, and then take what we need from those definitions to talk about them in the more general setting of topological spaces.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Technically I should have used the Archimedian property of the real numbers to show this, but it's extremely obvious and not particularly necessary for any of our other purposes. After all, this isn't a post about analysis or the foundations of the real number system. <a href="#fnref1" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
</ol>
</section>
]]></content:encoded></item><item><title><![CDATA[Metric Spaces (1)]]></title><description><![CDATA[Taken by themselves, sets do not have much structure to them. They are essentially barren wastelands with no relationships at all between their elements. In this post we will remedy that by defining a way to add a measure of proximity to the points in a set.]]></description><link>http://localhost:2368/metric-spaces-1/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae216</guid><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Fri, 31 Mar 2017 02:51:53 GMT</pubDate><content:encoded><![CDATA[<h3 id="contents">Contents</h3>
<ol>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#metrics">Metrics</a></li>
<li><a href="#open-and-closed-sets">Open and closed sets</a></li>
</ol>
<hr>
<p>Taken by themselves, sets do not have much structure to them. They are essentially barren wastelands with no relationships at all between their elements. In this post we will remedy that by defining a way to add a measure of proximity to the points in a set.</p>
<h3 id="motivationanamemotivation">Motivation<a name="motivation"></a></h3>
<p>First, I'd like to motivate the definition of a metric space a little bit. Recall the set $\mathbb{R}$ of real numbers. Given two numbers $a,b\in\mathbb{R}$ with $a&lt; b$, we make the following definitions:</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>open interval</strong> with endpoints $a$ and $b$ is the set $(a,b)=\{x\in\mathbb{R}\mid a&lt; x&lt; b\}$.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> The <strong>closed interval</strong> with endpoints $a$ and $b$ is the set $[a,b]=\{x\in\mathbb{R}\mid a\leq x\leq b\}$.</p>
</blockquote>
<p>Notice how the only difference between these two types of intervals is that the closed interval includes its endpoints whereas the open interval does not. You've no doubt been exposed to these definitions before so there isn't terribly much to discuss, but these will be our prototypes for the concepts of open and closed sets.</p>
<p>How do we measure the distance between two real numbers? We would like our distance to always be positive, so we take whichever number is lower and subtract it from the higher number. If we don't know which number is higher or we want to make a general statement, we make use of the <strong>absolute value function</strong>:</p>
<p>$$\vert x \vert=<br>
\begin{cases}<br>
x  &amp; \text{if } x\geq 0,\\<br>
-x &amp; \text{if } x&lt;0.<br>
\end{cases}$$</p>
<p>Notice that $\vert -x \vert = \vert x \vert$ for every $x\in\mathbb{R}$. Armed with this function which always returns a positive value, we can rephrase our notion of distance between real numbers $a$ and $b$. We'll call this distance $\vert a - b \vert$. The order in which the points appear no longer matters, which is a good sign.</p>
<p>Now let's explore some of the very nice properties of this distance function we've just defined.</p>
<ol>
<li>$\vert a-b \vert \geq 0$ for all $a,b\in\mathbb{R}$.</li>
<li>$\vert a-b \vert = 0$ if and only if $a=b$.</li>
<li>$\vert a-b \vert = \vert b-a \vert$ for all $a,b\in\mathbb{R}$.</li>
<li>$\vert a-c \vert \leq \vert a-b \vert + \vert b-c \vert$ for all $a,b,c\in\mathbb{R}$.</li>
</ol>
<p>The first three properties above should be somewhat obvious, and I will not bother proving them. The fourth property is known as the <strong>triangle inequality</strong>, and it is very important so I will prove it in two different ways so you'll have to believe it twice as hard.</p>
<blockquote>
<p><strong>Theorem.</strong> Let $a,b$ and $c$ denote real numbers. Then $\vert a-c \vert \leq \vert a-b \vert + \vert b-c \vert$.</p>
<p><strong>Proof 1.</strong> From the definition of the absolute value function, we have the following two facts:</p>
<p>$$-\vert a-b\vert\leq a-b\leq\vert a-b\vert,$$<br>
$$-\vert b-c\vert\leq b-c\leq\vert b-c\vert.$$</p>
<p>Adding together these two inequalities, we see that</p>
<p>$$-\vert a-b\vert-\vert b-c\vert\leq (a-b)+(b-c)\leq\vert a-b\vert+\vert b-c\vert,$$</p>
<p>which simplifies to</p>
<p>$$-\big(\vert a-b\vert+\vert b-c\vert\big)\leq a-c\leq\vert a-b\vert+\vert b-c\vert.$$</p>
<p>If we use the definition of the absolute value function yet again, it follows that $\vert a-c\vert\leq\vert a-b\vert+\vert b-c\vert$, as desired.</p>
<p><strong>Proof 2.</strong> We proceed directly by the following computation:</p>
<p>$$\begin{aligned}<br>
\vert a-c\vert^2 &amp;=\big\vert (a-b)+(b-c)\big\vert^2\\<br>
&amp;= \big((a-b)+(b-c)\big)\big((a-b)+(b-c)\big)\\<br>
&amp;=(a-b)^2+2(a-b)(b-c)+(b-c)^2\\<br>
&amp;=\vert a-b\vert^2+2(a-b)(b-c)+\vert b-c\vert^2\\<br>
&amp;\leq \vert a-b\vert^2+2\vert a-b\vert\vert b-c\vert+\vert b-c\vert^2\\<br>
&amp;=\big(\vert a-b\vert+\vert b-c\vert\big)^2.<br>
\end{aligned}$$</p>
<p>Taking the positive square root of each side, we see that $\vert a-c\vert\leq\vert a-b\vert+\vert b-c\vert$, completing the proof.</p>
</blockquote>
<p>It may not be too obvious why right now, but the four properties above are extremely nice. Let's now move away from the real numbers to more general sets.</p>
<h3 id="metricsanamemetrics">Metrics<a name="metrics"></a></h3>
<p>As I mentioned before, sets do not come with any notion of distance between their elements. We can remedy this problem with the concept of a metric function, which is really just a way to define a concept of distance between every pair of points. This can tell us quite a lot about the set we're dealing with. We would like our metrics to be meaningful in some sense, so we define them in such a way that they obey precisely the four properties we just discussed pertaining to the absolute value function.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>metric</strong> (or <strong>distance function</strong>) on a set $X$ is a function $d:X\times X\to\mathbb{R}$ which satisfies the following four properties:</p>
<ol>
<li>$d(x,y)\geq 0$ for all $x,y\in X$.</li>
<li>$d(x,y) = 0$ if and only if $x=y$.</li>
<li>$d(x,y)=d(y,x)$ for all $x,y\in X$.</li>
<li>$d(x,z)\leq d(x,y)+d(y,x)$ for all $x,y,z\in X$.</li>
</ol>
</blockquote>
<p>The concept of a metric thus captures many of the properties we associate with the concept of distance. For instance, the distance between a point and itself is always zero, which makes a lot of sense. The distance between two points is the same regardless of which direction you measure it. The distance between two points is never negative.</p>
<p>We add the triangle inequality into the mix because it ensures our metrics don't become too unruly (we will see later that without it, open balls wouldn't necessarily be open sets). There's a silly, but perhaps insightful, quote justifying it: &quot;The triangle inequality means that if you are going from $x$ to $z$ and you stop for a beer, it's going to take a little longer.&quot;</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>metric space</strong> is a set $X$ together with a metric on $X$.</p>
</blockquote>
<p>Notice that we can turn any set into a metric space! This means that we can talk about distances between points in any set. Not all such metrics are particularly useful, but we can at least define them.</p>
<p>We'll frequently just refer to the set $X$ as a metric space if its metric is implicitly understood. When talking about $n$-dimensional Euclidean space, $\mathbb{R}^n$, it should be understood that we are always referring to it as a metric space with the following metric, unless otherwise stated. This is really just the distance function you're used to extended to $n$-dimensional space:</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>standard metric</strong> (or <strong>Euclidean metric</strong>) on $\mathbb{R}^n$ is defined by</p>
<p>$$ d({\bf x,y})=\sqrt{\sum\limits_{i=1}^n(x_i-y_i)^2}$$</p>
<p>for all ${\bf x,y}\in\mathbb{R}^n$, where</p>
<p>$$\begin{aligned}<br>
{\bf x} &amp;= (x_1,x_2,\dotsc,x_n),\\<br>
{\bf y} &amp;= (y_1,y_2,\dotsc,y_n)<br>
\end{aligned}$$</p>
<p>for some $x_1,x_2,\dotsc,x_n,y_1,y_2,\dotsc,y_n\in\mathbb{R}$.</p>
</blockquote>
<h3 id="openandclosedsetsanameopenandclosedsets">Open and closed sets<a name="open-and-closed-sets"></a></h3>
<p>Next, let's generalize the notions of open and closed intervals to arbitrary metric spaces. In each of the following definitions, let $X$ denote a metric space with distance function $d$, and let $r\in\mathbb{R}$.</p>
<blockquote>
<p><strong>Definition.</strong> An <strong>open ball</strong> of radius $r&gt;0$ centered at the point $x_0\in X$ is the set $B(x_0,r)=\{x\in X\mid d(x,x_0)&lt; r\}$.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> A <strong>closed ball</strong> of radius $r&gt;0$ centered at the point $x_0\in X$ is the set $\overline{B}(x_0,r)=\{x\in X\mid d(x,x_0)\leq r\}$.</p>
</blockquote>
<p>Both open and closed balls contain all the points of distance less than $r$ from $x_0$. The only difference is that a closed ball also contains the points precisely $r$ away from $x_0$.</p>
<p>We're now set up to define the concept of an open set. This definition is crucial, so be certain to <a href="http://i.imgur.com/qdWoQQr.jpg">firmly grasp it</a> before moving on.</p>
<blockquote>
<p><strong>Definition.</strong> A subset $U$ of a metric space $X$ is <strong>open</strong> in $X$ if for every point $x\in U$ there exists a real number $r&gt;0$ for which the open ball $B(x,r)\subseteq U$.</p>
</blockquote>
<p>I'm going to rephrase this definition slightly before moving on, just to really drill it into your head. A set is open if every point in the set is at the center of some open ball which is itself completely contained in that set. In the familiar metric spaces, this definition neatly captures the idea that no matter how close you get to the edge of an open set, there are always more points inside which are closer to the edge.</p>
<p>Now let's prove something that really ought to be true. I'll even throw in a pretty picture for clarity.</p>
<blockquote>
<p><strong>Theorem.</strong> In any metric space, an open ball is an open set.</p>
<p><strong>Proof.</strong> Suppose we are given an arbitrary open ball of radius $r_0$ centered at a point $x_0$ in a metric space $X$. We need to show that for every point $x\in B(x_0,r_0)$, we can find a real number $r&gt;0$ such that $B(x,r)\subseteq B(x_0,r_0)$.</p>
<p>Before making this argument rigorous, let's take a look at the following diagram of an open ball in $\mathbb{R}^2$ for a hint as to how we should proceed:</p>
<p><img src="http://localhost:2368/content/images/2017/04/open-ball-is-open.svg" alt="open ball is open"></p>
<p>It certainly looks like we could choose $r=r_0-d(x,x_0)$, at least for the open ball above. We'd like to confirm that this choice will work for any open ball in any metric space. We'll argue that, given this choice of $r$, any point $y\in B(x,r)$ is also in $B(x_0,r_0)$. This will certainly show that $B(x,r)\subseteq B(x_0,r_0)$, and that's all we need to do!</p>
<p>With all this in mind, choose any point $y\in B(x,r)$. Notice first that, from our choice of $r$, we have that $d(x,x_0)=r_0-r$. Observe also that $d(x,y)&lt; r$ by the definition of our open ball $B(x,r)$. We now proceed with the following computation:</p>
<p>$$\begin{aligned}<br>
d(x_0,y) &amp;\leq d(x_0,x)+d(x,y)\\<br>
&amp;&lt;(r_0-r)+r\\<br>
&amp;=r_0.<br>
\end{aligned}$$</p>
<p>But this implies, by the definition of our open ball $B(x_0,r_0)$, that $y\in B(x_0,r_0)$. Thus we have shown that $B(x,r)\subseteq B(x_0,r_0)$, as desired.</p>
</blockquote>
<p>That was actually a decent amount of work to prove something which seemed obvious, right? Notice how without the triangle inequality in the first line of this computation, we could not have finished this proof.</p>
<p>Before going any further, let's look at some results which are easier to prove.</p>
<blockquote>
<p><strong>Theorem.</strong> In any metric space $X$, the entire set $X$ is itself open.</p>
<p><strong>Proof.</strong> For any point $x\in X$, every real number $r&gt;0$ satisfies the condition that $B(x,r)\subseteq X$, because every open ball consists only of points in $X$. It follows that $X$ is an open set.</p>
</blockquote>
<p>That was short! Here's an even simpler one:</p>
<blockquote>
<p><strong>Theorem.</strong> In any metric space $X$, the empty set $\varnothing=\{\}$ is open.</p>
<p><strong>Proof.</strong> Since there are by definition no points in the empty set, it is vacuously true that every point in $\varnothing$ is at the center of an open ball contained in $\varnothing$. It follows that the empty set is open.</p>
</blockquote>
<p>Since we're on a roll, let's move on to the definition of a closed set, shall we?</p>
<blockquote>
<p><strong>Definition.</strong> A subset $U$ of a metric space $X$ is <strong>closed</strong> in $X$ if its complement, $X-U$, is open in $X$.</p>
</blockquote>
<p>This leads to another closely-related pair of theorems with even easier proofs.</p>
<blockquote>
<p><strong>Theorem.</strong> In any metric space $X$, the empty set is closed.</p>
<p><strong>Proof.</strong> The complement of the empty set is $X-\varnothing=X$. Since $X$ is open, it follows that the empty set is closed.</p>
</blockquote>
<p>I bet you can already guess the next one.</p>
<blockquote>
<p><strong>Theorem.</strong> In any metric space $X$, the entire set $X$ is itself closed.</p>
<p><strong>Proof.</strong> The complement of $X$ is $X-X=\varnothing$. Since the empty set is open, it follows that $X$ is closed.</p>
</blockquote>
<p>At first glance, these might seem like they contradict our earlier statements about these very same sets being open. The empty set and the space itself are both open and closed? I've heard several times the phrase &quot;a set is not a door.&quot; Yes, sets can be both open and closed. The two are not mutually exclusive. It's also easy to find examples of sets in metric spaces which are neither open nor closed!</p>
<p>The next thing we should do is confirm that a closed ball is a closed set â€” otherwise we'd be in a fair bit of trouble. This proof is pretty similar to the proof that an open ball is open, but a teensy bit trickier.</p>
<blockquote>
<p><strong>Theorem.</strong> In any metric space, a closed ball is a closed set.</p>
<p><strong>Proof.</strong> Suppose we are given an arbitrary closed ball of radius $r_0$ centered at a point $x_0$ in a metric space $X$. If we can show that its complement, $X-\overline{B}(x_0,r_0)$, is open, then by the definition of a closed set, we will be done. Thus, we need to show that for every point $x\in X-\overline{B}(x_0,r_0)$ we can find a real number $r&gt;0$ such that $B(x,r)\subseteq X-\overline{B}(x_0,r_0)$.</p>
<p>Once again, let's look at a diagram for some intuition before we dive any further into the proof:</p>
<p><img src="http://localhost:2368/content/images/2017/04/closed-ball-is-closed.svg" alt="closed ball is closed"></p>
<p>A little bit of inspection indicates that it might suffice to choose $r=d(x,x_0)-r_0$. We'll argue that, given this choice of $r$, any point $y\in B(x,r)$ is also in $X-\overline{B}(x_0,r_0)$. This will certainly show that $B(x,r)\subseteq X-\overline{B}(x_0,r_0)$, and then we'll be finished.</p>
<p>With all this in mind, choose any point $y\in B(x,r)$. Notice first that, from our choice of $r$, we have that $d(x,x_0)=r+r_0$. Observe also that $d(x,y)&lt; r$ by the definition of our open ball $B(x,r)$. We now proceed with the following computation, again starting with a slightly rearranged form of the triangle inequality:</p>
<p>$$\begin{aligned}<br>
d(x_0,y)&amp;\geq d(x,x_0)-d(x,y)\\<br>
&amp;=(r+r_0)-d(x,y)\\<br>
&amp;&gt;r_0.<br>
\end{aligned}$$</p>
<p>The last step above may take a little bit of explanation. Since $d(x,y)&lt; r$, clearly $r-d(x,y)&gt;0$. Thus, $r+r_0-d(x,y)&gt;r_0$, which hopefully makes things a bit clearer.</p>
<p>Since $d(x_0,y)&gt;r_0$, it follows that $y\notin\overline{B}(x_0,r_0)$ by the definition of this closed ball. It follows that $y\in X-\overline{B}(x_0,r_0)$. Thus, $X-\overline{B}(x_0,r_0)$ is open and so its complement $\overline{B}(x_0,r_0)$ is closed, as desired.</p>
</blockquote>
<p>That's enough for one post, I think. Don't worry though â€” we've only just begun with the <em>joys</em> of metric spaces!</p>
]]></content:encoded></item><item><title><![CDATA[Set Theory]]></title><description><![CDATA[Everything in mathematics is built from sets. Even objects such as functions and arithmetic operations like addition are formally defined in terms of sets, although you would likely never expect it.]]></description><link>http://localhost:2368/set-theory/</link><guid isPermaLink="false">5c6c6452a4e2f60287eae215</guid><category><![CDATA[set-theory]]></category><dc:creator><![CDATA[Eric Shapiro]]></dc:creator><pubDate>Wed, 29 Mar 2017 00:49:23 GMT</pubDate><content:encoded><![CDATA[<h3 id="contents">Contents</h3>
<ol>
<li><a href="#what-are-sets">What are sets?</a></li>
<li><a href="#basic-facts-about-sets">Basic facts about sets</a></li>
<li><a href="#operations-on-sets">Operations on sets</a></li>
<li><a href="#functions">Functions</a></li>
</ol>
<hr>
<p>Everything in mathematics is built from sets.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> Even objects such as functions and arithmetic operations like addition are formally defined in terms of sets, although you would likely never expect it. A consequence of this fact is that a basic familiarity with set theory is necessary in order to understand the topics I'll be discussing here. For this reason, I've decided to include this introductory post on the topic so that adventurous readers with little mathematical background will be able to understand my future posts without having to leave the confines of this blog.</p>
<h3 id="whataresetsanamewhataresets">What are sets?<a name="what-are-sets"></a></h3>
<p>Already we encounter something of a problem â€” one which took mathematicians quite a while to resolve, and should not be underestimated. The issue is that, because we wish to use set theory as the foundation of all future mathematical exploration, we don't actually have a place to start when defining sets themselves.</p>
<p>It takes quite a bit of logical sophistication to arrive at a truly satisfactory definition of a set. If you're interested in such things, I encourage you to look into any resource you can find on <em>Zermelo-Fraenkel Set Theory with the Axiom of Choice</em> (usually abbreviated <em>ZFC</em>), which is a more rigorous set of axioms which govern the behavior of sets.</p>
<p>For our purposes, the standard &quot;naÃ¯ve&quot; set theory will do just fine. It is largely intuition-based, because it relies on the primitive notion that sets contain elements and elements are what make up sets. However, as long as we take certain precautions when dealing with sets, we should never encounter any problems stemming from this treatment:</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>set</strong> is a collection of <strong>elements</strong>.</p>
</blockquote>
<h3 id="basicfactsaboutsetsanamebasicfactsaboutsets">Basic facts about sets<a name="basic-facts-about-sets"></a></h3>
<p>Traditionally, capital letters like $A$, $B$ or $X$ are used to denote sets. If $x$ is an element of a set $X$, we write $x\in X$ and we say that &quot;$x$ is in $X$.&quot;</p>
<p>Sets are determined solely by the elements they contain. For instance, if the numbers $1$, $2$ and $3$ are the only elements of the set $X$, we can indicate this by writing $X=\{1,2,3\}$. When we define sets in this manner, by specifying a complete list of elements they contain, we always sandwich the list of elements between curly braces, as above.</p>
<blockquote>
<p><strong>Definition.</strong> If two sets $A$ and $B$ contain precisely the same elements, we say that they are <strong>equal</strong> and we write $A=B$. If they are <strong>not equal</strong>, we write $A\neq B$.</p>
</blockquote>
<p>If two sets are not equal, then by definition they do not contain the same elements. It follows then that one of them must contain at least one element that the other does not.</p>
<p>This definition of set equality should hopefully seem reasonable. Note that it does imply that the sets $\{x, y, z\}$ and $\{x, x, z, y, x, z\}$ are equal because they contain the same elements. Although some elements may appear more than once in the description of a set, or the elements may by written in a different order, a set either contains an element or it does not. <strong>Sets do not have any concept of multiplicity or order for their elements.</strong></p>
<p>There is nothing to prevent sets from containing other sets, but be careful when dealing with sets that have other sets as elements. If $x$ is some element, then $\big\{x, \{x\}\big\} \neq \{x\}$ because the set $\{x\}$ is not the same thing as the element $x$.</p>
<p>It may be helpful to think of sets as bags, and their elements as objects inside these bags. If $x$ is an apple, then $\big\{x, \{x\}\big\}$ represents a bag inside of which are two things: an apple and a bag. Inside of the inner bag is also an apple. This is very clearly not the same thing as $\{x\}$, which is just a bad with only an apple inside.</p>
<p>Equality is not the only relationship between sets that we will be concerned with. The following definitions give us a way to talk about sets which contain some of the same elements.</p>
<blockquote>
<p><strong>Definition.</strong> Let $A$ and $B$ denote two sets. If every element of $A$ is also and element of $B$, we say that $A$ is a <strong>subset</strong> of $B$ and we write $A\subseteq B$.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> If $A\subseteq B$ but $A\neq B$, we say that $A$ is a <strong>proper subset</strong> of $B$ and we write $A\subset B$.</p>
</blockquote>
<p>Notice that, by these definitions, every set is a subset of itself. Also note that if $A\subset B$, it follows that $B$ contains some element that $A$ does not. Thus, if both sets are finite then $B$ is larger than $A$. Infinite sets are weird and do weird things, so we cannot necessarily make the same claim when they get involved.</p>
<p>Again, we must take care and realize that $A\subseteq B$ is a completely different statement than $A\in B$. To visualize this, we refer again to our analogy between sets and bags. Take as an example the case where $A$ is a bag containing only an apple, while $B$ is a bag containing an apple and an orange. It should be clear that $A$ is not actually an element of $B$, but it is a subset.</p>
<p>We're ready now for our first proof!</p>
<blockquote>
<p><strong>Theorem.</strong> Let $A$ and $B$ denote sets. If $A\subseteq B$ and $B\subseteq A$, then $A=B$.</p>
<p><strong>Proof.</strong> Since $A$ is a subset of $B$, every element of $A$ is also an element of $B$. Likewise, since $B$ is a subset of $A$, every element of $B$ is also an element of $A$. It follows that the elements of each set are precisely the same, so $A=B$, as desired.</p>
</blockquote>
<p>One important set will come up in our discussions quite often:</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>empty set</strong> is the unique set containing no elements.</p>
</blockquote>
<p>It is easy to see that the empty set is a subset of every set, since all of the elements it contains (none) are trivially also in every other set.</p>
<p>In many upcoming posts, I am going to assume familiarity with the following sets:</p>
<ul>
<li>The set $\mathbb{N} = \{0, 1, 2, \dotsc\}$ of <strong>natural numbers</strong>.</li>
<li>The set $\mathbb{Z} = \{\dotsc, -1, 0, 1, \dotsc\}$ of <strong>integers</strong>.</li>
<li>The set $\mathbb{Q}$ of <strong>rational numbers</strong> whose elements are of the form $\frac{p}{q}$, where $p, q \in \mathbb{Z}$ and $q \neq 0$.</li>
<li>The set $\mathbb{R}$ of <strong>real numbers</strong>.</li>
<li>The set $\mathbb{C}$ of <strong>complex numbers</strong> whose elements are of the form $a+bi$, where $a, b \in \mathbb{R}$ and $i^2 = -1$.</li>
</ul>
<p>It is convenient and natural to assume that $\mathbb{N}\subset\mathbb{Z}\subset\mathbb{Q}\subset\mathbb{R}\subset\mathbb{C}$, and I encourage you to think this way.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p>
<p>It is often desirable to specify certain subsets of sets with which we are already acquainted in terms of some property that every element in the subset must possess. How would we, for instance, denote the set of positive real numbers? Or the set of even integers?</p>
<p>We can construct such subsets using <strong>set-builder notation</strong>. This is basically a fancy notation describing the form that elements in the set take, followed by a vertical bar which means &quot;such that,&quot; and then a property which every element obeys â€” all sandwiched between the usual set brackets.</p>
<p>In this new notation, $\{x\in\mathbb{R} \mid x&gt;0\}$ would be the set of positive real numbers. Similarly, $\{2n \mid n\in\mathbb{Z}\}$ is the set of even integers.</p>
<h3 id="operationsofsetsanameoperationsonsets">Operations of sets<a name="operations-on-sets"></a></h3>
<p>Given two or more sets, we will often find the need to combine them in certain ways to create new sets. Below are four very common ways of creating new sets from old. Each of their definitions can be extended to combine any finite collection of sets, and sometimes even an infinite collection. Make certain that you are comfortable with each of these definitions, since they will show up everywhere ever.</p>
<p>Let $A$ and $B$ denote sets. Let's say, for illustration, that they're sets of points in the plane of your screen and they look like these blobs:</p>
<p><img src="http://localhost:2368/content/images/2017/03/sets.svg" alt=""></p>
<blockquote>
<p><strong>Definition.</strong> The <strong>union</strong> of $A$ and $B$ is the set of all elements in either $A$ or in $B$, or both. It is written $A\cup B$ and is defined as the set $\{x \mid x\in A \text{ or } x\in B\}$.</p>
</blockquote>
<p>The union of the sets above would look like this:</p>
<p><img src="http://localhost:2368/content/images/2017/03/union.svg" alt=""></p>
<blockquote>
<p><strong>Definition.</strong> The <strong>intersection</strong> of $A$ and $B$ is the set of all elements in both $A$ and $B$. It is written $A\cap B$ and is defined as the set $\{x \mid x\in A \text{ and } x\in B\}$.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> Two sets are said to be <strong>disjoint</strong> if their intersection is empty.</p>
</blockquote>
<p>The intersection of the above sets would look like this:</p>
<p><img src="http://localhost:2368/content/images/2017/03/intersection.svg" alt=""></p>
<blockquote>
<p><strong>Definition.</strong> The <strong>complement</strong> (or <strong>difference</strong>) of $A$ in $B$ is the set of all elements that are in $B$ but not in $A$. It is written $B-A$ and is defined as the set $\{x\in B \mid x\notin A\}$.</p>
</blockquote>
<p>For the sets above, the complement of $A$ in $B$ would look like this:</p>
<p><img src="http://localhost:2368/content/images/2017/03/complement.svg" alt=""></p>
<p>It is worth nothing that, unlike unlike unions and intersections, complements depend on which set comes first. That is, $A-B\neq B-A$.</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>Cartesian product</strong> of $A$ and $B$, written $A \times B$, is the set of all ordered pairs whose first component is in $A$ and whose second component is in $B$.</p>
</blockquote>
<p>The Cartesian product of the above sets would be a bit trickier to draw since this would require four spacial dimensions. As a simpler example, suppose that $A$ and $B$ are both $\mathbb{R}$, the set of real numbers. Then $A\times B=\mathbb{R}\times\mathbb{R}$ and this set would consist of all points in the plane. This is because $\mathbb{R}\times\mathbb{R}$ by definition contains all pairs of the form $(x,y)$ where $x$ and $y$ are real numbers. This is why two-dimensional Euclidean space is often written as $\mathbb{R}^2$, as shorthand for $\mathbb{R}\times\mathbb{R}$.</p>
<p>It should be somewhat obvious how to take the union of an arbitrary collection of sets. For a finite collection of $n$ sets â€” $A_1, A_2, \dotsc, A_n$ â€” their union contains all of the elements that are in at least one of the sets:</p>
<p>$$\bigcup\limits_{i=1}^n A_i = \{x\mid x\in A_1 \text{ or } x\in A_2 \text{ or } \dotsc \text{ or } x\in A_n\}.$$</p>
<p>We can go even further, and take the union of any collection of sets. To do so, we first let $I$ be some <strong>indexing set</strong> such that $A_i$ is a set for every $i\in I$. Notice that there is nothing to stop $I$ from being an infinite set, in which case there would be infinitely many sets $A_i$ in our collection. We can take the union of all these sets as follows:</p>
<p>$$\bigcup\limits_{i\in I} A_i = \{x \mid x\in A_i \text{ for some } i\in I\}.$$</p>
<p>The same type of thinking allows for intersections and Cartesian products to be defined for more than two sets. In the case of Cartesian products, multiplying $n$ sets yields a set whose elements are ordered $n$-tuples, where the $i$th component is an element of the $i$th set being multiplied. Cartesian products of infinitely many sets are a bit too complicated to discuss here, although we can easily take the intersection of an arbitrary collection of sets, much as we did for their union:</p>
<p>$$\bigcap\limits_{i\in I} A_i = \{x \mid x\in A_i \text{ for every } i\in I\}.$$</p>
<p>There are two interesting ways in which the operation of set difference distributes over the operations of unions and intersections. These are called <strong>De Morgan's Laws</strong>:</p>
<p>$$X - (A \cup B) = (X - A) \cap (X - B),$$<br>
$$X - (A \cap B) = (X - A) \cup (X - B).$$</p>
<p>These highly symmetric rules often turn out to be very useful, and I encourage you to draw some pictures to try to understand them. I will not prove them here. To illustrate the second rule, here is an example: if a ball is not both red and blue, then it is not red or it is not blue (or possibly both).</p>
<p>De Morgan's Laws also extend naturally to arbitrary collections of sets:</p>
<p>$$X - \bigcup\limits_{i\in I} A_i = \bigcap\limits_{i\in I} (X - A_i),$$<br>
$$X - \bigcap\limits_{i\in I} A_i = \bigcup\limits_{i\in I} (X - A_i).$$</p>
<h3 id="functionsanamefunctions">Functions<a name="functions"></a></h3>
<p>The last topic I'll talk about in this post is the concept of a function, as well as several important traits that functions may or may not possess. You've likely encountered functions before as things that eat variables and poo out numbers, and that's not a terrible way of thinking about them. We're going to make things a bit more formal though.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>function</strong> from a set $A$ to a set $B$, denoted $f:A\to B$, is a subset of $A\times B$ such that for each $x\in A$, there is exactly one $y\in B$ for which $(x, y)\in f$. In this definition, the set $A$ is called the <strong>domain</strong> of $f$ and $B$ is called the <strong>codomain</strong>.</p>
</blockquote>
<blockquote>
<p><strong>Definition.</strong> The <strong>range</strong> (or <strong>image</strong>) of a function $f$ is the set $\text{im } f = \{y\in B \mid (x,y)\in f \text{ for some } x\in A\}$.</p>
</blockquote>
<p>If you've never seen the above definition of a function before, it's natural to wonder at this point just what the crud I'm talking about. Why are we defining functions as sets? As I stated earlier, <em>everything</em> in mathematics is defined in terms of sets, and functions are no exception. We don't usually think of functions as sets of ordered pairs, but it is useful to define them this way so that we can make precise statements about them which stem from their definition. It also allows us to immediately infer what is meant by function equality, because it is inherited from our earlier definition of set equality.</p>
<p>So using this definition, a function is basically a list of $(x,y)$ values where $y$ is what you'd usually call $f(x)$. That is, the first component is the &quot;input&quot; to the function and the second component is the &quot;output.&quot; The second condition in the definition is a rephrasing of what you may know as the &quot;vertical line test.&quot; In two dimensions, a function from $\mathbb{R}$ to $\mathbb{R}$ is anything you can draw that exists above every value on the $x$-axis, but not more than once.</p>
<p>As I mentioned above, a more familiar way of rephrasing the last part of the definition might be this:</p>
<p>... for each $x\in A$, there exists exactly one $y\in B$ for which $y=f(x)$.</p>
<p>This notation is much more common, and I'll be using it almost exclusively from now on. Often, $f(x)$ is referred to as the <strong>image</strong> of $x$ under the function $f$. This is an unfortunate overloading of the word &quot;image,&quot; but it will not lead to confusion if we are careful.</p>
<p>One of the nice things about our definition of a function it that it allows us to define functions between any two sets we want, and the values of the function don't have to obey any particular rule. We can simply check each point and its image to make sure they obey the definition.</p>
<p>If you're a bit confused by all this, just sit and absorb it for a minute. Realize that ordinary things like the function $f:\mathbb{R}\to\mathbb{R}$, defined by $f(x)=x^2$ for every $x\in\mathbb{R}$, still satisfy our new definition. Think everything through â€” maybe draw some pictures â€” and then continue reading.</p>
<p>Now, notice that something like $f:\mathbb{R}\to\mathbb{R}$ defined by $f(x)=\frac{1}{x}$ for all $x\in\mathbb{R}$ is not actually a function! This is because not every element of the domain gets mapped to a real number. In particular, there is no $y\in\mathbb{R}$ for which $y=f(0)$. We would need to restrict the domain to $\mathbb{R} - \{0\}$ to turn this thing into a function.</p>
<p>The next thing we'll do is introduce some common classes of functions.</p>
<blockquote>
<p><strong>Definition.</strong> A function $f:A\to B$ is <strong>injective</strong> if for every $y\in B$ there exists at most one $x\in A$ for which $y=f(x)$.</p>
</blockquote>
<p>Basically, an injective function cannot have two points in its domain get mapped to the same point in its codomain. We can check that a function is injective by showing that if $x\neq y$ then $f(x)\neq f(y)$. Equivalently, we can do this by instead showing that if $f(x)=f(y)$ then $x=y$. (These two statements are contrapositives so they are logically equivalent, but sometimes one will be easier to show than the other.) Injective functions in the plane satisfy what might constitute a &quot;horizontal line test,&quot; in that any horizontal line will intersect their graph at most once.</p>
<blockquote>
<p><strong>Definition.</strong> A function $f:A\to B$ is <strong>surjective</strong> if for every $y\in B$, there exists at least one $x\in A$ for which $y=f(x)$.</p>
</blockquote>
<p>A surjective function has every point in its codomain get mapped to by some point in the domain. That is, every element of the codomain is the image of some element in the domain. In this case, the codomain of the function is equal to its range. Notice that we can turn any function into a surjective function by restricting its codomain to its range.</p>
<blockquote>
<p><strong>Definition.</strong> A function $f:A\to B$ is <strong>bijective</strong> if for every $y\in B$, there exists exactly one $x\in A$ for which $y=f(x)$. Equivalently, a function is bijective if it is both injective and surjective.</p>
</blockquote>
<p>Bijective functions are particularly nice because they have <strong>inverse functions</strong>. That is, if $f:A\to B$ is bijective, we can find a function $f:B\to A$ with the property that $f\big(g(y)\big)=y$ for every $y\in B$ and $g\big(f(x)\big)=x$ for every $x\in A$.</p>
<p>A function's inverse essentially allows us to undo what that function did in the first place. It turns out that if a function has an inverse, then that inverse is unique and thus we can unambiguously write the inverse of $f$ as $f^{-1}$.</p>
<p>Functions that aren't bijective do not have inverses, but we can still define something similar:</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>preimage</strong> of a set $Y\subseteq B$ under a function $f:A\to B$ is the set of all $X\in A$ for which $f(x)\in Y$. We write $f^{-1}[Y]=\{x\in A\mid f(x)\in Y\}$.</p>
</blockquote>
<p>Note that $f^{-1}$ here does <em>not</em> denote an inverse function! We haven't even defined the preimage as a function at all (yet). The square brackets around its argument (which is always a set) help us to distinguish between whether we are talking about inverse functions or preimages. Given a subset of the codomain, the preimage tells us which elements in the domain get mapped into that set.</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>power set</strong> of a set $X$ is the set of all subsets of $X$.</p>
</blockquote>
<p>We often denote the power set of $X$ as $2^X$ because a simple combinatorial argument shows that if $X$ is a finite set with $n$ elements, then $X$ has $2^n$ distinct subsets.</p>
<p>Preimages are technically functions themselves, though perhaps not in the way you might expect. Since preimages take subsets of a function's codomain and map them to subsets of a function's domain, the preimage of a function $f:X\to Y$ is a set-valued function $f^{-1}:2^Y\to 2^X$ defined by $f^{-1}[U]=\{x\in X\mid f(x)\in U\}$ for every $U\in 2^Y$.</p>
<p>Now the overloading of the symbol $f^{-1}$ is even worse, because we have defined both inverses and preimages as functions! However, careful use of square brackets for the preimage and the fact that one function is set-valued while the other is usually not will help us to avoid confusion. Further, inverse functions only exist for bijections, but preimages are defined for all functions.</p>
<blockquote>
<p><strong>Definition.</strong> Given two functions $f:A\to B$ and $g:B\to C$ we can form a new function $g\circ f:A\to C$, called the <strong>composition</strong> of $f$ and $g$, by defining $(g\circ f)(x)=g\big(f(x)\big)$ for every $x\in A$.</p>
</blockquote>
<p>Intuitively, when taking the composition of $f$ and $g$, we first take $f(x)$ and then feed the result into $g$. Note that the composition of functions is only defined when the first function's codomain is equal to the second function's domain.</p>
<blockquote>
<p><strong>Definition.</strong> For any set $X$, we can define an <strong>identity function</strong> $1_X:X\to X$ by $1_X(x)=x$ for every $x\in X$.</p>
</blockquote>
<p>An identity function maps every element of a set to itself, so essentially it does nothing at all. If no confusion can arise about which set we're talking about, we sometimes drop the subscript and write $1$ instead of $1_X$ to denote this function.</p>
<p>Notice now that we can phrase the definition of an inverse function more compactly: A function $f:A\to B$ has an inverse $f:B\to A$ if $g\circ g=1_B$ and $g\circ f=1_A$. That is, the two possible compositions of $f$ and $g$ are equal to the identity functions on their respective domains.</p>
<p>One final remark I will make is that the composition of functions is always <strong>associative</strong>. That is, if we have three functions $f:A\to B$, $g:B\to C$ and $h:C\to D$, then $(h\circ g)\circ f=h\circ(g\circ f)$. This may look obvious, but the proof â€” while straightforward â€” might elude you if you are not accustomed to writing proofs. Nonetheless, I encourage you to try showing this fact for yourself. Here's a hint: use the definition of function composition. If you've done the proof right, it will probably feel as though you haven't really done anything at all.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>One sentence in and I'm already lying to you. There are alternative formulations of mathematics, most notably via topos theory or type theory. If you are a beginner, I would recommend not looking these things up or you will likely be very confused. Set theory is by far the simplest foundation to start from, which explains why it is the most common approach (and the one I will take here). <a href="#fnref1" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
<li id="fn2" class="footnote-item"><p>More formally it happens that each set, equipped with a certain algebraic structure, is isomorphic to a subobject of the next set, equipped with its own structure. If I ever decide to write a post (or five) detailing the constructions of each of the aforementioned sets, you'll see this idea in excruciating detail. <a href="#fnref2" class="footnote-backref">â†©ï¸Ž</a></p>
</li>
</ol>
</section>
]]></content:encoded></item></channel></rss>